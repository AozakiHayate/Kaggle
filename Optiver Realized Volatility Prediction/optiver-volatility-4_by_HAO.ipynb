{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c437bb71",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-19T04:05:16.393166Z",
     "iopub.status.busy": "2021-09-19T04:05:16.392505Z",
     "iopub.status.idle": "2021-09-19T04:05:17.551274Z",
     "shell.execute_reply": "2021-09-19T04:05:17.551850Z",
     "shell.execute_reply.started": "2021-09-12T04:45:17.208857Z"
    },
    "papermill": {
     "duration": 1.200432,
     "end_time": "2021-09-19T04:05:17.552152",
     "exception": false,
     "start_time": "2021-09-19T04:05:16.351720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "\n",
    "path_submissions = '/'\n",
    "\n",
    "target_name = 'target'\n",
    "scores_folds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6debab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:05:17.610073Z",
     "iopub.status.busy": "2021-09-19T04:05:17.608856Z",
     "iopub.status.idle": "2021-09-19T04:05:17.672378Z",
     "shell.execute_reply": "2021-09-19T04:05:17.672887Z",
     "shell.execute_reply.started": "2021-09-12T04:45:18.297331Z"
    },
    "papermill": {
     "duration": 0.093099,
     "end_time": "2021-09-19T04:05:17.673072",
     "exception": false,
     "start_time": "2021-09-19T04:05:17.579973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "    #train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    #train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    #print(f'Our training set has {train.shape[0]} rows')\n",
    "    return test\n",
    "\n",
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.std],\n",
    "        'wap2': [np.sum, np.std],\n",
    "        'wap3': [np.sum, np.std],\n",
    "        'wap4': [np.sum, np.std],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        # new\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "        # vol vars\n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91828e2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:05:17.731260Z",
     "iopub.status.busy": "2021-09-19T04:05:17.730487Z",
     "iopub.status.idle": "2021-09-19T04:05:24.038577Z",
     "shell.execute_reply": "2021-09-19T04:05:24.037897Z",
     "shell.execute_reply.started": "2021-09-12T04:45:18.36654Z"
    },
    "papermill": {
     "duration": 6.339963,
     "end_time": "2021-09-19T04:05:24.038725",
     "exception": false,
     "start_time": "2021-09-19T04:05:17.698762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train =pd.read_pickle(\"../input/optiver006/train.pkl\")\n",
    "test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "#train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "#train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "#train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "#train = get_time_stock(train)\n",
    "test = get_time_stock(test)\n",
    "\n",
    "train1=train\n",
    "test1=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac74d031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:05:24.105662Z",
     "iopub.status.busy": "2021-09-19T04:05:24.104901Z",
     "iopub.status.idle": "2021-09-19T04:05:24.128196Z",
     "shell.execute_reply": "2021-09-19T04:05:24.128745Z",
     "shell.execute_reply.started": "2021-09-12T04:45:24.54331Z"
    },
    "papermill": {
     "duration": 0.062361,
     "end_time": "2021-09-19T04:05:24.128924",
     "exception": false,
     "start_time": "2021-09-19T04:05:24.066563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace by order sum (tau)\n",
    "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n",
    "#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\n",
    "train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n",
    "#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\n",
    "train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5723af1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:05:24.191032Z",
     "iopub.status.busy": "2021-09-19T04:05:24.189943Z",
     "iopub.status.idle": "2021-09-19T04:05:24.220680Z",
     "shell.execute_reply": "2021-09-19T04:05:24.220111Z",
     "shell.execute_reply.started": "2021-09-12T04:45:24.576462Z"
    },
    "papermill": {
     "duration": 0.065574,
     "end_time": "2021-09-19T04:05:24.220828",
     "exception": false,
     "start_time": "2021-09-19T04:05:24.155254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "# delta tau\n",
    "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c73f88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:05:24.282511Z",
     "iopub.status.busy": "2021-09-19T04:05:24.281759Z",
     "iopub.status.idle": "2021-09-19T04:05:24.285173Z",
     "shell.execute_reply": "2021-09-19T04:05:24.285724Z",
     "shell.execute_reply.started": "2021-09-12T04:45:24.613884Z"
    },
    "papermill": {
     "duration": 0.038861,
     "end_time": "2021-09-19T04:05:24.285900",
     "exception": false,
     "start_time": "2021-09-19T04:05:24.247039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNames = [col for col in list(train.columns)\n",
    "            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\n",
    "len(colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a78d4a40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:05:24.352032Z",
     "iopub.status.busy": "2021-09-19T04:05:24.351211Z",
     "iopub.status.idle": "2021-09-19T04:05:26.807467Z",
     "shell.execute_reply": "2021-09-19T04:05:26.807959Z",
     "shell.execute_reply.started": "2021-09-12T04:45:24.625763Z"
    },
    "papermill": {
     "duration": 2.495373,
     "end_time": "2021-09-19T04:05:26.808137",
     "exception": false,
     "start_time": "2021-09-19T04:05:24.312764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# making agg features\n",
    "\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train.loc[train['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9400f34d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:05:26.875770Z",
     "iopub.status.busy": "2021-09-19T04:05:26.875064Z",
     "iopub.status.idle": "2021-09-19T04:05:27.052216Z",
     "shell.execute_reply": "2021-09-19T04:05:27.051572Z",
     "shell.execute_reply.started": "2021-09-12T04:45:26.902473Z"
    },
    "papermill": {
     "duration": 0.215388,
     "end_time": "2021-09-19T04:05:27.052359",
     "exception": false,
     "start_time": "2021-09-19T04:05:26.836971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6b8a2b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:05:27.122307Z",
     "iopub.status.busy": "2021-09-19T04:05:27.121623Z",
     "iopub.status.idle": "2021-09-19T04:05:35.390736Z",
     "shell.execute_reply": "2021-09-19T04:05:35.391257Z",
     "shell.execute_reply.started": "2021-09-12T04:45:27.082624Z"
    },
    "papermill": {
     "duration": 8.309406,
     "end_time": "2021-09-19T04:05:35.391445",
     "exception": false,
     "start_time": "2021-09-19T04:05:27.082039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] \n",
    "train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "520f16f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:05:35.582700Z",
     "iopub.status.busy": "2021-09-19T04:05:35.581965Z",
     "iopub.status.idle": "2021-09-19T04:05:35.585994Z",
     "shell.execute_reply": "2021-09-19T04:05:35.586480Z",
     "shell.execute_reply.started": "2021-09-12T04:45:35.376783Z"
    },
    "papermill": {
     "duration": 0.166722,
     "end_time": "2021-09-19T04:05:35.586685",
     "exception": false,
     "start_time": "2021-09-19T04:05:35.419963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del mat1,mat2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f17be87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:05:35.664257Z",
     "iopub.status.busy": "2021-09-19T04:05:35.659663Z",
     "iopub.status.idle": "2021-09-19T04:17:41.614566Z",
     "shell.execute_reply": "2021-09-19T04:17:41.613382Z"
    },
    "papermill": {
     "duration": 725.99829,
     "end_time": "2021-09-19T04:17:41.615025",
     "exception": false,
     "start_time": "2021-09-19T04:05:35.616735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[250]\ttraining's rmse: 0.000428698\ttraining's RMSPE: 0.198264\tvalid_1's rmse: 0.00043934\tvalid_1's RMSPE: 0.203917\n",
      "[500]\ttraining's rmse: 0.000406922\ttraining's RMSPE: 0.188193\tvalid_1's rmse: 0.000425343\tvalid_1's RMSPE: 0.19742\n",
      "[750]\ttraining's rmse: 0.000392958\ttraining's RMSPE: 0.181735\tvalid_1's rmse: 0.000416835\tvalid_1's RMSPE: 0.193471\n",
      "[1000]\ttraining's rmse: 0.000383231\ttraining's RMSPE: 0.177236\tvalid_1's rmse: 0.000412414\tvalid_1's RMSPE: 0.191419\n",
      "[1250]\ttraining's rmse: 0.00037525\ttraining's RMSPE: 0.173545\tvalid_1's rmse: 0.000409508\tvalid_1's RMSPE: 0.190071\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1400]\ttraining's rmse: 0.000371108\ttraining's RMSPE: 0.17163\tvalid_1's rmse: 0.000408068\tvalid_1's RMSPE: 0.189402\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[250]\ttraining's rmse: 0.000428653\ttraining's RMSPE: 0.19859\tvalid_1's rmse: 0.000440463\tvalid_1's RMSPE: 0.203012\n",
      "[500]\ttraining's rmse: 0.000406379\ttraining's RMSPE: 0.188271\tvalid_1's rmse: 0.000425018\tvalid_1's RMSPE: 0.195894\n",
      "[750]\ttraining's rmse: 0.000392668\ttraining's RMSPE: 0.181918\tvalid_1's rmse: 0.000417108\tvalid_1's RMSPE: 0.192248\n",
      "[1000]\ttraining's rmse: 0.000382831\ttraining's RMSPE: 0.177361\tvalid_1's rmse: 0.000412924\tvalid_1's RMSPE: 0.19032\n",
      "[1250]\ttraining's rmse: 0.00037469\ttraining's RMSPE: 0.173589\tvalid_1's rmse: 0.000409702\tvalid_1's RMSPE: 0.188835\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1400]\ttraining's rmse: 0.000370548\ttraining's RMSPE: 0.17167\tvalid_1's rmse: 0.000408376\tvalid_1's RMSPE: 0.188224\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[250]\ttraining's rmse: 0.00042857\ttraining's RMSPE: 0.198198\tvalid_1's rmse: 0.000465999\tvalid_1's RMSPE: 0.21632\n",
      "[500]\ttraining's rmse: 0.000406477\ttraining's RMSPE: 0.187981\tvalid_1's rmse: 0.000453135\tvalid_1's RMSPE: 0.210349\n",
      "Early stopping, best iteration is:\n",
      "[708]\ttraining's rmse: 0.000394571\ttraining's RMSPE: 0.182475\tvalid_1's rmse: 0.000445891\tvalid_1's RMSPE: 0.206986\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[250]\ttraining's rmse: 0.00042854\ttraining's RMSPE: 0.198538\tvalid_1's rmse: 0.0004457\tvalid_1's RMSPE: 0.205425\n",
      "[500]\ttraining's rmse: 0.000406238\ttraining's RMSPE: 0.188205\tvalid_1's rmse: 0.00043002\tvalid_1's RMSPE: 0.198198\n",
      "[750]\ttraining's rmse: 0.000392756\ttraining's RMSPE: 0.18196\tvalid_1's rmse: 0.000422586\tvalid_1's RMSPE: 0.194771\n",
      "[1000]\ttraining's rmse: 0.000382971\ttraining's RMSPE: 0.177426\tvalid_1's rmse: 0.000418005\tvalid_1's RMSPE: 0.19266\n",
      "[1250]\ttraining's rmse: 0.000374918\ttraining's RMSPE: 0.173695\tvalid_1's rmse: 0.00041472\tvalid_1's RMSPE: 0.191146\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1400]\ttraining's rmse: 0.000370688\ttraining's RMSPE: 0.171735\tvalid_1's rmse: 0.00041362\tvalid_1's RMSPE: 0.190639\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[250]\ttraining's rmse: 0.000429751\ttraining's RMSPE: 0.198779\tvalid_1's rmse: 0.000442733\tvalid_1's RMSPE: 0.205379\n",
      "[500]\ttraining's rmse: 0.000407766\ttraining's RMSPE: 0.188609\tvalid_1's rmse: 0.000428896\tvalid_1's RMSPE: 0.19896\n",
      "[750]\ttraining's rmse: 0.000394267\ttraining's RMSPE: 0.182366\tvalid_1's rmse: 0.000421757\tvalid_1's RMSPE: 0.195648\n",
      "[1000]\ttraining's rmse: 0.000383985\ttraining's RMSPE: 0.17761\tvalid_1's rmse: 0.000417711\tvalid_1's RMSPE: 0.193771\n",
      "[1250]\ttraining's rmse: 0.000375959\ttraining's RMSPE: 0.173897\tvalid_1's rmse: 0.000414695\tvalid_1's RMSPE: 0.192372\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1400]\ttraining's rmse: 0.000371667\ttraining's RMSPE: 0.171912\tvalid_1's rmse: 0.000413697\tvalid_1's RMSPE: 0.191909\n",
      "Our out of folds RMSPE is 0.19355447288123742\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAEWCAYAAADGuvWEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAB7gklEQVR4nO2debzV0/7/n69GKZUGVDShScNBwjUdZUxXIdyre3XCzxUZ7r2l7kUK3ysyK9wrhFCJDJlVJ6RCNCJCVNJEKZp7//5Ya5+z22fvffY5nXG3no/H53E+n/VZn7Xe788+td9nrfd6LZkZgUAgEAgEAiVFhdI2IBAIBAKBwJ5FCD4CgUAgEAiUKCH4CAQCgUAgUKKE4CMQCAQCgUCJEoKPQCAQCAQCJUoIPgKBQCAQCJQoIfgIBAKBMoqkf0saVdp2BAJFjYLORyAQSEckLQH2B3ZEFbcwsx93s83LzOzd3bOu/CFpCHCImf2ltG0JlH/CyEcgEEhn/mhmNaKOQgceRYGkSqXZf2Epr3YHyi4h+AgEAnsUkmpJekzSCknLJd0mqaK/d7CkKZLWSloj6RlJtf29p4HGwKuSNkq6XlKmpGUx7S+RdIo/HyJpgqQxkn4FspL1H8fWIZLG+POmkkxSH0lLJf0i6QpJR0maJ2mdpBFRz2ZJmi5phKT1kr6U1CXqfkNJr0j6WdJiSf8vpt9ou68A/g1c6H2f6+v1kfSFpA2SvpX0t6g2MiUtk/RPSau8v32i7leTdLek7719H0iq5u8dI+lD79NcSZmF+KgDZZgQfAQCgT2N0cB24BDgcOA04DJ/T8DtQEOgNXAQMATAzP4K/EDuaMqdKfbXHZgA1Aaeyaf/VDgaOBS4ELgPuAE4BTgMuEDSSTF1vwHqATcDL0qq4++NBZZ5X3sC/5HUOYHdjwH/AcZ53zv4OquAbkBNoA9wr6Qjoto4AKgFNAIuBUZK2tffuws4EvgDUAe4HtgpqRHwGnCbL+8PvCCpfgHeUaCME4KPQCCQzrzk/3peJ+klSfsDXYHrzOw3M1sF3Av8CcDMFpvZO2a2xcxWA/cAJyVuPiVmmNlLZrYT9yWdsP8UudXMNpvZ28BvwHNmtsrMlgPv4wKaCKuA+8xsm5mNAxYBZ0k6CDgOGOjbmgOMAi6OZ7eZbYpniJm9ZmbfmGMa8DZwQlSVbcAtvv/XgY1AS0kVgEuAa81suZntMLMPzWwL8BfgdTN73ff9DvCJf2+BNCHM4wUCgXSmR3RyqKROQGVghaRIcQVgqb+/P3A/7gt0H3/vl920YWnUeZNk/afIyqjzTXGua0RdL7ddVxV8jxvpaAj8bGYbYu51TGB3XCSdiRtRaYHzY29gflSVtWa2Per6d29fPWAv3KhMLE2A8yX9MaqsMjA1P3sC5YcQfAQCgT2JpcAWoF7Ml2KE/wAGtDOznyX1AEZE3Y9dHvgb7gsXAJ+7ETs9EP1Mfv0XNY0kKSoAaQy8AvwI1JG0T1QA0hhYHvVsrK+7XEuqCryAGy152cy2SXoJN3WVH2uAzcDBwNyYe0uBp83s/+V5KpA2hGmXQCCwx2BmK3BTA3dLqimpgk8yjUyt7IObGljvcw8GxDSxEmgedf0VsJeksyRVBm4Equ5G/0XNfsA1kipLOh+Xx/K6mS0FPgRul7SXpPa4nIwxSdpaCTT1UyYAVXC+rga2+1GQ01Ixyk9BPQ7c4xNfK0o61gc0Y4A/Sjrdl+/lk1cPLLj7gbJKCD4CgcCexsW4L87PcVMqE4AG/t5Q4AhgPS7p8cWYZ28HbvQ5JP3NbD1wJS5fYjluJGQZyUnWf1EzC5ecugb4P6Cnma319/4MNMWNgkwEbs5Hv+R5/3OtpE/9iMk1wHicHxfhRlVSpT9uiuZj4GfgDqCCD4y641bXrMaNhAwgfF+lFUFkLBAIBNIQSVk4QbTjS9uWQCCWEEkGAoFAIBAoUULwEQgEAoFAoEQJ0y6BQCAQCARKlDDyEQgEAoFAoEQJOh+BQArUrl3bDjnkkNI2o0j47bffqF69emmbsdukix+QPr6kix8QfCkqZs+evcbM8kjjh+AjEEiB/fffn08++aS0zSgSsrOzyczMLG0zdpt08QPSx5d08QOCL0WFpO/jlYdpl0AgEAgEAiVKCD4CgUAgEAiUKCH4CAQCgUAgUKKE4CMQCAQCgUCJEoKPQCAQCAQCJUoIPgKBQCAQ2ANYt24dPXv2pFWrVrRu3ZoZM2bw888/c+qpp3LooYdy6qmn8ssvvwDw5Zdfcuyxx1K1alXuuuuuPG3t2LGDww8/nG7duhXKlhB8BEoVSddJ2ruQzw6R1D/FurdIOiVOeaakSYXpPxAIBMoT1157LWeccQZffvklc+fOpXXr1gwbNowuXbrw9ddf06VLF4YNGwZAnTp1eOCBB+jfP/5/sffffz+tW7cutC0h+AiUNtcBhQo+CoKZDc5nu/BAIBBIW9avX897773HpZdeCkCVKlWoXbs2L7/8Mr179wagd+/evPTSSwDst99+HHXUUVSuXDlPW8uWLeO1117jsssuK7Q9QWQsUGJIqg6MBw4EKgLPAw2BqZLWmNnJkv4M/BsQ8JqZDfTPngH8xz+3xsy6xLT9/4BzgXPNbFOcvkcDk8xsgm/rPuB34INUbN+0bQdNB71WcKfLIP9st52sNPAlXfyA9PElXfyA9PJl9BnV+e6776hfvz59+vRh7ty5HHnkkdx///2sXLmSBg0aAHDAAQewcuXKfNu77rrruPPOO9mwYUOhbQrBR6AkOQP40czOApBUC+gDnGxmayQ1BO4AjgR+Ad6W1AOYDjwKnGhm30mqE92opH7AqUAPM9uSzABJe/m2OgOLgXFJ6l4OXA5Qr159BrfbXnCPyyD7V3P/sZZ30sUPSB9f0sUPSC9fNm7cyKJFi5g9ezZZWVlkZWXx4IMP0rdvX7Zv3052dnZO3R07duxyvWTJEqpVq5ZTNmPGDLZt28aGDRuYM2cOa9eu3aV+yphZOMJRIgfQAliCCzBO8GVLgHr+vDvwVFT9S4F7gD8Cz8RpbwgwD3gNqJxP36OBnkAG8F5U+dm4EZGktrdo0cLShalTp5a2CUVCuvhhlj6+pIsfZunny4oVK6xJkyY5Ze+995517drVWrRoYT/++KOZmf34448W+3/dzTffbMOHD8+5HjRokDVq1MiaNGli+++/v1WrVs169eqVsG/gE4vzf2rI+QiUGGb2FXAEMB+4TdLgImh2PtAUN5UTCAQCgTgccMABHHTQQSxatAiAyZMn06ZNG84++2yefPJJAJ588km6d++etJ3bb7+dZcuWsWTJEsaOHUvnzp0ZM2ZMge0J0y6BEsNPq/xsZmMkrQMuAzYA+wBrgI+AByTVw027/Bl4EJgJPCSpmflpFzP72Tf7GfAw8Iqk083sx3zM+BJoKulgM/vG9xEIBAJpz4MPPkivXr3YunUrzZs354knnmDnzp1ccMEFPPbYYzRp0oTx48cD8NNPP9GxY0d+/fVXKlSowH333cfnn39OzZo1i8SWEHwESpJ2wHBJO4FtQF/gWOBNST+aSzgdBEwlN+H0ZcjJv3hRUgVgFS7HAwAz+8AvuX1N0qlmtiaRAWa22bf1mqTfgfdxwU8gEAikNRkZGXF35548eXKesgMOOIBly5YlbS8zM7PQu+WG4CNQYpjZW8BbMcWf4EY3InWeA56L8+wbwBsxZUPyaTu6blbU+ZtAqwIZHwgEAoEiIwQfgUAZp2nTpuyzzz5UrFiRSpUq8cknn3DhhRfmzN2uW7eO2rVrM2fOHN555x0GDRrE1q1bqVKlCsOHD6dz586l7EEgEAjsSgg+AmmFpJHAcTHF95vZE6VhT1ExdepU6tWrl3M9blzuCuF//vOf1KpVC4B69erx6quv0rBhQxYsWMDpp5/O8uXLS9zeQCAQSEZY7VIESKot6cp86jSVdFEKbTWVtKAIbcuSNKKo2iuLSOot6WtJXwMfmVlGzPFEVN1WkmZI2pKqNHtZxswYP348f/6zy5s9/PDDadiwIQCHHXYYmzZtYsuWpNIngUAgUOKEkY+ioTZwJfBQkjpNgYuAZ0vAnj0GLzh2M9ARMGC2pFfM7JcEj/wMXAP0KEg/paFwumTYWQBI4rTTTkMSf/vb37j88stz6rz//vvsv//+HHrooXmef+GFFzjiiCOoWrVqidkcCAQCqSCnARLYHSSNxQlkLQLe8cVn4r4MbzOzcZJmAq2B74AngYnA00B1X7+fmX0oqSlO9Kptgr5mApea2UJ/nQ30B74FHgea42TDLzezeZKygI5m1i9aYtw/u9HMakjKBIYC63ArUsbj9DOuBarhlEO/kVQfeARo7M25zsymJ7DzJOB+f2nAiTjl0v5m1s3XGYEToBktaQku0fRMYDtOWfR24BBguJk9kqCfPwOZZvY3f/1fINvMnksmyS5pCLDRzPJu15hbJ1rh9MjB9z2aqGqx0K6Rm0pZvXo19evX55dffqF///5cc801dOjQAYB7772XRo0accEFF+zy7HfffceNN97InXfeSaNGjXa5t3HjRmrUqFEyThQj6eIHpI8v6eIHBF+KipNPPnm2mXXMcyOe8lg4Cqzc2RRY4M/PwwUgFYH9gR+ABkAmUUqauM3U9vLnh+JV4KLbStDX34Gh/rwBsMifPwjc7M87A3P8eRYwwqJUPqPa2uh/ZuICjwZAVWB5VB/XAvf582eB4/15Y+CLJHa+Chznz2vgRtli38EIIMufLwH6+vN7ccql+wD1gZVJ+ukP3Bh1fZMvqw8sBZr58joxzw3BBUIpfcZlReE0Wm1w27Zttt9++9nSpUt3qbN06VI79NBD7YMPPojbRrooN6aLH2bp40u6+GEWfCkqCAqnJcbxwHNmtsPMVgLTgKPi1KsMPCppPm6DtTYptj8eJxMOcAEwIarfpwHMbApQV1JB1GA+NrMV5vZG+QZ425dHFEQBTgFGSJoDvALUlJQonJ4O3CPpGqC2maWyScIrUX3OMrMNZrYa2CKpdgF8ATgGJ6P+HYDlipKVK3777beczZt+++033n77bdq2dYNi7777Lq1ateLAA3PFXdetW8dZZ53FsGHDOO642LzbQCAQKBuE4KP0+DuwEuiAy1eokspDZrYcWCupPXAhSTZGi8N2/Gfuxbqi+4zOStwZdb2T3NygCsAxlpvI2cjMNiawcxhOwbQaMF1Sq+j+PXvFPBbdZ6w9ifKTlgMHRV0f6MvSgpUrV3L88cfToUMHOnXqxFlnncUZZ5wBwNixY3MSTSOMGDGCxYsXc8stt5CRkUFGRgarVq0qDdMDgUAgISHhtGiISISDU8z8m6QngTq4XIcBQCN2VdKsBSwzs52SeuOmaVJlHHA9UMvM5kX12wu41edwrDGzXyVFP7cEl3cxHrehWuUC9AluNORqYDiApAwzmxOvopcvnw/Ml3QUTtRrNtBGUlVcUNKFFLe0T8JbwH8k7euvTwP+hXufiSTZyw3Nmzdn7ty5ce+NHj06T9mNN97IjTfeWMxWBQKBwO4Rgo8iwMzWSprul8i+gctXmItLtLzezH6StBbYIWkuLvfiIeAFSRcDbwK/FaDLCbhkzlujyoYAj0uah0s47R3nuUeBl70NBe0T3CqRkb6PSsB7wBUJ6l4n6WTcqMVC4A0z2yJpPLAAl3j7WQH7z4OZ/SzpVuBjX3RLJMiIJ8ku6QCcqmpNYKek64A2Zvbr7toSCAQCgdQIwUcRYWaxGh4DYu5vwyWCRtM+6nygr7cEiLvSJaqtlcR8dv4Lt0ecuqNxwU7kuWPi9JkNZEc9kxl1nnPP3J4pFyazLeq5qxOUX48btYktbxrP5th7Cdp8HLfSJ7Y8niT7T5TxHXB37NhBx44dadSoEZMmTWLy5MkMGDCAnTt3UqNGDUaPHs0hhxySU/+FF16gZ8+efPzxx3TsmDepPBAIBMoaIecjEChj3H///bRu3Trnum/fvjzzzDPMmTOHiy66iNtuuy3n3oYNG7j//vs5+uijS8PUQCAQKBQh+CijSDpd0pyYY2Ix9jdKUqorbqKf6xPHzsmSGu6GLadKmi1pvv/ZWVK7OP3M8vXflDRX0kJJj0hKmj/j66+TNKmwNhYXy5Yt47XXXuOyyy7LKZPEr7+6WaH169fnKJgC3HTTTQwcOJC99orN3Q0EAoGyS5h2KaNYPru0FkN/l+VfK+5zTwC77Jvihc8aAj8W0pw1wB/N7EdJbYG3zKwRkJGg/gU+uVa4fJjzgbFJ2h+O01n5W6oGFbfCaUTN9LrrruPOO+/MWV4LMGrUKLp27Uq1atWoWbMmM2fOBODTTz9l6dKlnHXWWQwfPrzYbAsEAoGiJgQfeyCSquNWvByIWxVyK9AXJ87VELjFV60GVDGzZpKOBO7BCYatwYmDrYjTdk/c0uFnJG0CjsXlv/zRt/ch8Dczs4g6q5l9IqkeToymqZlFJ6IuBKpJquo1SPIQlSxaCbd82Lwth+AUWesDO4DzzewbM5vsVwTl956iFU4Z3C4VqZLCkZ2dzYwZM9i2bRsbNmxgzpw5rF27luzsbAYPHsytt95KmzZtcpbX/vOf/+Qf//gHgwYNIjs7m3Xr1jF79mw2boy78nkXNm7cSHZ2drH5UlKkix+QPr6kix8QfCl24imPhSO9D5wK66NR17VwSaUdY+qNB67CLcn9EKjvyy8EHk/S/i5tEaUuihNC+2NsPaAesCROWz2Bd1Pw6S3gF5wKa0VfNgs4x5/vBewdVT+TKLXV/I6SUDgdNGiQNWrUyJo0aWL777+/VatWzbp27WrNmzfPqfP9999b69atbd26dVa3bl1r0qSJNWnSxKpWrWoNGjSwjz/+ON9+0kW5MV38MEsfX9LFD7PgS1FBUDgNRDEft+z0DkknmNn62AqSrgc2mdlIoCVuBc47Xt30Rgq2YuRkSbO8mmtn4LBUHpJ0GHAHKUyPmNnp5MrDd5a0D9DIzCb6+5vN7PcC2Fzi3H777SxbtowlS5YwduxYOnfuzMsvv8z69ev56quvAHjnnXdo3bo1tWrVYs2aNSxZsoQlS5ZwzDHH8Morr4TVLoFAoFwQpl32QMzsK0lHAF2B2yRNjr4v6RRc3sSJkSJgoZkdW9C+JO2F0zTpaGZL/YZukezIaMXTvWKeOxC3+d7FZvZNin5tlvQybpO/mQW1tSxSqVIlHn30Uc477zwqVKjAvvvuy+OP51lVHAgEAuWKEHzsgfiVKD+b2RhJ63Ay6JF7TYCRwOlmtskXLwLqSzrWzGZIqgy0ML+zbhyiFV8jQcUavw9MT3L3o1mCU1z9iNz9avD7uLwGDLIEu+ZG1a0B7GNmKyRVAs4C3jezDZKWSephZi95VdWKZX30I0JmZiaZmZkAnHPOOZxzzjlJ65e5+dxAIBBIQph22TNpB3zkp1BuBm6LupcF1AVe8stZXzezrbjg4A6vjjoH+EOS9kcDj/j2t+CUVRfg8jI+jqp3F9BX0me4nI8I/YBDgMFRy2r3S9BXdeAVr7o6B6dk+oi/91fgGn/vQ+AAAEnv4zbz6+IDlNOT+BIIBAKBIiaMfOyBWPxlvJn+5yfA0DjPzCF3Gia/9l8AXogqutEfsfW+ZFeV1xt9+W3sGhAl62sl8XcNxsy+Jq+qLGZ2QiptBwKBQKB4CCMfgUAZYseOHRx++OF069YNgKysLJo1a5azQ+2cOXNy6mZnZ5ORkcFhhx3GSSedVEoWBwKBQMEJIx+BQiNpJHBcTPH95oTHiqO/WbjVLNH81dzuuWlBRFo9omgKMHz4cHr27LlLvXXr1nHllVfy5ptv0rhxY1atWlXSpgYCgUChCSMfAaBw8upmdpWZZcQcT0jKKmp5dd/f0XH6my/p/yQtlZS/wpZr/3FJq/wuxGWGeNLqiXj22Wc599xzady4MQD77ZcoJSYQCATKHmHkIwAUXl49AVm4BNMik1cHGiWp/yowAvg6xfZH+/pPpWpQccqrJ5NWB7jhhhu45ZZb6NKlC8OGDaNq1ap89dVXbNu2jczMTDZs2MC1117LxRdfXCz2BQKBQFETgo89kDSUV5/p+461ZX/cypfmvqivmX1oZu9JaprCeyoRefVk0up//OMf6d27N9u2bePuu+/miiuuoHfv3nz//fcsWrSIu+++m61bt3LVVVchiYMOOijf/sqk1HIhSBc/IH18SRc/IPhS7MSTPQ1Heh+koby6r7sx5noccJ0/rwjUirrXFFiQ6jsrbnn1eNLqvXr12qXO1KlT7ayzzjIzs9tvv90GDx6cc++SSy6x8ePHp9RXushGp4sfZunjS7r4YRZ8KSoI8uqBKNJOXj0BnYGHAcxsRzw/ywrxpNXHjBnDihVucMnMeOmll2jbti0A3bt354MPPmD79u38/vvvzJo1i9atW5emC4FAIJAyYdplD8TSVF49HenVqxerV6/GzMjIyOCRR5x+WuvWrTnjjDNo3749FSpU4LLLLssJTAKBQKCsE4KPPZB0klfPh8m4XJb7JFUEapTl0Y8I0dLqU6ZMSVhvwIABDBgwoISsCgQCgaIjTLvsmaSTvDqS7pS0DNjby6UP8beuxU35zAdmA218/eeAGUBLX//SJL4EAoFAoIgJIx97IJZG8uq+/vXA9XHKV+J2uI0t/3OqbZckO3bsoGPHjjRq1IhJkyaRlZXFtGnTqFWrFgCjR48mIyOD7OxsunfvTrNmzQA499xzGTx4cGmaHggEAgUiBB+BQBkhVXVTgBNOOIFJkyaVpHmBQCBQZBTbtEuqapPF0O91kvYu4jbflLROUrH/by9ptNfKKJTqaJz2mhalkqdXLx3hz0dGTYtEjj6SMiX9IeqZKyRd7M/j+ifp3yn0PStOf+0SfT6SmvlnFksaJ6mKL6/qrxf7+02L6v0UloKomwYCgUB5p1yOfEiqaGY7Ety+DhgD/F6A9iqZWTIFqeHA3hRwyWc+duaLFa3qaJFjZlfFK/c5Fxtx2iCY2SPx6sX492/gP/n0d3SC/hJ9PncA95rZWEmPAJfilt5eCvxiZodI+pOvd2Gyvotb4bQg6qYAM2bMoEOHDjRs2JC77rqLww5LafVyIBAIlAmKPfiQk528EzgTMOA2MxsnqQJO4rozsBTYhhOumpCgnSU40ahTgTsl/YzLTagKfAP0AS7BKXROlbTGzE6WtNHMavg2egLdzCxL0mhgM3A4MF1SHeBXnDrnAcD1EVvMbLKkzBT9zddOM9soaTBxVD9j2sqmEKqjvvxxX//tfOydCVwaWbkS1ee3vo3muEDucjObF/PsH3F5GlWAtUAvb98VwA5JfwGuBrrgBMDuSuBfT5yK6Rycouk3uNU49/l6/wesMrP74/kQ7/Pxv3edgYt80ZPAEFzw0d2fg1t5M0KS4rz/ElE4vf322wukbvrbb78xZswYqlWrxsyZMzn99NMZM2ZMyv2VSbXDQpAufkD6+JIufkDwpdiJpzxWFAdebRKnpvkOTmFyf+AHoAHuC+d13NTPAcAvQM8k7S3BBQTgVka8B1T31wOBwVH16sXaYblqmaP9+WhgElAx6vp5b08bYHFM/5nApBT8TtXORKqfoyPvgUKqjgLzgBP9+XCSKHkCfweG+vMGwCJ//iBwsz/vDMzx51nACH++LyB/fhlwtz8fgpNNJ/Y6kX8xn1NT4FN/XgEXjNTN573v8vn4d7846vqgyHvArbw5MOreN9G/M/GO4lQ4Lai6aSxNmjSx1atXp9xfuig3posfZunjS7r4YRZ8KSooRYXT44HnzClMrgSmAUf58ufNbKeZ/QRMTaGtcf7nMbgAYbr/a7k30KQQtj1vu06LvOTt+RwXKBWWVOw8WQVU/VQKqqNeI6O2mb3nH3s6n2bHk6uxcQG5GhzHR541sylAXUk1Y549EHjL+zAgFR9SwcyWAGslHQ6cBnxmZmuLou2ySEHVTX/66adI0MRHH33Ezp07qVu3bqnZHwgEAgWlvOV8/OZ/CnjHUlsyGT2UvlfMvd9irqM3LhOFJ6mdSq76GRelqDrqg4+UMbPlktZKao8bPbmiAI8/CNxjZq/4aY8hBek7H0bhRlkOIHcKqSCsBWpH5fMcCCz395bjRkKWSaqE29umzAU3idRNJ0yYwMMPP0ylSpWoVq0aY8eOzbOpXiAQCJRlSmLk433gQkkVJdXHfXl+BEwHzpNUQW730cwCtDkTOE7SIeB2aZXUwt+LVtcEWCmptc8xOWc3fSkoieyMp/qZEOWqjp5vcVRHfZ3Kkg4zs3XAOknH+3q9UrBzHE4no5bl5nW8H3nWBxZrzOzXmOdqkfuF3juqPPYzSIVtcsqpESYCZ+BGyWI1SfLFD/dNJffd9gZe9uevRNnbE5hikaGEUiYzMzNnCe2UKVOYP38+CxYsYMyYMdSoUQOAfv36sXDhQubOncvMmTP5wx+S6b0FAoFA2aMkgo+JuByEucAUXD7ETzgRqmXA57jVKZ8CKUlfm9lq3F/Fz0mah1OrbOVv/w94U1JkGmcQLrfjQyDPFvCpIOl9XD5IFzlFzNN3x04fICRS/YxHFgVTHe0DjPTTMan8STwB+BNuCibCEOBIb/cwdg0uous8L2k2LuE1wqvAOd7WE1LoH9znNk/SMwDev6nAeMtnxVCSz2cg8A9Ji3Hv7zFf/hhuGmkx8A/c70ggEAgESop4iSAldeD22gD3xfANcEBp2hOOsnPgAuM5wKGlbYtZ8Sacmplt377dMjIycpJKL7roImvRooUddthh1qdPH9u6dauZma1bt866detm7du3tzZt2tjjjz9e4L7SJZEuXfwwSx9f0sUPs+BLUUEpJpwmY5L/6/x94FZzIyKBPRwvPLYYmGxmX5e2PSVBRN00Qq9evfjyyy+ZP38+mzZtYtSoUQCMHDmSNm3aMHfuXLKzs/nnP//J1q1bS8vsQCAQKBSlqnBqZplmlmFmbcxstH9uYhwVy5SmOfzzJaJwurt2JumrWBROJZ0ex96JhWgvR+E0SZ3dUjg1s8/NrLmZ/TOqjXZx7J8lKUPSDEkLJc2TdGHUM81UDhRO46mbdu3aFUlIolOnTixbtgwASWzYsAEzY+PGjdSpU4dKlcpb3nggENjTKXP/a5lZvkmhKgMKp0VgZ75YESqcWvzN5IqLTIpY4dTM5gMZseU+gfdiM/taUkNgtqS3zOXVlHmF02TqpgDbtm3j6aef5v77nb5av379OPvss2nYsCEbNmxg3LhxVKhQ2gOYgUAgUDCCwmlQOC3XCqdm9lXU+Y+SVuFWAa2nHCicJlI3jXDXXXfRvHlzduzYQXZ2NtOmTaNevXo8++yz/Pjjj1x22WWMGjWK6tWrp9xnmVQ7LATp4gekjy/p4gcEX4qbkhj5OBf3F2sHnOrkx5LeA47DKVm2AfYDviB/PYe1ZnaEpHrAi8ApZvabpIHAP8zsFkn/AE42szXJmwKc9sMfzGyHD0Ya4MS1WuGWY8YNhFIgqZ24QGKEmd0CIOlpoBtulUgezOwVbw+SxgPT5JalPgh0N7PVfrrh/3AB2BNAPzN7T27fk2SMw4mL3SypAdDAzD6R9CBO3KuHpM7AU+QdefgAOMbMTNJluIDtn36UISfYkNQlmQFmNkhSPzPL8PWb+vd2nw9S/wR0yscPJHXCBULf4JKY10WNaC0DGvnzRriAFzPb7gOVuuy6Ygcz+x9uFQ6Nmx9id88v+n8uf9avzJ49m6ysLDZv3syvv/7KqFGjGDNmDEOHDqVSpUqMHz8+Z3Rj+PDhDBo0iBNOcIuIHnvsMerXr0+nTvm+nhyys7PJzMwscl9KmnTxA9LHl3TxA4IvxU1JBB85Cqc4zY08CqfAT8pdGpuMeMqh4L5wZhTCtrgKp8DnctojhSUVO0+WUyzdG6iD+4s/bvARQVEKp5LakqtwCk6+foXiK5yemaTZ8bjRkZvJq3B6HjiFU0mJFE7H+aClCvBdMvtTxcyWyAmfHY5Tmv3M8lE49TY8DfQ2s50qYtGtapUrsmjYWUXapuMsbr/9dsD9B3HXXXcxZswYRo0axVtvvcXkyZN3mVZp3LgxkydP5oQTTmDlypUsWrSI5s2bF4NdgUAgUHyUuZyPfAgKp0HhNA8+KHoNuMHMZvricq1wesUVV9CkSROOPdZ9vOeeey6DBw/mpptuIisri3bt2mFm3HHHHdSrV6+UrQ0EAoGCURLBx/vA3yQ9ifsL/0TcPiBVgd6+vD4uSfHZFNuciRPROsTMFkuqDjTy8/8Rdc3IEPpKSa1xiqDn+PslRVw7gVX+frTCacIpHuUqnJ5ucRROzWyGn4ZpYWYL5VbmHG9mH7D7Cqe3KkrhNGZEIZnCaewoSX5sk1TZzLb564m46anK5OZt5MGvYJkIPBWdL+SngiIKp2OJr3A6gzKkcJqZmZkzNLp9e/z8koYNG/L220nTeAKBQKDMExROU0BB4bQsK5xegAtos5S7BDfD3wsKp4FAIFAGUWn+wSephl/5URe338txQWgsAOATTT/F7WdT6kJjLVu2tEWLFpW2GUVCWUw+Kwzp4gekjy/p4gcEX4oKSbPNrGNseWkLBASF00AetAcqnO7YsYPDDz+cbt26AU7htGXLlrRt25ZLLrmEbdvcbNTLL79M+/btycjIoGPHjnzwwQelaXYgEAgUilJNODWzzNgyOdXNZjHFA71IVpmhvNgZwU8V3RFT/F0qYmkljZl9jtMXyUFSO9xqlmi2mNnRJWZYMRKRV//1V7dxcK9evRgzZgwAF110EaNGjaJv37506dKFs88+G0nMmzePCy64gC+//LI0TQ8EAoECU+ZWu5TFL8N4lFU7Jd0CvGdm70aXl7DCaZEQLRCXSOE0HYjIq99www3cc889gJNXjxAtr16jRo2c8t9++42iXlIcCAQCJUGZCz4ChUdOzn1wadsBKUnWlyuKQ159idcNKYi8OsDEiRP517/+xapVq3jttaKXfA8EAoHiJgQf5QSv+vkmMBs4AidKdjFutVC0nPsZwCQzmyDpKOB+oDpOw6QLTip9GG5pc1VgpJn9N0GfDXzbNXG/K33N7H25TQMfBU4DfgL+5FVWs3Grbo7HrfDJJr78+//DyZZXweV2/NXMfpfUDLfcuga5y2ITvY+EtiWR09+Ek9PfD6cEezFwLDDLzLLi9FGs8urZ2dnMmDGjQPLqAPvuuy+PPPIIc+fOpV+/ftx9990F6rcsSi0XhnTxA9LHl3TxA4IvxY6ZhaMcHDgpesOtCAInvNUfWIJbvhypNxq3BLcKbn+Wo3x55Ev6cuBGX1YV+ARolqDPf+KEu8ApqO7jzw3o5c8H46TiAbKBh/x5Zdzy5vr++kLc3j0AdaP6uA242p+/gtskDuAqnER7oveRyLaNUXV6AqOj3stY3NLj7rh9fNrhkq5nAxnJ3n+LFi2sOBg0aJA1atTImjRpYvvvv79Vq1bNevXqZWZmQ4YMse7du9uOHTsSPt+sWTNbvXp1gfqcOnXq7phcZkgXP8zSx5d08cMs+FJUAJ9YnP9TS3u1S6BgLDWz6f58DG6EAXLl3KNpCawws48BzOxXc9MgpwEX+1VGs3D6F4cm6O9joI9XYG1nZpF5gZ1RfUbbEW1LS3Ll3+fgNqA70N9rK+l9SfNxQmaH+fLjgOf8eWxyaaq2JeNV/49hPrDSzOabk9NfiAvuSpzbb7+dZcuWsWTJEsaOHUvnzp13kVd/7rnndpFXX7x4cSSw4tNPP2XLli3UrVu3NEwPBAKBQhOmXcoXsaIsketYmfhkCDfSkG/yqbmN6U4EzgJGS7rHzJ7Kx65oafk88u+e0UAPM5srKQs3BRSvrcLYlkxOPyKfv5NdpfR3Usb+LSSSV3/hhRd46qmnqFy5MtWqVWPcuHEh6TQQCJQ7ytR/uIF8aRyRU8dJjn+Ay2GIxyKggaSjzOxjSfvgch7eAvpKmmJm2yS1AJabWZ4Axsu6LzOzRyVVxeWaPIWbqojIlkfsiNd/XPl3nPz9Cl/Wi1yJ9uk4pdUx5CMLn8S20pTT3y1SkVcfOHAgAwcOLEGrAoFAoOgJ0y7li0XAVZK+APYFHk5U0Zw8+YXAg15+/R3cSMAoXJLqp5IWAP8lcRCaCcyV9JlvK7Lk4jegk3++M24Plnj9J5J/vwk35TMdiBapuNb7Nx+3B04yEtm223L6gUAgEChewshH+WK7mf0lpqxp9IVFrdrw+R7HxGnn3/5Iipk9CTyZ4N4/4pRlxlzPIXcX3ujyh4kTOJnZd7jVJxFuLKht5jaXy7NJX8x7WYLLR8lzr7TYsWMHHTt2pFGjRkyaNIkRI0Zw33338c0337B69eqcnWvXr1/PX/7yF3744Qe2b99O//796dOnTylbHwgEAgUjjHwEAmWAiMJphOOOO453332XJk2a7FJv5MiRtGnThrlz55Kdnc0///lPtm7dWtLmBgKBwG4Rgo8iQFJtSVfmU6eppIRbw8fUWxBbbmZLzKxtvGfyaS9L0oh86rSL2hE2csxKVN+8jkZJkIptkt6UtE7SpBTaqytpqqSN+b2XkiKicHrZZZfllB1++OE0bdo0T11JbNiwATNj48aN1KlTh0qVwgBmIBAoX4T/tYqG2sCVwENJ6jTFJWc+WwL2FAgrw9LlKdo2HNgb+FsKTW7G5Zy0JWrqJT9KS+E0ln79+nH22WfTsGFDNmzYwLhx43ZZihsIBALlgRB8FA3DgIO9nsU7vuxM3LLP28xsnK/T2td5EpiI07Ko7uv3M7MP8+tI0kzgUr9qBK8i2h8nKPY4bkO234HLzWxezLOj8eqn/nqjmdWQlAkMBdbhhLfG47QwrgWq4ZbFfiOpPvAI0Ng3eV2U7kisnSeRmwRquNyPI4H+ZtbN1xmBE6AZLWkJTuPjTGA7TgztduAQYLiZPZLonZjZZO9DrA15FF69HsgHkg5J1F7U86WucLp582amT59OrVq1AJg2bRr16tXj2Wef5ccff+Syyy5j1KhRVK9ePUEveSmTaoeFIF38gPTxJV38gOBLsRNPeSwchVIfXeDPz8MFIBWB/YEfgAa41RmTop7ZG9jLnx+KV4GLbitBX38HhvrzBsAif/4gcLM/7wzM8edZ5CqQjgZ6RrW10f/MxAUeDXCqp8uj+rgWuM+fPwsc788bA18ksfNVctVYa+AC3dh3MAInuQ5OqbWvP78XmIdbklsfJwiW32cQ23Zchdeo+znvJZWjNBROzcyaNGmyi4Jp165d7b333su5Pvnkk23WrFkF6jNdlBvTxQ+z9PElXfwwC74UFQSF0xLjeOA5M9thZiuBacBRcepVBh71y0qfB9qk2P543BJWgAvIXdlxPF4V1MymAHUl1SyA3R+b2Qoz2wJ8A7zty+eTu6LmFGCEH715BagpKVH+x3TgHknXALUttU3mXonqc5aZbTCz1cAWSbUL4AskVngtUyRSOE1E48aNmTx5MgArV65k0aJFNG/evKTMDQQCgSIhBB+lx9+BlUAHoCPuL/V8MbPlwFpJ7XH6FvGk1ROxHf+ZS6oQ02es4me0Gmhkeq4CcIyZZfijkZltTGDnMOAy3LTNdEmtovv3lFsF0uLmgQce4MADD2TZsmW0b98+Jxn1pptu4sMPP6Rdu3Z06dKFO+64I2cZbiAQCJQX9qj/0IuRDbgpAoD3gb9JehKog8t1GIATzdon6plaOIXOnZJ646ZpUmUccD1Qy3LzOt7HqYLe6vMf1pjZrzHS20tweRfjgbNxoy8F4W3galyCJ5IyzGl55EHSweaSRef73ItWuA3c2nhF0mq4XXbjqaMWBXEVXsvi6EeEaIXTa665hmuuuSZPnYYNG/L222/nKQ8EAoHyRAg+igAzWytpul8i+wYuX2EuLtHyejP7SdJaYIdX+xyNWxnzgqSLgTcp2P4sE3CJlLdGlQ0BHpc0D5dw2jvOc48CL3sbCtonwDXASN9HJeA94IoEda+TdDJu1GIh8IaZbZE0HlgAfAd8VsD+4yLpfVxwU0PSMlxC7luSIgqv1XDS8qcAG31ya02giqQewGlm9nlR2BIIBAKB/AnBRxFhZrEaHgNi7m/DJYJG0z7qfKCvt4R8loD6XJJKMWU/Az3i1B2NC3Yiz0Urnkb6zAayo57JjDrPuWdma3BTPfliZlcnKL8eN2oTW940ns2x9xK0eUKC8rgKr/m1V9LEqpt+9913/OlPf2Lt2rUceeSRPP3001SpUoV77rmHUaNGUalSJerXr8/jjz+eR4QsEAgEygMp5XxIOtgPlSMpU9I1hUgADAQCcYhVNx04cCB///vfWbx4Mfvuuy+PPfYY4ITHPvnkE+bNm0fPnj25/vo8MVwgEAiUC1JNOH0BN2VwCPA/4CDKoFhWQUmkJloC/TaUlGf/kZg6p8eoem6UNLUAfWSmovi5u0jqE0eBdGQx9JNQ6VTSGZIWSVosaVA+7ZQphdNYdVMzY8qUKfTs6RY09e7dm5deegmAk08+mb333huAY445hmXLlpWKzYFAILC7pDrtstPMtks6B3jQzB70u4kGCoGZ/UjuctlEdd4C3opcezGxAQkfKCXM7AngiRLoJ67SqaSKwEjgVGAZ8LGkV5LkcBRK4bS4iFU3Xbt2LbVr186RTD/wwANZvnx5nucee+wxzjzzzBK1NRAIBIqKVIOPbZL+jEti/KMvK+hKiRJB0jBgqZmN9NdDcImV+5FXdTT6uSygo5n189eTgLvMLFvSRtwurF1x27T/G7gTJ7R1nZm94r8Eh+HErqoCI83svwlsbIoTxGrr++2BU+E8FLgLtwT2r7jlpl19PgfAXyWNwn1ul5jZR5I64ZJP98IlVfYxs0Ux/cWt4/s+Gyd4djAw0edkIOkM4D+4VThrzKyLpOo4MbO2uM9/iJm9nMDHw3BBSRXcCNt5wLaI375Of6CGmQ3xwdVnwAn+XVwM/AunuDrOzBLtcNsJWGxm3/o2xwLdgc93V+E0mqKWV18y7CwmTZrEfvvtx5FHHlkg9cExY8bwySefMG3atCKzJxAIBEqSVIOPPrhVDf9nZt9JaoYXtCqDjAPuw/01DE6I6w7gNJymRj3cX8fvFaDN6sAUMxsgaSJwG+4v7TY4qfRXgEuB9WZ2lM+PmS7pbXPbxOdHW+BwXHCwGBhoZodLuhf3JXyfr7e3mWVIOhEnpd4W+BI4wY9MnYILGM6LaT9ZnQzf9xZgkaQHcaMDjwIn+s+7jq97g38Pl/icn48kvWtm8VbNXAHcb2bPSKpCruJrMraaWUdJ1wIv45YF/wx8I+leM1sb55lGwNKo62XA0b7PccCFfqltTVzglTLFKa+enZ3Nc889x9tvv82LL77I1q1b+f333/nTn/7E6tWrmTx5MhUrVmThwoVUq1YtJziZPXs2DzzwAPfddx8zZswoVN9lUmq5EKSLH5A+vqSLHxB8KW5SCj7M7HNJA/F7evgv1DuK07DCYmafSdpPUkOcNPcvuC/Y58xsB7BSUkR1dF7ilnZhK25pKjj1zS1mts2rkzb15acB7SVFplNq4UYyUgk+pvq/yDdIWo+TJo/0Fb0i5jnv43uSavoAYB/gSUmH4kZ14o1I1UpSZ7KZrQeQ9DnQBNgXeC8SOEWNvJwGnO1HLMAFS42BL+L0OQO4QdKBwItm9nWM5kg8ohVOF5rZCm/Xt7g8o3jBRyLyKJwW4Fn8M//D5TjRsmVLu7pX94I2kZSIpge4YOSuu+5i0qRJnH/++axevZo//elPjB07lj59+pCZmclnn33GQw89xLvvvsuhhx5a6H6zs7N36bu8ki5+QPr4ki5+QPCluEl1tcsfgTn4L2BJGZJeSfpQ6fI8LqeiIAqgydQ3t3mNeohS3zSzaOVNAVdHqX82M7NU1aBSURcFFzgQc30rLnhpi5sSi1UNJZ860X3vIHlAKuC8KB8bm1m8wAMzexY3pbMJeF1SZ4pH4XQ5LjCJcKAvK7fccccd3HPPPRxyyCGsXbuWSy+9FIABAwawceNGzj//fDIyMjj77LNL2dJAIBAoHKlOuwzBza1nA5jZHElleUOJcbhpg3rAScCxxFcdjf7yWwJcKSc73gjnb0F4C+graYofFWkBLE8wJVFYLgSmSjoeN8WzXlItcr9ssxI8l0qdaGYCD0lqFpl28aMfbwFXS7razEzS4WYWN/HY/358a2YPSGqMG8F5H9hPUl1gI9CN3BGlwvIxcKifClwO/Am4CPiacqRwGq1u2rx5cz766KM8dd59990StioQCASKh5QTTv0XXXTZzmKwp0gws4X+y2a5ma3weRrHkld1tGnUY9NxUySf46YRPi1gt6NwUzCfyr2o1cQR/dpNNvtVRpWBS3zZnbgplRuBRBmRqdTJwcxW+3yHF30wtgqX43IrLv9kni//DhdAxOMCXILsNuAn4D8+KLsF+AgXKHyZny0p2LpdUj9cYFQReNzMFgIoKJwGAoFAmUS5swlJKkmPAZOBQbhExWuAymaWSFo7EEgrWrZsaYsWLcq/YjmgLM7/FoZ08QPSx5d08QOCL0WFpNlm1jG2PFWRsauBw3Dz8M8C64Hrisy6QGAPZPPmzXTq1IkOHTpw2GGHcfPNNwMwZcoUjjjiCNq2bUvv3r3Zvt3NFA0fPpyMjAwyMjJo27YtFStW5Oeff07WRSAQCJRJ8p128foVr5nZybilloEUkdSOvEuSt5jZ0aVhT3Eg6XTyrnz6zszOKeJ+6uJG32LpkmAJbpmnatWqTJkyhRo1arBt2zaOP/54Tj/9dHr37s3kyZNp0aIFgwcP5sknn+TSSy9lwIABDBjgdOZeffVV7r33XurUqZNPL4FAIFD2yHfkwy9P3ekTGwNxkFRb0pWx5WY2P7IyBJf/cWd+gYeKWPJdUlZxyoib2VtRq18iR5EGHr6ftXH6yYgEHn7p8bL8fJXUStIMSVuilgyXCpKoUaMGANu2bWPbtm1UrFiRKlWq0KJFCwBOPfVUXnjhhTzPPvfcc/z5z38uUXsDgUCgqEg14XQjMF/SO0Rtw25m1xSLVeWP2sCVwENJ6jTFrcIo93vilFFuBVIRjvsZl7PUoyCNF4fCKbgdbY888kgWL17MVVddRadOndi+fTuffPIJHTt2ZMKECSxdunSXZ3///XfefPNNRowo9a1pAoFAoFCkGny86I9AfIYBB0uaA7zjy2Kl3IcBrX2dJ4GJuCmZ6r5+PzP7ML+OJM0ELo1a0ZEN9Ae+xameNgd+By43s3kxz47GyZtP8NcbzayGpExgKLAOJ2c+Hif0dS1QDehhZt9Iqg88ghebw0nLT09g50k4aXP8ezgRp1ja38y6+TojgE/MbLRfgfKcf2/bccqitwOHAMPN7JEk7+RInHrqm0DHqPI8EvFmtgpYJemsRO1FPV+sCqcR7rvvPjZu3MhNN91Eq1atuP7667nkkkvYtm0bHTt2ZNOmTbvUnzJlCq1atWLevFQ18nalLKodFoZ08QPSx5d08QOCL8WOmYVjNw/cqMYCf34eLgCJyIn/ADTA7fkyKeqZvYG9/PmhuC/hXdpK0NffgaH+vAGwyJ8/CNzszzsDc/x5FjDCn48Geka1tdH/zMQFHg1w+9Isj+rjWuA+f/4scLw/bwx8kcTOV4Hj/HkNXKAb+w5GAFn+fAnQ15/fi1Of3QenUrsyST8VcPozB8b4Wh8nu97MX9eJeW4ILhBK6TNu0aKFFTdDhw614cOH71L21ltv2fnnn79LWY8ePeyZZ54pdD9Tp04t9LNliXTxwyx9fEkXP8yCL0VF5Lst9khV4fQ7Sd/GHqk8uwdyPF7K3cxWAhEp91gqA496ifbncfvEpMJ4cnfEvQCYENXv0wBmNgWo6/czSZWPzWyFmW0BvgEi6qzREvKnACP86M0rQE1JNRK0Nx24R9I1QG1LTdwrWl59lpltMLPVwBYvJR+PK4HXzSx2f/ljiC8RX2ZYvXo169atA2DTpk288847tGrVilWrVgGwZcsW7rjjDq64IndF+/r165k2bRrduxet1HsgEAiUJKlOu0Sv0d0LOB+nFBooPH8HVuI2u6uA28wtX8xsuaS1ktrjFE8LorWSI2/uRcKqRN1LReK9AnCMmeVrq5kNk/Qabifg6X5VTHHIqx8LnOATfmvghMM24oKfMs2KFSvo3bs3O3bsYOfOnVxwwQV069aNAQMGMGnSJHbu3Enfvn3p3LlzzjMTJ07ktNNOo3r16klaDgQCgbJNqhvLxS5lvE/SbGBw0ZtULtmAmyIAJyEeT8q9UVQdcJLny8xsp6TeuGmaVBkHXA/Usty8jveBXsCtPodjjZn9GqNKuwSXdzEet+9KvE3okvE2TvNlOLg9fsxsTryKkg42s/m4ROWjgFbAbKCN3K6/1YAuwAcFtGEXzKxXVJ9ZQEczG+TzU+JJxJcZ2rdvz2ef5VWnHz58OMOHD4/7TFZWFllZWcVsWSAQCBQvKQUfko6IuqyAGwlJddQk7TGztZKm+yWyb+DyFWKl3NcCOyTNxeVePAS8IOliXKJkQfaAmYBL5rw1qmwI8LikebiE095xnnsUeNnbUNA+wa0SGen7qIRbXZJo5OU6SSfjRi0WAm+Y2RZJ44EFOGn2uPvCFAWWQCJe0gHAJzh59Z2SrgPaWCF2vQ0EAoFA4Ug1gLg76nw77ovjgqI3p/xiZhfFFA2Iub8NlwgaTfuo84G+3hKgbT59rSTms/N/1feIU3c0LtiJPHdMnD6z8ZsG+uvMqPOce2a2BjfVky9mdnWC8utxozax5U3j2Rx7L58+Y597AxcMRtf5CZecWqps3ryZE088kS1btrB9+3Z69uzJ0KFDmTJlCv3792fr1q0ceeSRPPbYY1SqVAkz49prr+X1119n7733ZvTo0RxxxBH5dxQIBAJlkFTl1S81s5P9caqZXQ5sLU7DAoF0JqJuOnfuXObMmcObb77Jhx9+SO/evRk7diwLFiygSZMmPPnkkwC88cYbfP3113z99df873//o2/fvqXsQSAQCBSeVIOPCSmWlSuKWk20AP02lJT0/Uk6XdKcqGOjpKkF6CNT0qTdtzbffvrE2DlH0shi6KddnH5m+XuPS1qV6mcp6U1J60ri/SSxoUDqpi+//DIXX3wxkjjmmGNYt24dK1asKC3zA4FAYLdIOu0iqRVuQ7laks6NulWTvCsVAiliZj+Su1w2UZ23cNvEAzliYgMSPlBKmNkTwBMl0M98ICPB7dE4zZCnUmxuOE5n5W+p9l8cCqcFUTddvnw5Bx10UM7zBx54IMuXL6dBgwZFZlMgEAiUFPnlfLQEuuHkw/8YVb4B+H/FZNNuIWkYsNTMRvrrIbjEyv3Iqzoa/VwWbqVEP389CbjLzLL90s2HcctGVwD/Bu7ECW1dZ2av+A34huGEtKoCI83svwlsbIoT22rr++2BUzo9FLgLtwT2r7jlpl2jVmn8VdIo3Od2iZl9JKkTLvl0L2AT0MfMdtn7PVEd3/fZuC/ig4GJPicjrjqopOo4MbO2uJUyQ8zs5QQ+HoYLSqrgRtjOA7ZF/PZ1+gM1zGyID64+A07w7+Ji4F84xdVxZnZjvH4AzOw9/05jbTgEp8haH9gBnG9m35jZZL8iKCkloXCaqrrp2rVr+eyzz3J2uP3ll1+YPXs2GzduLHDfZVLtsBCkix+QPr6kix8QfCl24imPxR7AsanUKwsHcDgwLer6c9zKj3iqo03JVSbNwqtj+utJQKY/N+BMfz4Rt+S0Mk6jI6Ikejlwoz+viltR0SyBjbH9LiZXzXM9cIXlKn1e58+zgUf9+YlRz9cEKvnzU4AXLFe1dFI+dbJwsuy1cIHJ98BBJFAHxQUjf/HntYGvgOoJfHwQ6OXPq+CW1ub47cv74wKYiH93+PNrgR/JVVxdBtTN53PfpW1fNgs4x5/vBewddS/n/aRyFLfCaX7qppdffrk9++yzOfdatGhhP/74Y6H6ShflxnTxwyx9fEkXP8yCL0UFu6NwCnwm6SpJD/n59cclPZ7isyWKmX0G7OfzKjoAv+CG61NRHU3EVtzSVHDqm9PMrV6JVv88DbjYq3/OAuriRjJSYarlqnmux0mTR/pqGlXvOe/jezh10dq4wOF5n+9wL26aLJZkdSab2XpzwmGfA01IrA56GjDI+5iN+0JvTHxmAP+WNBBoYmabUngP0QqnCy1XcfVbXFCUMpL2ARqZ2UTvw2Yz+70gbRQnBVU3Pfvss3nqqacwM2bOnEmtWrXClEsgECi3pLrU9mngS+B04BacmNUXxWVUEfA8LqfiAJwgV7MUnkmmvrnNR3AQpb5pTiAs8g4FXG0uV6OgpKIuCm4EhpjrW3HByzl+6iE7TvvJ6kT3vYPkvxMCzrOYaZ14mNmzPiH0LOB1SX/DjZQUtcJpuaSg6qZdu3bl9ddf55BDDmHvvffmiSeKPc0mEAgEio1U/0M/xMzOl9TdzJ6U9CxOUbOsMg4nqFUPOAknwR1PdTT6y28JcKUXpGoEdCpgn28BfSVNMbNtkloAy82soEJeybgQmCrpeGC9ma2XVAu3ERy4aZR4pFInmpnEVwd9C7ha0tVmZpIO9yNNeZDUHPjWzB6Q1BinafI+blSqLrARl0/0Zrzndxcz2yBpmaQeZvaSV1WtWFZGPwqqbiqJkSOLfBFRIBAIlAqpTrts8z/XSWqL+zLbr3hM2n3MbTe/D+7LfwUuTyOiOjoFrzoa89h0nHja58ADwKcF7HaUf/ZTP73xX4r+r/XNkj7DJVFe6svuBG735Yn6S6VODn76J6IOOhcXzIEbQakMzJO0kF0VVmO5AFjgp2jaAk/5qapbgI9wOThf5mdLKkh6DjfN09IHHJF381fgGq/I+iFuJAxJ7+NGx7r4+qcXhR2BQCAQSJF4iSCxB3AZsC9uFOFbnFT1Fak8G45wpMNR1AmnmzZtsqOOOsrat29vbdq0scGDB5uZ2bvvvmuHH364dejQwY477jj7+uuvzcxs2rRpdvjhh1vFihXt+eef362+0yWRLl38MEsfX9LFD7PgS1FBgoTTVDeWG+VPpwHNiz4ECgT2LCIKpzVq1GDbtm0cf/zxnHnmmfTt25eXX36Z1q1b89BDD3HbbbcxevRoGjduzOjRo7nrrrtK2/RAIBDYbVLdWG5/3DLLhmZ2pqQ2uOW3jxWrdeUcSe1wybrRbDGzo0vDnmR4/ZB7zOzzAj53OnBHTPEO4I/mxNQKY8upOM2UKriVRgNwU2aT41TvYn7XZUmvAM3N64gkaf9N3IqeD8ysW2Fs3F3iKZxKQhK//ur2uFu/fj0NGzYEoGnTpgBUqJDqTGkgEAiUXVLNSRiNE4y6wV9/hcsDCMFHEiy5KmeZwswuK+RzuyixQo4aa0OcVkdhWIMPXnyO0Vtm1ogk79Ir8KaquFVghdPiIFbh9Oijj2bUqFF07dqVatWqUbNmTWbOnFmaJgYCgUCxkGrwUc/Mxkv6F4CZbZe0oxjtChQjXql0PG5314q4xNG+ONGvhrikUHDCYFXMrJmkI4F7gBq44CDLXDJvbNs9gY7AM5I24VYaDcAp5FbDJX7+zczMByn9zewTSfVwc4NNbdcVNAuBapKqmtP8iOdPDeAfuCTZ8VHlu6VwGk1RyqsvGXYWABUrVmTOnDmsW7eOc845hwULFnDvvffy+uuvc/TRRzN8+HD+8Y9/MGrUqHxaDAQCgfJFqsHHb355pAFIOgYnhhUon5wB/GhmZwH45bp9AczsFbzYl6TxwDRJlXGKpd3NbLWkC4H/Ay6JbdjMJkjqhw8qfDsjzOwWf/40bontq7HPJuA84NNEgYfnVuBuIHYZ7TPAMDObKGkvUl/dhbe1WOTV48kcN23alBEjRjBr1qwcSfXGjRszcuTIXer/9NNPLFy4kHr16hW6/zIptVwI0sUPSB9f0sUPCL4UO/GyUGMP4AjcUtT1/udXQPtUng1H2TuAFjhdkzuAE3xZNm5vm0id64En/Xlb4Fdgjj/mA28naT+2rfNwqq/zcXojg2Lr4TRZlsS0cxjwDXBwkr4ygFf8eVNyZef3AZYleS6TUpRXX7Vqlf3yyy9mZvb777/b8ccfb6+++qrVrVvXFi1aZGZmo0aNsnPPPXeX53r37h1Wu3jSxQ+z9PElXfwwC74UFRRmtYukxmb2g5l9Kukk3EZzAhaZ02wIlEPM7CtJR+A2yrtN0i6JnJJOAc7HibGB+8wXmtmxBe3Ljzg8hAsylvqN/iLibtGqsnvFPHcgTp/lYjP7JkkXxwIdJS3BjeTt56dz/pjkmVInkcLpo48+ynnnnUeFChXYd999efxxt4vBxx9/zDnnnMMvv/zCq6++ys0338zChQtL2YtAIBAoHPlNu7yEG/UAt7PoecVrTqAkkNQQ+NnMxkhah9NxidxrAowETrfc/VgWAfUlHWtmM/w0TAtzYm7x2IAbeYDcoGKNz83oCUzwZUuAI3GiYz2jbKgNvIYbIZmezBczexi343D0bsGZ/rrcKZyec845nHPOOXnKjzrqKJYtW1YSpgUCgUCxk98cuKLOg75H+tAO+Mirj94M3BZ1Lwu3Kd5LkuZIet3MtuKCgzu84ukc4A9J2h8NPOLb34KTul+AWxXzcVS9u3CS9J/hpl0i9AMOAQZ7G+ZIKoyiblA4DQQCgTJIfiMfluA8UI6xOMtjcTkQAJ8AQ+M8M4fcaZj82n8BeCGq6EZ/xNb7ErfnS3Q9zOw2dg2IUsLMluDyUyLXXwOd49Q7oaBtBwKBQKDoyC/46CDpV9wISDV/jr82M6tZrNYFAmnK5s2bOfHEE9myZQvbt2+nZ8+eDB06lBNOOIENGzYAsGrVKjp16sRLL70EuFUy1113Hdu2baNevXpMmzatFD0IBAKBwpM0+DCziiVlSKD8IWkkcFxM8f1mViz7vUuaBVSNKf6rOTG3ckUiefX338/dLPq8886je/fuAKxbt44rr7ySN998k8aNG7Nq1arSMj0QCAR2m6LedTVQTimMvLqZXZWgrSzcUtwik1c3symWQJZeUhVgBG7qaCdwg5/6SdT+4zitkVWWjxR7cZFIXj3Cr7/+ypQpU3jiCRfHPfvss5x77rk0btwYgP32K7ObSgcCgUC+hOAjABReXj0BWbgE0yKTVwcaJal/Ay6QaCGpAlAnn/ZH44KVp1I1qDgUTuPJq0d46aWX6NKlCzVrupnNr776im3btpGZmcmGDRu49tprufjii4vEnkAgEChp5DRAAnsSJSCvPhonJlYoefWY9gSsBRpYYnn1pUArM/stpnx/nLx6ZKVWXzP70N9riluWm3DkI0bh9MjB9z2aqGqBaNeo1i7XGzdu5KabbuKaa66hWbNmAAwcOJCuXbty0kknAXD//fezaNEi7r77brZu3cpVV13F7bffzkEHHVTg/jdu3Jgz6lKeSRc/IH18SRc/IPhSVJx88smzzaxjnhvxlMfCkd4HTnH00ajrWsSokvry8cBVQGVc0FDfl18IPJ6k/V3aAupEnT+NG9XYpR5xFE59eU/g3SR91QaW4gKjT3FLaPf398YB1/nzikCtqOea4tVQUzmKWuE0lqFDh9rw4cPNzGz16tVWp04d27RpU87922+/3QYPHpxzfckll9j48eML1Ve6KDemix9m6eNLuvhhFnwpKkigcBr2594zmQ+cKukOSSeYWZ59eiRdD2wys5E4Zdu2wDteu+NG3KhJqpwsaZak+bilr4el8pCkw3AS8Ml2n63kbfnQzI4AZuD0Q/B9PQxgZjvi+VlarF69mnXr1gGwadMm3nnnHVq1agXAhAkT6NatG3vtlSv62r17dz744AO2b9/O77//zqxZs2jdunVpmB4IBAK7Tcj52AOx9JJXX4vbUO5Ff/08cGlB7SxpEsmrA4wdO5ZBgwbtUr9169acccYZtG/fngoVKnDZZZfRtm2p5MoGAoHAbhOCjz2QNJNXN0mv4la6TAG6AJEVO5NxuSz3SaoI1Cgrox+J5NUh/q63AAMGDGDAgAHFaFUgEAiUDGHaZc8k3eTVBwJDvIz6X4F/+vJrcVM+84HZQBsASc/hpmdaenn1Mj9SEggEAulEGPnYA7E0k1c3s+/j2WZmK4Huccr/nGrbxUVBFU6zs7Pp3r17zmqYc889l8GDB5emC4FAIFBoQvARCJQCBVU4BTjhhBOYNGlSaZgbCAQCRUqYdilFJNWWdOVutpElaUQR2dNQ0oT8a+bUHxk1LRI5+hSFLQn6mxWnv3aSHpM0V9I8SRN8bkmydh6XtErSguKyNT9SVTjt0aNHKVkYCAQCxUcY+ShdagNX4laD5CCpkpltL2ljzMmh98y3Ym79uPLqxYUlllf/u5n96s/vweWMDEvS1GjKmcIpwIwZM+jQoQMNGzbkrrvu4rDDUlqxHAgEAmWOoHBaikgai8tJWARsAzYDv+DUOltIegk4CLdi5H4z+59/rg/wL2AdMBfYYmb9JNXHKXo29l1cl2i1iKSTgPv9peFyJuriVT/9Xi8RVbpGwAgzGyppAHABboO3iWZ2c4L286iomtk4SUtwy27XSOoI3GVmmX4JbjOcGmlj4O/AMcCZOLXUP5rZtnzep3CB3BIzuyOdFE5/++03KlSoQLVq1Zg5cyYjRoxgzJgxheo/XZQb08UPSB9f0sUPCL4UFUHhtAweRKls4hI+fwOaRd2v439Ww60WqQs0AH4A6uM2XpuOCwwAngWO9+eNgS+S9P0qcJw/r4EbBcuxJ6peE+AL//M04H843Y8KwCTgxATt51FR9T+XAPX8eUcg258PAT7Aqal2wGl3nOnvTQR65PMunwBWAlOBvX1Z2iicxtKkSRNbvXp1ofpKF+XGdPHDLH18SRc/zIIvRQVB4bRc8JGZfRd1fY1f2joTNwJyKHA07gt7tbklsOOi6p8CjPBLXF8BaibJf5gO3CPpGqC2xZnm8QJhzwNXm1tRcpo/PsNJmbfyNsUjXxXVOLxhbnRjPi5YeDOqrabJHjSzPrh9ab7Ayb9DGimc/vTTT5GgiY8++oidO3dSt27dErc7EAgEioKQ81G2yNkYTVImLpg41sx+95uw7RX/sRwqAMeY2eb8OjKzYZJew6mcTpd0Om7aJ5pHgBfN7N2IWcDtZvbfFNrPo6JqZreQRNUUpwmCme2UtM0i37awkxR+V81sh5/Kuh43ElJmKajC6YQJE3j44YepVKkS1apVY+zYsbskqAYCgUB5IgQfpUu0EmgstYBffODRCpf/ADALuF9SXeBXnAz6XH/vbeBqYDiApAxz+hx5kHSwmc0H5ks6CjeKMSfq/lXAPmYWnbj5FnCrpGfMbKOkRsA2M1sVp/1EKqpLcKqmb+CmZnYLn+dxsJkt9udnA1/622mjcNqvXz/69etXzFYFAoFAyRCmXUoRM1uLG3VYgA8YongTqCTpC9zKjZn+mRW4/IgZuKmTL6KeuQbo6Jecfg5ckaT76yQt8Kqg23DBQDT9gXZRS1qvMLO3cXklM7xq6AQSB0+JVFSH4oKnT4AdSexLFQFPenvm43JibvH3gsJpIBAIlEHCyEcpY2YXJSjfglvpEe/eE8SZVjCzNeTmO+TX79Vxipfgdq/FzJoleO5+clfJJGs/nooqZvY+0CJO+ZCY6xqJ7sXU2wkcl+BemVU4DQQCgT2ZMPIRCJQCmzdvplOnTnTo0IHDDjuMm292K5ZPOOEEMjIyyMjIoGHDhnlExj7++GMqVarEhAkpa8EFAoFAmWOPHvlIReuhmPptCDxgZikLevmE0/5m9kmK9TNxUycv4KYfopluRSQQ5nNPJse51cVPKxUZkibitECiGYjLgxmFG7Ux4BIzm5GknTdxOTQfmFm3orQxVQojr75jxw4GDhzIaaedVhomBwKBQJGxRwcfpYUVUEl0N/uKO0VThO2vBTKKq/2Yvs6JVy7pSeBNM+spqQqwdz5NDfd1/lbEJqZMqvLqTzyR+9E9+OCDnHfeeXz88cd52gsEAoHyRNoFH5KGAUvNbKS/HoJbwrofLofCgNvMbFzMc1k45c1+/noSTn0zW9JGnF5EV2AF8G/gTpyQ13Vm9opfTTEMJxZWFRiZaElq9IiL77cHUB2nmXEXTjzsr7ilp13N7Gf/6F+98mgl3F/3H0nqhMvB2AvYBPQxs0Ux/cWt4/s+G/dFfDBOsfR6/8wZwH9wehtrzKyLVy19EDfCUBkYYmYvJ/DxMFzQUwU3vXceLrE1Z6RJUn/cCpQhfmTnM+AE/y4uxqm4tgPGmVmeXXF9G7Vw6qxZAF77ZKu/dwhuuXB9XHLr+Wb2jZlN9iNDKVPa8urLly9n4sSJTJ06NQQfgUCg3JN2wQdOdOs+YKS/vgC4AyeO1QGoB3ws6b0CtFkdmGJmA/zQ/23AqbjVE0/iBL0uBdab2VGSquJWsbwdIxqWiLbA4bjgYDEw0MwOl3Qv7kv4Pl9vbzPLkHQi8Lh/7kvgBDPbLukUXMAQu4Q1WZ0M3/cWYJGkB3F6H4/i1Eu/k1TH173Bv4dLJNXGrWZ518x+Iy9X4CThn/GjERWB/fN5D1vNrKOka4GXcUtyfwa+kXRvgmmcZsBq4AlJHXCrWq71Nj0DDDOziV4wrUA5TjHy6gxuVzTb7UQvpb3vvvty5NVbtWqVI68+cuRIunbtmlN3yJAhXHjhhbz33nv89NNPLFy4kHr16hWq/40bN8ZdzlveSBc/IH18SRc/IPhS3KRd8GFmn0naz+dV1MftlZIBPGdmO4CVkqYBRwHzUmx2K7uqbW4xs21+CWdTX34a0F5SZDqlFm4kI5XgY6qZbQA2SFqPkz6P9NU+qt5z3sf3JNX0AcA+uKWmh+JGdSrHab9WkjqTI9oXfnluE2Bf4L1I4BQ18nIacLYfsQAXLDVm1+W+EWYAN0g6ECdU9nUKolivRPm90C8rRtK3OIXXeMFHJeAInArrLEn3A4Mk3Qk0MrOJ3od8hddiMbeXzv8AWrZsaVf3yrNwpsj49NNPWbt2LX369GHNmjUsXryYgQMH5qicfv/999x5550ArFmzhk8//ZQOHToUatfb7OxsMjMzi9D60iFd/ID08SVd/IDgS3GTdsGH53lcTsUBuJGQuMtGY4hW3oRd1Tdj1TajlTgj71C4L8A8y0tTYEvU+c6o61hlz9hdAA24FRe8nOOnc7LjtJ+sTnTfO0j+OyHgvNhpnXiY2bOSZgFnAa9L+hvwFYnfcbQt0e8gcp3IrmXAMjOb5a8nAIMS1C0zrF69msqVK1O7du0cefWBAwcC8eXVv/suN4bNysqiW7duhQo8AoFAoCyQrkttxwF/wgUgzwPvAxdKqii38+uJwEcxzywBMiRVkHQQ0KmAfb4F9JVUGUBSC58jUZRc6Ns+HjfFsx43qrHc389K8FwqdaKZCZwoqZnvLzLt8hZwtVcSRdLhiRqQ1Bz41swewE2htMdt/LafpLp+amq3V5qY2U/AUkktfVEX4HM/krRMUg9vT1VJ+SWilhgrVqzg5JNPpn379hx11FGceuqpu8ir//nPQYokEAikL2k58mFmCyXtAyw3sxU+T+NYnAy5Adeb2U9+FCDCdNwUyee4aYRPC9jtKNwUzKf+y3k1LpG0KNks6TPctMklvuxO3JTKjUCijMhU6uRgZqt9vsOLkioAq3A5Lrfi8k/m+fLvSBxAXIBLkN0G/AT8x09V3YIL/JaTK4O+u1wNRHJLvgX6+PK/Av/1fW7DSdF/K+l9nJx8DUnLgEsLOWJVaAoqrx7N6NGji96gQCAQKEGUO5sQCAQS0bJlS1u0KN/ZpnJBWZz/LQzp4gekjy/p4gcEX4oKSbPNrGNsebpOuwQCZZaCqpuaGddccw2HHHII7du359NPCzooFwgEAmWLtJx2KStIagc8HVO8xcyOjle/PCLpdNxS5mi+SyQIthv9lJiSanFTUHXTN954g6+//pqvv/6aWbNm0bdvX2bNmpWo+UAgECjzhOCjGPFb1mckuu+Xyl5kZg8Vto9YcbTdoTCy74k2kCtqkimp+sTYsUBdnM7HX73YWFwkPY7LVVlV0tL6vv8CqZu+/PLLXHzxxUjimGOOYd26daxYsYIGDRqUtOmBQCBQJITgo3SpDVwJ7BJ8SKpkZkWjaFUASlL2vYi5A7jXzMZKegQn+PZwkvqjgRHAU6l2UNQKpwVVNz3ooINy7h944IEsX748BB+BQKDcEoKP0mUYcLCkObjVGJtxomitgBaSXsKJa+2FUwv9H4CkPjjp8XW4FTxbfHl9nJx4Y9/+dWY2PV7Hkk7CSa6DWwF0Im7kICL7PgqIJAk1AkaY2VBJA3ArWari5NhvTtB+dWA8cCBO3fRWMxsnaQlupGaNpI44CftML4PfDGju7f87bvO3M3ErY/5oZtvi9COgM3CRL3oSGAI8LGl//z6a+3t9zexDL9LWNJ7dMW0Xq8Jpquqma9eu5bPPPmP7dtf/L7/8wuzZs9m4cWOh+i+LaoeFIV38gPTxJV38gOBLsWNm4SilA7c0d4E/z8TtQdMs6n4d/7MasAAXHDQAfsCpt1bBLREe4es9CxzvzxsDXyTp+1XgOH9eAxeI5tgTVa8JbulxE5zC6f9wYmMVgEk4CfZ47Z8HPBp1Xcv/XALU8+cdgWx/PgT4ALeMuAPwO3CmvzcR6JGgn3rA4qjrg6Le6ThcAAYuAKoV792ncrRo0cKKi6FDh9rw4cPNzGz16tVWp04d27RpU879yy+/3J599tmc6xYtWtiPP/5Y6P6mTp1a6GfLEunih1n6+JIufpgFX4oK4BOL839qWO1StvjIdt0L5hpJc3GiXwfh5NqPxn1hrzaX1xC9Qd4pwAg/kvIKUFNSjQR9TQfukXQNUNviTPP4/VCexym3fo8LPk7DbQD3KW6E5tAE7c8HTpV0h6QTzEu458Mb5kY35uOChWhJ+6YpPB9LZ/z0i5ntSNGGYmf16tWsW7cOIEfdtFWrVkB8ddOzzz6bp556CjNj5syZ1KpVK0y5BAKBck2Ydilb5GzQ5nddPQU41sx+97u+xsqRx1IBOMZS2MfEzIZJeg23U+90v2ol9rlHcPuyvBsxC7jdEuzWG9P+V5KO8O3fJmmymd3CrjL2ceXVzcnWx0raJ/pdXQvUjsqTOZBcNdcyyYoVK+jduzc7duxg586dXHDBBbuomw4atKs6fNeuXXn99dc55JBD2HvvvXMSUQOBQKC8EoKP0mUDbmO4eNQCfvGBRytc/gPALOB+v/T0V5xq51x/722c2udwAEkZZjYnXuOSDja3Gme+pKNwoxhzou5fBexjZsOiHnsLuFXSM2a2UVIj3L43q+K03xD42czGSFoHXOZvLcHtVvsGeXffLTBmZpKm4hJlxwK9cXLu4Jbm9gXuk1QRqFEWRj8Kqm4qiZEjR+atHAgEAuWUMO1SiphbPjpd0gJ8wBDFm0AlSV/gElNn+mdW4PIjZuCmTqJ3lL0G6Chpnt+h9ook3V8naYGkebhk1zdi7vcH2kma448rzOxtXF7JDLkdfSeQOHhqB3zkp4BuBm7z5UNxwdMnuI3sioKBwD8kLcblxTzmy68FTva2zgbaAEh6Dvf+WkpaJunSIrIjEAgEAikQRj5KGTO7KEH5FtxKj3j3ngDyjL2b2Rr85nMp9Ht1nOIlQFt/P+5OwGZ2P7mrZJK1H1f/w8zeB1rEKR8Sc10j0b04z35LnI0AzWwl0D1Oeanu2rZ582ZOPPFEtmzZwvbt2+nZsydDhw7FzLjxxht5/vnnqVixIn379uWaa65h+PDhPPPMMwBs376dL774gtWrV1OnTp18egoEAoGySQg+AoESJpHC6RdffMHSpUv58ssvqVChAqtWudmsAQMGMGDAAABeffVV7r333hB4BAKBck2YdklzJPWJmjqJHHkSCCSNktSmEO3XjdP+D5IO2w2bT5U0W9J8/7OzL58Yp6/TJR3p6y6W9ICi5ULjt/+mpHWSJhXWxt0hkcLpww8/zODBg6lQwf2z3G+//fI8+9xzz/HnP5fqwE0gEAjsNmHkI81JNEUTp95l+dVJ8Fwe2XO/MqdaYdrzrMGJiv0oqS1u+qaRJdgvRtJHwP/DJeO+DpxB3hyWaIYDewN/S9WgklA4/eabbxg3bhwTJ06kfv36PPDAAxx6aO5K5t9//50333yTESNGFIkdgUAgUFqE4GMPJJ76KG5VSH+gIXCLr1oNqGJmzSQdCdyDEyRbA2T55NfYtnvixMOekbQJOBYYAPzRt/ch8De/SiUb6G9mn0iqhxOjaWpm0UtBFgLVJFX1eTCx/TUAaprZTH/9FNADeEPSIbjlwvVxya3nm9k3ZjbZL2XO7z2VqMLp77//zvLly7nrrrt47733OO+883jggQdynpsyZQqtWrVi3rx5u9V/mVQ7LATp4gekjy/p4gcEX4qdeMpj4Ujvgzjqo0A2TvY8ut544Cqc6uiHQH1ffiHweJL2d2kLr9Tqz5/GjWrsUg+nVLokTls9gXeT9NUx+j5wAk4iHtxIyDn+fC9g76h6mZF6qRwloXDasmVL+/bbb83MbOfOnVazZs1d6vXo0cOeeeaZ3e4vXZQb08UPs/TxJV38MAu+FBUEhdNAFPmqj0q6HthkZiOBlrhVMO/4pbM34kZNUuVkSbP8ktfOQEr5ID5v5A4KMD0S9ew+uKmaiQBmttnMfi9oO8VBIoXTHj16MHXqVACmTZtGixa5i4LWr1/PtGnT6N49z+KdQCAQKHeEaZc9EIujPhp9X9IpOPGyEyNFwEIzO7agfXmJ9odwIxxL/QZyEWXThGqnkg7E7elysZl9k6SL5ewaCJVbhdPjjz+eXr16ce+991KjRg1GjRqV88zEiRM57bTTqF69eilaHggEAkVDCD72QJKojyKpCTASON3MNvniRUB9Scea2QxJlYEWZrYwQRfRyq2RoGKN32emJ06cDHLVTj/y5REbagOvAYMswa68EcxshaRfJR2Dm2a5GHjQzDZ4AbEeZvaSpKpAxbIw+pFI4bR27dq89lr8pNasrCyysrKK2bJAIBAoGcK0y55JIvVRgCycSuhLfinr6+Y2sOsJ3CG30d0c4A9J2h8NPOLb3wI8ituV9y3g46h6dwF9JX2Gy/mI0A84BBgctaQ277rTXK4ERgGLgW/IXenyV9zmfPNwOSsHAEh6H7dhXhcfoJyepO1AIBAIFDFh5GMPxOKrj2b6n5/gJNBjn5lD7jRMfu2/ALwQVXSjP2LrfQm0j6mHmd3GrgFRfv19gldmjSn/GpdjElt+QqptBwKBQKDoCSMfgUAJs3nzZjp16kSHDh047LDDuPnmmwG38uyGG26gRYsWtG7dOmeZ7fDhw8nIyCAjI4O2bdtSsWJFfv7559J0IRAIBHaLMPKRAElNcUsx28aUjwLuMbPPY8qzcEmV/Yqg70yc/kW33W2rOJE0ETey8F1U8f3mhM2Ko79ZQNWY4r+a2XxJjXFTLwcBBnQ1syUJ2qmLyzs5ChhdFJ9ZQQjy6oFAYE8nBB8FxAqpBFoWkVTRzHZnZ9n7gcolFSSZ2dFJbj8F/J+ZveMTW3cmqbsZuAk3VZNnuqa4SSav/uyzzwZ59UAgkPaE4CM5lSQ9AxyBU9q8GCffHVHl7AP8C1gHzMUlV8ZF0h9xOQ1VgLVALzNbKekkcneJNWLyKiQdBfwP6BlvyWmC54/EqZRuwCVuTgWuNLOdkjYC/wVOAa7yIzzXeLtm+Xo7JD2MGxmoBkwws5t9f2cA9wG/Ax8ke3lJbMsZ1ZE0AidCM1rSEuA53G6+23Hqord7H4ab2SMJ+mkDVDKzdwDMbGPM+7sfqI77fLqY2QbgA6+AmhJBXj0QCASKjhB8JKclcKmZTZf0OG5VBZAj6z0U92W6HvcFn3f9ZC4fAMeYmUm6DLge+CdO0vwq30cN3F/lkT7+ADwIdDezHxK0m+j5TkAb4HvgTeBc3FRDdWCWmf1TUmtgIHCcmW2T9BDQCzeKcIOZ/SypIjBZUnvgK9zKlc64lSXj8nl/CX1Lwg9mliHpXtyqmeNwy3UX4KTS49ECWCfpRaAZ8C4wCCcdPw640Mw+llQT2JSgjTwEefWyTbr4AenjS7r4AcGX4iYEH8lZGqUzMQY3QhDhaCDbzFYDSBqH+xJMxIHAOB+0VCE3T2I6cI8fYXnRzJb5TVlb40Y8TjOzH5O0m+j5j8zsW2/bc8DxuOBjB7krUbrggqeP/TPVgFX+3gX+y7cS0AAXyFQAvvOrSJA0Bv/lXEDbkvGK/zkfqOFHKTZI2iKptpmti/NMJZys+uHAD7iAIwunH7LCzD4GMLNf8+s8GjP7H+4zoGXLlnZ1r+JRF/30009Zu3YtTZo0YcCAATRr1oyTTjqJu+++m8zMzJx6999/P/369dulrDBkZ2fvdhtlgXTxA9LHl3TxA4IvxU1Y7ZIcy+e6IDwIjDCzdji58L0AzGwYTuSrGjBdUitffwVupODwpAYmfj6R7Zuj8jwEPGlmGf5oaWZDJDXDjVp0MbP2OMGvvSggCWyLVjUlTruRqaud7DqNtZPEwfIyYI6ZfWtm24GXcFNlZZIgrx4IBPZ0QvCRnMaSIpLiF7FrjsMs4CRJdb3i5/n5tFWLXNnv3pFCSQeb2XwzuwMnwBUJHtYBZwG3J9uBNcnznSQ1k1QBtxFcvPyMyUDPiICXpDpe4bQm8BuwXtL+uBwMgC+BppIO9tdJMx8T2PY90EZSVa9k2iVZGynyMVBbUn1/3Rn4HKfM2sDnfSBpH0mlPtq3YsUKTj75ZNq3b89RRx3FqaeeSrdu3Rg0aBAvvPAC7dq141//+leQVw8EAmlLqf9HXMZZhEvKfBz3ZfYwbmv4iKz3EGAGLlCYk09bQ4DnJf0CTMHlJgBcJ+lk3F/2C3HqnMf6PlZK6obbHv4SM5sVp91Ez38MjCA34XRi7INm9rmkG4G3fZCyDZejMdOrjn4JLMVNn2Bmm/1UzGuSfgfeJ1dGPR55bDOzLZLG43I4viN5nkxK+ATZ/rjcFAGzcbv2bpV0IfCgpGq4fI9TgI0+ubUmUEVSD9z01ufxeyhagrx6IBDY0wnBRwK8RkSrOLcyo+o8AaSkaWFmLwMvxym/Ok71bH/gE00T7gIb73mfV/FrvCWwZlYj5noccRJHzSwrQX9vEv+9pGSbL78el3AbW9406nw0LuE0z70Ebb7DrmqpkfKPgWOS9RUIBAKBkiVMuwQCJUwihdOsrCyaNWuWo2Y6Z84cAH755RfOOecc2rdvT6dOnViwYEEpWh8IBAK7Txj5KGIk3UDe/I/nzez/drPdPsC1McXTzeyq2Lpmlo0fOSkJCmLbbvbTDng6pnhLPuJjZY5ECqfgpNR79uy5S/3//Oc/ZGRkMHHiRL788kuuuuoqJk+eXBqmBwKBQJEQgo8iwCdOXmRmD/kgI0+g4cW8/mBmz+bTVlPiyLoXZIonpr0sikj2PRGFta0Q/cwHMqLLJDWR9CluFK8y8GAiMTJfvxXO1iNwWiZ3FZ/FCW2Iq3CaiM8//5xBgwYB0KpVK5YsWcLKlSvZf//9S8TeQCAQKGpC8FE01MYJkD2UpE5T3IqZpMFHoMCsAI71iaw1gAWSXkmijfIzTq+lR0E6KSqF0yXDzgKIq3D68MMPc8MNN3DLLbfQpUsXhg0bRtWqVenQoQMvvvgiJ5xwAh999BHff/89y5YtC8FHIBAot8hsd6QrAgCSxgLdcatj3vHFZ+K0NW4zs3GSZuKEw74DnsStPnkapzgK0M/MPkw08hHV10yc6upCf52N0+T4FngcaI6TPr/czOZFj3xIGu3bnuCf3WhmNfxS3qG4VTvtgPE4ka9rcRodPczsG7+U9RGgsTfnuigRtlg7S0RaPabPurjVM8eY2Y9eCv4/OKXTNWbWJaruEGBjspGPGIXTIwff92h+JuRLu0a1drmOKJxec8011KxZkzp16rBt2zbuvvtuGjZsSO/evfntt98YMWIEX3/9Nc2bN+eHH36gf//+HHJIyurwefqMjLyUZ9LFD0gfX9LFDwi+FBUnn3zybDPrmOeGmYVjNw/cqMYCf34eLgCpCOyPU9xsgFslMynqmb2Bvfz5obgv4V3aStDX34Gh/rwBsMifPwjc7M8740S3wCl9jvDno3F7xETa2uh/ZuICjwa4XWOXR/VxLXCfP38WON6fNwa+SGLnqzjZdoAauFG22HcwAsjy50uAvv78XmAebhlvfWBlPu//IF//d9xSYfxzS4Fm/rpOzDNDcIFQSp9xixYtrLgYOnSoDR8+fJeyqVOn2llnnZWn7s6dO61Jkya2fv36Qvc3derUQj9blkgXP8zSx5d08cMs+FJURL7bYo+w2qXoOR54zsx2mNlKYBpug7ZYKgOPSpoPPI+TL0+F8UAkI/ECnGR6pN+nAcxsClDX72WSKh+b2Qoz2wJ8A7zty+fjAiJwGhkjJM3ByaDX9FMd8YhIq18D1DanPJof0dLqs8xsgzn5+i0+ryYuZrbUnBLrIUBvL4x2DPCemX3n6/ycQv8lQiKF0xUrVgDuD4KXXnqJtm3d4Ne6devYunUrAKNGjeLEE0+kZs2CfLSBQCBQtgg5H6XH34GVQAdcsmQqm65hZsslrfUbvV0IXFGAPnOkzb2oWJWoe7FS5tEy55Hfkwq4KY18bTWzYZJeA7ripNVPp3ik1aP7/FHSAtw+Lwl3GC5tVqxYQe/evdmxYwc7d+7kggsuoFu3bnTu3JnVq1djZmRkZPDII26m6YsvvqB3795I4rDDDuOxxx4rZQ8CgUBg9wjBR9GwgVylz/eBv0l6EqiDy3UYADRiVzXQWsAyc9vc98ZN06TKOJxIVy0zi2xx+j5uR9pbfQ7HGjP7NWYVxRJc3sV44Gzc6EtBeBu4GhgOICnDzObEqxiRVgfme3nzVjjl0TaSquJySboQX/Y9ZSQdCKw1s02S9sWNAN0L/AQ8JKmZmX0nqU5ZGf1IpHA6ZcqUuPWPPfZYvvrqq+I2KxAIBEqMEHwUAWa2VtJ0/1f3G7j8g7m4RMvrzewnSWuBHZLm4nIvHgJekHQxbsv73wrQ5QRcMuetUWVDgMclRXIfesd57lHgZW9DQfsEt0pkpO+jEvAeiUdeSkRaHZfEe7ckw22Ud5cPeiIJoy/6UZ5VwKmSDgA+wUmr75R0HdDGCrjjbSAQCAQKTwg+iggzuyimaEDM/W24RNBoouXAB/p6S4C4K12i2lpJzGfn/6rvEafuaLxMuX8uWmo80mc2UaJkZpYZdZ5zz8zW4KZ68sVKSFrdEsiq+3tv4ILB6LKfgAOTmF7sbN68mRNPPJEtW7awfft2evbsydChQ8nKymLatGnUquVWxIwePZqMjAzWr1/PX/7yF3744Qe2b99O//796dOnT2m6EAgEArtFCD4CgRKmoAqnI0eOpE2bNrz66qusXr2ali1b0qtXL6pUqRKv+UAgECjz7NGrXSQ19VMlJd1vQ0kT8qlzuqQ5UcdGSVML0EempEm7b22+/fSJsXOOpJHF0E+7OP3MkrSXpI8kzZW0UNLQfNqpK2mqf58jitrOVCiowqkkNmzYgJmxceNG6tSpQ6VK4e+GQCBQfgn/g5UC5tQ3e+ZT5y3grci1FxMbkPCBUsJKUVodQO5bu7OZbZRUGfhA0htmNjNBU5uBm3BTW0mnt6IpTYXTfv36cfbZZ9OwYUM2bNjAuHHjqFBhj/67IRAIlHPSTuFU0jBgqZmN9NdDcImV+5FXdbQpXk00dg8UP2pwl5llS9oIPIxbNroC+DdwJ05o6zoze0VSRWAYTkirKjDSzP6bwMbYfnvglE4PBe7CLYH9K265aFcz+9kHH3OBk3BB4yVm9pGkTrjk072ATUAfM1vkV7z0N7NuSepk4Va97A0cDEz0ORnEUweVVB0nZtYWt1JmiJm9nMDHw3BBSRXcCNt5wDai1Fsl9QdqmNkQ799nuGWy1YGLgX/hFFfHmdmN8fqJ6XNv3OqZvmY2y6+yud+3twXoYmYbfN0s8tnzpqwonE6bNo0FCxZw5ZVX8uOPP9K/f39GjRpF9erVE/SSnHRRbkwXPyB9fEkXPyD4UlTsMQqnwOHAtKjrz3ErP+KpjjYlV5k0C68E6q8nAZn+3IAz/flE3JLTyjiNjoiS6OXAjf68Km5FRbMENsb2u5hcNc/1wBWWq/R5nT/PBh715ydGPV8TqOTPTwFesFzV0kn51MnCybLXwgUm3+PUQuOqg+KCkb/489rAV0D1BD4+CPTy51VwS2tz/Pbl/XEBTMS/O/z5tcCP5CquLgPqJvnMKwJzgI1RbVTxvh0V+w7ifd75HaWpcNq1a1d77733cu6dfPLJNmvWrEL3ly7Kjenih1n6+JIufpgFX4oK9hSFUzP7DNjP51V0AH7BDdenojqaiK24pang1DenmVu9Eq3+eRpwsVf/nAXUxY1kpMJUy1XzXI+TJo/01TSq3nPex/dw6qK1cYHD8z535V7gsDjtJ6sz2czWmxMO+xxoQmJ10NOAQd7HbFzA0pj4zAD+LWkg0MTMNqXwHqIVThdaruLqt7igKC7+c83ArWLpJKkt0BJYYWYf+zq/Wmoqq8VOQRVOGzduzOTJkwFYuXIlixYtonnz5qVieyAQCBQF6Zrz8Twup+IAnCBXsxSeSaa+uc1HcBClvmlOICzyDgVcbS5Xo6Ckoi4KbgSGmOtbccHLOX46JztO+8nqRPe9g+S/EwLOM7NFSeo4w8yelTQLOAt4XdLfcCMlxalwus4n5Z5BVL5MWaOgCqc33XQTWVlZtGvXDjPjjjvuoF69eqXsRSAQCBSedA0+xuEEterhciSOJb7qaPSX3xLgSi9I1QjoVMA+3wL6SppiZtsktQCWm1lBhbyScSEwVdLxwHozWy+pFm4jOHBTCfFIpU40M4mvDvoWcLWkq83MJB3uR5ryIKk58K2ZPSCpMU6L433cqFRd3BRJN3JHlAqF3E6723zgUQ04FbgDt8NwA0lHmdnHkvYBNpWF0Y+CKpw2bNiQt99+O+69QCAQKI+kZfBhZgv9l81yM1shaSIuAIlVHW0a9dh0nOrm58AXwKcF7HYUborkU78CYzVxRL92k82SPsPlm1ziy+4EnpR0I5BoOUYqdXIws9Xx1EFxIyj3AfN8+Xe4ACIeFwB/lbQNJ3X+Hx+U3QJ8hAuGvszPlhRogPOtIm5UZbyZTQKQdCHwoA9KNuHyXTZKWoLLAakiqQdwmpl9XgS2BAKBQCAF0m61SyBQHLRs2dIWLcp3tqlckJ2dTWZmZmmbsdukix+QPr6kix8QfCkqJMVd7ZJ2CaeBQCAQCATKNmk57VJWkNQOeDqmeIuZHV0a9hQHkk7H5VhE852ZnVPE/dQFJse51cXM1hZlX4FAIBAoXkLwUYxYAlXOdMJilFiLsZ+1pPm7DAQCgT2FMO0SCAQCgUCgRAkJp4FACkjagFu+mw7UA9aUthFFQLr4AenjS7r4AcGXoqKJmdWPLQzTLoFAaiyKl7FdHpH0STr4ki5+QPr4ki5+QPCluAnTLoFAIBAIBEqUEHwEAoFAIBAoUULwEQikxv9K24AiJF18SRc/IH18SRc/IPhSrISE00AgEAgEAiVKGPkIBAKBQCBQooTgIxAIBAKBQIkSgo9AIAmSzpC0SNJiSYNK2554SHpc0ipJC6LK6kh6R9LX/ue+vlySHvD+zJN0RNQzvX39ryX1LgU/DpI0VdLnkhZKurYc+7KXpI8kzfW+DPXlzSTN8jaPk1TFl1f114v9/aZRbf3Lly/y2xmUOJIqSvpMUmTH6PLqxxJJ8yXNkfSJLyt3v1/ehtqSJkj6UtIXko4tV76YWTjCEY44B1AR+AZoDlQB5gJtStuuOHaeCBwBLIgquxMY5M8HAXf4867AG4CAY4BZvrwO8K3/ua8/37eE/WgAHOHP9wG+AtqUU18E1PDnlYFZ3sbxwJ98+SNAX39+JfCIP/8TMM6ft/G/d1WBZv73sWIp/I79A3gWmOSvy6sfS4B6MWXl7vfL2/EkcJk/rwLULk++hJGPQCAxnYDFZvatmW0FxgLdS9mmPJjZe8DPMcXdcf854X/2iCp/yhwzgdqSGgCnA++Y2c9m9gvwDnBGsRsfhZmtMLNP/fkG4AugEeXTFzOzjf6ysj8M6AxM8OWxvkR8nAB0kSRfPtbMtpjZd8Bi3O9liSHpQOAsYJS/FuXQjySUu98vSbVwf3Q8BmBmW81sHeXIlxB8BAKJaQQsjbpe5svKA/ub2Qp//hOwvz9P5FOZ8tUP1x+OGzEol774qYo5wCrcf+rfAOvMbHscu3Js9vfXA3UpG77cB1wP7PTXdSmffoALAN+WNFvS5b6sPP5+NQNWA0/46bBRkqpTjnwJwUcgkOaYG18tN2vqJdUAXgCuM7Nfo++VJ1/MbIeZZQAH4v7Kb1W6FhUcSd2AVWY2u7RtKSKON7MjgDOBqySdGH2zHP1+VcJNtT5sZocDv+GmWXIo676E4CMQSMxy4KCo6wN9WXlgpR9Wxf9c5csT+VQmfJVUGRd4PGNmL/riculLBD8cPhU4FjfcHdlTK9quHJv9/VrAWkrfl+OAsyUtwU07dgbup/z5AYCZLfc/VwETcUFhefz9WgYsM7NZ/noCLhgpN76E4CMQSMzHwKE+s78KLoHulVK2KVVeASKZ672Bl6PKL/bZ78cA6/0w7VvAaZL29Rnyp/myEsPnBjwGfGFm90TdKo++1JdU259XA07F5bBMBXr6arG+RHzsCUzxf7m+AvzJryJpBhwKfFQiTgBm9i8zO9DMmuJ+/6eYWS/KmR8AkqpL2idyjvu9WEA5/P0ys5+ApZJa+qIuwOeUJ19KIqs1HOEorwcuS/wr3Hz9DaVtTwIbnwNWANtwfxFdiptnnwx8DbwL1PF1BYz0/swHOka1cwkuEXAx0KcU/DgeN0w8D5jjj67l1Jf2wGfelwXAYF/eHPeluxh4Hqjqy/fy14v9/eZRbd3gfVwEnFmKv2eZ5K52KXd+eJvn+mNh5N9zefz98jZkAJ/437GXcKtVyo0vQV49EAgEAoFAiRKmXQKBQCAQCJQoIfgIBAKBQCBQooTgIxAIBAKBQIkSgo9AIBAIBAIlSgg+AoFAIBAIlCgh+AgEAns0knb4XU4jR9NCtNFDUptiMA9JDSVNyL9mkfaZIalrSfYZ2LOolH+VQCAQSGs2mZNB3x16AJNwQk8pIamS5e6PkhAz+5FcQa9ixyuTZgAdgddLqt/AnkUY+QgEAoEYJB0paZrfgOytKMnq/yfpY0lzJb0gaW9JfwDOBob7kZODJWVL6uifqeflyZGUJekVSVOAyV5183FJH/kNwvLsmiypqaQFUc+/JOkdSUsk9ZP0D//sTEl1fL1sSfd7exZI6uTL6/jn5/n67X35EElPS5oOPA3cAlzon79QUidJM3w/H0aUNb09L0p6U9LXku6MsvsMSZ/6dzXZl+Xrb2DPIIx8BAKBPZ1qcrvPAnwHXAA8CHQ3s9WSLgT+D6cE+aKZPQog6TbgUjN7UNIrOPXPCf5esv6OANqb2c+S/oOTIL/Ey7F/JOldM/styfNtcTv+7oVTpRxoZodLuhe4GLcLLcDeZpYht3na4/65ocBnZtZDUmfgKdwoB0Ab3MZrmyRl4VQw+3l/agInmNl2SacA/wHO889leHu2AIskPQhsBh4FTjSz7yJBEU7ltKD+BtKQEHwEAoE9nV2mXSS1xX1Rv+ODiIo4+XqAtj7oqA3UoHD7YLxjZj/789NwG7f199d7AY1x+8AkYqqZbQA2SFoPvOrL5+Nk3SM8B2Bm70mq6b/sj8cHDWY2RVJdH1gAvGJmmxL0WQt4UtKhOAn8ylH3JpvZegBJnwNNcFLf75nZd76v3fE3kIaE4CMQCAR2RcBCMzs2zr3RQA8zm+tHBzITtLGd3GntvWLuRf+VL+A8M1tUAPu2RJ3vjLreya7/p8funZHfXhrJRh9uxQU95/iE3OwE9uwg+fdKYfwNpCEh5yMQCAR2ZRFQX9KxAJIqSzrM39sHWCGpMtAr6pkN/l6EJcCR/jxZsuhbwNXyQyySDt9983O40Ld5PG4X0/XA+3i7JWUCa8zs1zjPxvpTi9yt1rNS6HsmcKLcDrZETbsUp7+BckQIPgKBQCAKM9uKCxjukDQXt7vuH/ztm4BZwHTgy6jHxgIDfBLlwcBdQF9JnwH1knR3K24KY56khf66qNjs+38Et9MxwBDgSEnzgGHkbr8ey1SgTSThFLgTuN23l++IuZmtBi4HXvTvcJy/VZz+BsoRYVfbQCAQSDMkZQP9zeyT0rYlEIhHGPkIBAKBQCBQooSRj0AgEAgEAiVKGPkIBAKBQCBQooTgIxAIBAKBQIkSgo9AIBAIBAIlSgg+AoFAIBAIlCgh+AgEAoFAIFCi/H8wtJFlc53WtQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "seed0=2021\n",
    "params0 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'max_bin':100,\n",
    "    'min_data_in_leaf':500,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "    'categorical_column':[0],\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'n_jobs':-1,\n",
    "    'verbose': -1}\n",
    "seed1=42\n",
    "params1 = {\n",
    "        'learning_rate': 0.1,        \n",
    "        'lambda_l1': 2,\n",
    "        'lambda_l2': 7,\n",
    "        'num_leaves': 800,\n",
    "        'min_sum_hessian_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 42,\n",
    "        'min_data_in_leaf': 700,\n",
    "        'max_depth': 4,\n",
    "        'categorical_column':[0],\n",
    "        'seed': seed1,\n",
    "        'feature_fraction_seed': seed1,\n",
    "        'bagging_seed': seed1,\n",
    "        'drop_seed': seed1,\n",
    "        'data_random_seed': seed1,\n",
    "        'objective': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs':-1,\n",
    "    }\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params):\n",
    "    # Hyperparammeters (just basic)\n",
    "    \n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    y = train['target']\n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
    "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
    "        model = lgb.train(params = params,\n",
    "                          num_boost_round=1400,\n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          verbose_eval = 250,\n",
    "                          early_stopping_rounds=30,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(test[features]) / 5\n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    lgb.plot_importance(model,max_num_features=20)\n",
    "    # Return test predictions\n",
    "    return test_predictions\n",
    "# Traing and evaluate\n",
    "predictions_lgb1= train_and_evaluate_lgb(train, test,params0)\n",
    "#test['target'] = predictions_lgb\n",
    "#test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ef18209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:17:41.726342Z",
     "iopub.status.busy": "2021-09-19T04:17:41.711984Z",
     "iopub.status.idle": "2021-09-19T04:29:56.437351Z",
     "shell.execute_reply": "2021-09-19T04:29:56.437944Z"
    },
    "papermill": {
     "duration": 734.780412,
     "end_time": "2021-09-19T04:29:56.438175",
     "exception": false,
     "start_time": "2021-09-19T04:17:41.657763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428203\ttraining's RMSPE: 0.198249\tvalid_1's rmse: 0.000444544\tvalid_1's RMSPE: 0.205445\n",
      "[500]\ttraining's rmse: 0.000406459\ttraining's RMSPE: 0.188182\tvalid_1's rmse: 0.000429863\tvalid_1's RMSPE: 0.19866\n",
      "[750]\ttraining's rmse: 0.000393027\ttraining's RMSPE: 0.181963\tvalid_1's rmse: 0.000422827\tvalid_1's RMSPE: 0.195409\n",
      "[1000]\ttraining's rmse: 0.000382694\ttraining's RMSPE: 0.177179\tvalid_1's rmse: 0.000417773\tvalid_1's RMSPE: 0.193073\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\ttraining's rmse: 0.000376045\ttraining's RMSPE: 0.174101\tvalid_1's rmse: 0.000415398\tvalid_1's RMSPE: 0.191975\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000429315\ttraining's RMSPE: 0.198558\tvalid_1's rmse: 0.000463175\tvalid_1's RMSPE: 0.214945\n",
      "[500]\ttraining's rmse: 0.000407322\ttraining's RMSPE: 0.188386\tvalid_1's rmse: 0.000447908\tvalid_1's RMSPE: 0.20786\n",
      "[750]\ttraining's rmse: 0.000394141\ttraining's RMSPE: 0.18229\tvalid_1's rmse: 0.000439609\tvalid_1's RMSPE: 0.204009\n",
      "[1000]\ttraining's rmse: 0.000383843\ttraining's RMSPE: 0.177527\tvalid_1's rmse: 0.000434419\tvalid_1's RMSPE: 0.2016\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\ttraining's rmse: 0.000377397\ttraining's RMSPE: 0.174546\tvalid_1's rmse: 0.000432726\tvalid_1's RMSPE: 0.200814\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428287\ttraining's RMSPE: 0.198307\tvalid_1's rmse: 0.000440318\tvalid_1's RMSPE: 0.203413\n",
      "[500]\ttraining's rmse: 0.000406561\ttraining's RMSPE: 0.188247\tvalid_1's rmse: 0.000425659\tvalid_1's RMSPE: 0.196642\n",
      "[750]\ttraining's rmse: 0.000392988\ttraining's RMSPE: 0.181963\tvalid_1's rmse: 0.000417953\tvalid_1's RMSPE: 0.193081\n",
      "[1000]\ttraining's rmse: 0.000382979\ttraining's RMSPE: 0.177328\tvalid_1's rmse: 0.000413454\tvalid_1's RMSPE: 0.191003\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\ttraining's rmse: 0.000376398\ttraining's RMSPE: 0.174281\tvalid_1's rmse: 0.000410941\tvalid_1's RMSPE: 0.189842\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428504\ttraining's RMSPE: 0.198068\tvalid_1's rmse: 0.000443053\tvalid_1's RMSPE: 0.206077\n",
      "[500]\ttraining's rmse: 0.00040637\ttraining's RMSPE: 0.187837\tvalid_1's rmse: 0.000428895\tvalid_1's RMSPE: 0.199492\n",
      "[750]\ttraining's rmse: 0.000392952\ttraining's RMSPE: 0.181635\tvalid_1's rmse: 0.000422792\tvalid_1's RMSPE: 0.196653\n",
      "[1000]\ttraining's rmse: 0.000383079\ttraining's RMSPE: 0.177071\tvalid_1's rmse: 0.000418812\tvalid_1's RMSPE: 0.194802\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\ttraining's rmse: 0.000376725\ttraining's RMSPE: 0.174134\tvalid_1's rmse: 0.000417047\tvalid_1's RMSPE: 0.193981\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000427492\ttraining's RMSPE: 0.19807\tvalid_1's rmse: 0.000448381\tvalid_1's RMSPE: 0.206585\n",
      "[500]\ttraining's rmse: 0.000406021\ttraining's RMSPE: 0.188122\tvalid_1's rmse: 0.000433827\tvalid_1's RMSPE: 0.19988\n",
      "[750]\ttraining's rmse: 0.00039266\ttraining's RMSPE: 0.181931\tvalid_1's rmse: 0.000426841\tvalid_1's RMSPE: 0.196661\n",
      "[1000]\ttraining's rmse: 0.000382493\ttraining's RMSPE: 0.17722\tvalid_1's rmse: 0.000422981\tvalid_1's RMSPE: 0.194882\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1200]\ttraining's rmse: 0.000376161\ttraining's RMSPE: 0.174287\tvalid_1's rmse: 0.000420993\tvalid_1's RMSPE: 0.193967\n",
      "Our out of folds RMSPE is 0.19415068951748152\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "seed0=1111\n",
    "params111 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'max_bin':100,\n",
    "    'min_data_in_leaf':500,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "    'categorical_column':[0],\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'n_jobs':-1,\n",
    "    'verbose': -1}\n",
    "seed1=42\n",
    "params1 = {\n",
    "        'learning_rate': 0.1,        \n",
    "        'lambda_l1': 2,\n",
    "        'lambda_l2': 7,\n",
    "        'num_leaves': 800,\n",
    "        'min_sum_hessian_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 42,\n",
    "        'min_data_in_leaf': 700,\n",
    "        'max_depth': 4,\n",
    "        'categorical_column':[0],\n",
    "        'seed': seed1,\n",
    "        'feature_fraction_seed': seed1,\n",
    "        'bagging_seed': seed1,\n",
    "        'drop_seed': seed1,\n",
    "        'data_random_seed': seed1,\n",
    "        'objective': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs':-1,\n",
    "    }\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params):\n",
    "    # Hyperparammeters (just basic)\n",
    "    \n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    y = train['target']\n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 1111, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
    "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
    "        model = lgb.train(params = params,\n",
    "                          num_boost_round=1200,\n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          verbose_eval = 250,\n",
    "                          early_stopping_rounds=50,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(test[features]) / 5\n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    lgb.plot_importance(model,max_num_features=20)\n",
    "    # Return test predictions\n",
    "    return test_predictions\n",
    "# Traing and evaluate\n",
    "predictions_lgb2= train_and_evaluate_lgb(train, test,params111)\n",
    "#test['target'] = predictions_lgb\n",
    "#test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71d01d93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:29:56.552377Z",
     "iopub.status.busy": "2021-09-19T04:29:56.551747Z",
     "iopub.status.idle": "2021-09-19T04:29:56.560547Z",
     "shell.execute_reply": "2021-09-19T04:29:56.559930Z"
    },
    "papermill": {
     "duration": 0.067182,
     "end_time": "2021-09-19T04:29:56.560698",
     "exception": false,
     "start_time": "2021-09-19T04:29:56.493516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00186785, 0.00190121, 0.00190121]),\n",
       " array([0.00165038, 0.00162601, 0.00162601]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_lgb2,predictions_lgb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55e05898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:29:56.680449Z",
     "iopub.status.busy": "2021-09-19T04:29:56.679720Z",
     "iopub.status.idle": "2021-09-19T04:30:02.225642Z",
     "shell.execute_reply": "2021-09-19T04:30:02.224916Z"
    },
    "papermill": {
     "duration": 5.609766,
     "end_time": "2021-09-19T04:30:02.225800",
     "exception": false,
     "start_time": "2021-09-19T04:29:56.616034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "    \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98303560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:30:02.358896Z",
     "iopub.status.busy": "2021-09-19T04:30:02.358028Z",
     "iopub.status.idle": "2021-09-19T04:30:14.142639Z",
     "shell.execute_reply": "2021-09-19T04:30:14.142060Z"
    },
    "papermill": {
     "duration": 11.859688,
     "end_time": "2021-09-19T04:30:14.142799",
     "exception": false,
     "start_time": "2021-09-19T04:30:02.283111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kfold based on the knn++ algorithm\n",
    "\n",
    "out_train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "#out_train[out_train.isna().any(axis=1)]\n",
    "out_train = out_train.fillna(out_train.mean())\n",
    "out_train.head()\n",
    "\n",
    "# code to add the just the read data after first execution\n",
    "\n",
    "# data separation based on knn ++\n",
    "nfolds = 5 # number of folds\n",
    "index = []\n",
    "totDist = []\n",
    "values = []\n",
    "# generates a matriz with the values of \n",
    "mat = out_train.values\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "mat = scaler.fit_transform(mat)\n",
    "\n",
    "nind = int(mat.shape[0]/nfolds) # number of individuals\n",
    "\n",
    "# adds index in the last column\n",
    "mat = np.c_[mat,np.arange(mat.shape[0])]\n",
    "\n",
    "\n",
    "lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n",
    "\n",
    "lineNumber = np.sort(lineNumber)[::-1]\n",
    "\n",
    "for n in range(nfolds):\n",
    "    totDist.append(np.zeros(mat.shape[0]-nfolds))\n",
    "\n",
    "# saves index\n",
    "for n in range(nfolds):\n",
    "    \n",
    "    values.append([lineNumber[n]])    \n",
    "\n",
    "\n",
    "s=[]\n",
    "for n in range(nfolds):\n",
    "    s.append(mat[lineNumber[n],:])\n",
    "    \n",
    "    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n",
    "\n",
    "for n in range(nind-1):    \n",
    "\n",
    "    luck = np.random.uniform(0,1,nfolds)\n",
    "    \n",
    "    for cycle in range(nfolds):\n",
    "         # saves the values of index           \n",
    "\n",
    "        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n",
    "\n",
    "        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n",
    "        totDist[cycle] += sumDist        \n",
    "                \n",
    "        # probabilities\n",
    "        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n",
    "        j = 0\n",
    "        kn = 0\n",
    "        for val in f:\n",
    "            j += val        \n",
    "            if (j > luck[cycle]): # the column was selected\n",
    "                break\n",
    "            kn +=1\n",
    "        lineNumber[cycle] = kn\n",
    "        \n",
    "        # delete line of the value added    \n",
    "        for n_iter in range(nfolds):\n",
    "            \n",
    "            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n",
    "            j= 0\n",
    "        \n",
    "        s[cycle] = mat[lineNumber[cycle],:]\n",
    "        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n",
    "        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n",
    "\n",
    "\n",
    "for n_mod in range(nfolds):\n",
    "    values[n_mod] = out_train.index[values[n_mod]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12501859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:30:14.262649Z",
     "iopub.status.busy": "2021-09-19T04:30:14.261930Z",
     "iopub.status.idle": "2021-09-19T04:30:42.897488Z",
     "shell.execute_reply": "2021-09-19T04:30:42.896861Z"
    },
    "papermill": {
     "duration": 28.698643,
     "end_time": "2021-09-19T04:30:42.897641",
     "exception": false,
     "start_time": "2021-09-19T04:30:14.198998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#colNames.remove('row_id')\n",
    "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "qt_train = []\n",
    "train_nn=train[colNames].copy()\n",
    "test_nn=test[colNames].copy()\n",
    "for col in colNames:\n",
    "    #print(col)\n",
    "    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n",
    "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
    "    test_nn[col] = qt.transform(test_nn[[col]])    \n",
    "    qt_train.append(qt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78c40c57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:30:43.019648Z",
     "iopub.status.busy": "2021-09-19T04:30:43.018940Z",
     "iopub.status.idle": "2021-09-19T04:30:43.028187Z",
     "shell.execute_reply": "2021-09-19T04:30:43.027511Z"
    },
    "papermill": {
     "duration": 0.074588,
     "end_time": "2021-09-19T04:30:43.028336",
     "exception": false,
     "start_time": "2021-09-19T04:30:42.953748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\n",
    "test_nn[['stock_id','time_id']]=test[['stock_id','time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4d84c4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:30:43.153089Z",
     "iopub.status.busy": "2021-09-19T04:30:43.152369Z",
     "iopub.status.idle": "2021-09-19T04:30:44.837471Z",
     "shell.execute_reply": "2021-09-19T04:30:44.838211Z"
    },
    "papermill": {
     "duration": 1.754242,
     "end_time": "2021-09-19T04:30:44.838539",
     "exception": false,
     "start_time": "2021-09-19T04:30:43.084297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "# making agg features\n",
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90fc5a0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:30:44.963137Z",
     "iopub.status.busy": "2021-09-19T04:30:44.962457Z",
     "iopub.status.idle": "2021-09-19T04:30:44.965082Z",
     "shell.execute_reply": "2021-09-19T04:30:44.965584Z"
    },
    "papermill": {
     "duration": 0.068295,
     "end_time": "2021-09-19T04:30:44.965760",
     "exception": false,
     "start_time": "2021-09-19T04:30:44.897465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4ee3c7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:30:45.091562Z",
     "iopub.status.busy": "2021-09-19T04:30:45.090475Z",
     "iopub.status.idle": "2021-09-19T04:30:45.240048Z",
     "shell.execute_reply": "2021-09-19T04:30:45.240822Z"
    },
    "papermill": {
     "duration": 0.216794,
     "end_time": "2021-09-19T04:30:45.241054",
     "exception": false,
     "start_time": "2021-09-19T04:30:45.024260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a199175c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:30:45.367297Z",
     "iopub.status.busy": "2021-09-19T04:30:45.366629Z",
     "iopub.status.idle": "2021-09-19T04:30:50.969204Z",
     "shell.execute_reply": "2021-09-19T04:30:50.969714Z"
    },
    "papermill": {
     "duration": 5.669541,
     "end_time": "2021-09-19T04:30:50.969897",
     "exception": false,
     "start_time": "2021-09-19T04:30:45.300356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "train_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\n",
    "test_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')\n",
    "\n",
    "train1=train_nn\n",
    "test1=test_nn\n",
    "del mat1,mat2\n",
    "del train,test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c947dc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:30:51.097683Z",
     "iopub.status.busy": "2021-09-19T04:30:51.096907Z",
     "iopub.status.idle": "2021-09-19T04:30:51.114021Z",
     "shell.execute_reply": "2021-09-19T04:30:51.113349Z"
    },
    "papermill": {
     "duration": 0.085653,
     "end_time": "2021-09-19T04:30:51.114166",
     "exception": false,
     "start_time": "2021-09-19T04:30:51.028513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\n",
    "from keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (128,64,32)\n",
    "stock_embedding_size = 24\n",
    "\n",
    "cat_data = train_nn['stock_id']\n",
    "\n",
    "def base_model():\n",
    "    \n",
    "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(244,), name='num_data')\n",
    "\n",
    "\n",
    "    #embedding, flatenning and concatenating\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "        \n",
    "\n",
    "    #out = keras.layers.Concatenate()([out, num_input])\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "371f8bb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:30:51.239196Z",
     "iopub.status.busy": "2021-09-19T04:30:51.238543Z",
     "iopub.status.idle": "2021-09-19T04:30:51.240519Z",
     "shell.execute_reply": "2021-09-19T04:30:51.241029Z"
    },
    "papermill": {
     "duration": 0.067874,
     "end_time": "2021-09-19T04:30:51.241206",
     "exception": false,
     "start_time": "2021-09-19T04:30:51.173332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4433069",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:30:51.361870Z",
     "iopub.status.busy": "2021-09-19T04:30:51.361184Z",
     "iopub.status.idle": "2021-09-19T04:45:26.879237Z",
     "shell.execute_reply": "2021-09-19T04:45:26.841485Z"
    },
    "papermill": {
     "duration": 875.579455,
     "end_time": "2021-09-19T04:45:26.879473",
     "exception": false,
     "start_time": "2021-09-19T04:30:51.300018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 4s 18ms/step - loss: 23.6953 - val_loss: 1.7192\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.9857 - val_loss: 0.5698\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6524 - val_loss: 0.5460\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6289 - val_loss: 0.7143\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5223 - val_loss: 0.7325\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.9121 - val_loss: 0.7280\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6053 - val_loss: 0.5918\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5474 - val_loss: 0.5482\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5969 - val_loss: 0.6247\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6996 - val_loss: 0.3952\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4268 - val_loss: 0.5293\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.4212 - val_loss: 0.4298\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4442 - val_loss: 0.3448\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3713 - val_loss: 0.2358\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2307 - val_loss: 0.2692\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2302 - val_loss: 0.2331\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2391 - val_loss: 0.2683\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.4012 - val_loss: 0.2457\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2456 - val_loss: 0.2255\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2290 - val_loss: 0.2152\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2257 - val_loss: 0.2235\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2268 - val_loss: 0.2654\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2242 - val_loss: 0.2178\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2210 - val_loss: 0.2153\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2248 - val_loss: 0.2362\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2332 - val_loss: 0.2676\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2474 - val_loss: 0.2359\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2137 - val_loss: 0.2093\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2103 - val_loss: 0.2099\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2095 - val_loss: 0.2107\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2099 - val_loss: 0.2101\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2094 - val_loss: 0.2109\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2094 - val_loss: 0.2091\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2088 - val_loss: 0.2139\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2088 - val_loss: 0.2122\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2095 - val_loss: 0.2104\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2084 - val_loss: 0.2096\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2098 - val_loss: 0.2090\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2088 - val_loss: 0.2094\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2097 - val_loss: 0.2121\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2065 - val_loss: 0.2088\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2059 - val_loss: 0.2087\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2091\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2069 - val_loss: 0.2090\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2082\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2069 - val_loss: 0.2081\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2093\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2051 - val_loss: 0.2100\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2059 - val_loss: 0.2087\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2101\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2069 - val_loss: 0.2092\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2063 - val_loss: 0.2084\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2057 - val_loss: 0.2102\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.2054 - val_loss: 0.2081\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 3s 18ms/step - loss: 0.2048 - val_loss: 0.2084\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2084\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2040 - val_loss: 0.2083\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2046 - val_loss: 0.2084\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2035 - val_loss: 0.2088\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2047 - val_loss: 0.2087\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2039 - val_loss: 0.2082\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2040 - val_loss: 0.2081\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2081\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2036 - val_loss: 0.2081\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2082\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2043 - val_loss: 0.2081\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2036 - val_loss: 0.2081\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2050 - val_loss: 0.2081\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2033 - val_loss: 0.2081\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2037 - val_loss: 0.2082\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2037 - val_loss: 0.2081\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2048 - val_loss: 0.2081\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2033 - val_loss: 0.2083\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2034 - val_loss: 0.2082\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2082\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2039 - val_loss: 0.2081\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2042 - val_loss: 0.2081\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2041 - val_loss: 0.2081\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2047 - val_loss: 0.2081\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2037 - val_loss: 0.2081\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2081\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2035 - val_loss: 0.2081\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2037 - val_loss: 0.2081\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2039 - val_loss: 0.2081\n",
      "Fold 1 NN: 0.20807\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 14ms/step - loss: 25.6566 - val_loss: 1.6543\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.1011 - val_loss: 0.9733\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.8101 - val_loss: 1.2899\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.1323 - val_loss: 0.8391\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.8537 - val_loss: 0.5552\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.7836 - val_loss: 0.7285\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6763 - val_loss: 1.1901\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.0350 - val_loss: 0.2397\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2926 - val_loss: 0.2799\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3044 - val_loss: 0.2815\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3052 - val_loss: 0.3467\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3394 - val_loss: 0.2916\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3152 - val_loss: 0.3920\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4840 - val_loss: 5.6690\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 3.2251 - val_loss: 0.2610\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2268 - val_loss: 0.2277\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2206 - val_loss: 0.2262\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2161 - val_loss: 0.2270\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2171 - val_loss: 0.2225\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2150 - val_loss: 0.2215\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2140 - val_loss: 0.2215\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2124 - val_loss: 0.2267\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2149 - val_loss: 0.2186\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2127 - val_loss: 0.2189\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2127 - val_loss: 0.2197\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2132 - val_loss: 0.2180\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2124 - val_loss: 0.2201\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2131 - val_loss: 0.2214\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2119 - val_loss: 0.2199\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2117 - val_loss: 0.2246\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2146 - val_loss: 0.2200\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2139 - val_loss: 0.2181\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2126 - val_loss: 0.2202\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2071 - val_loss: 0.2153\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2055 - val_loss: 0.2147\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2145\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2157\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2059 - val_loss: 0.2157\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2136\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2052 - val_loss: 0.2136\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2063 - val_loss: 0.2154\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2054 - val_loss: 0.2147\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2060 - val_loss: 0.2165\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2055 - val_loss: 0.2153\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2051 - val_loss: 0.2130\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2060 - val_loss: 0.2158\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2054 - val_loss: 0.2130\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2060 - val_loss: 0.2159\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2082 - val_loss: 0.2135\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2181\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2129\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2046 - val_loss: 0.2169\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2049 - val_loss: 0.2176\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2132\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2040 - val_loss: 0.2148\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2042 - val_loss: 0.2161\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2055 - val_loss: 0.2147\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2205\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2023 - val_loss: 0.2137\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2017 - val_loss: 0.2136\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2017 - val_loss: 0.2136\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2116\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2020 - val_loss: 0.2138\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2013 - val_loss: 0.2138\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2020 - val_loss: 0.2134\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2023 - val_loss: 0.2137\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2014 - val_loss: 0.2135\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2019 - val_loss: 0.2135\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2017 - val_loss: 0.2117\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2126\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2005 - val_loss: 0.2128\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2005 - val_loss: 0.2125\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2004 - val_loss: 0.2128\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2007 - val_loss: 0.2123\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2010 - val_loss: 0.2129\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2014 - val_loss: 0.2131\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2005 - val_loss: 0.2128\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2007 - val_loss: 0.2128\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2014 - val_loss: 0.2126\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2004 - val_loss: 0.2129\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2008 - val_loss: 0.2128\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2010 - val_loss: 0.2130\n",
      "Fold 2 NN: 0.2116\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 25.1836 - val_loss: 0.8318\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6045 - val_loss: 0.5699\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5980 - val_loss: 0.3885\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6727 - val_loss: 0.4867\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.5336 - val_loss: 0.5026\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4848 - val_loss: 0.4726\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5260 - val_loss: 0.4488\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4300 - val_loss: 0.2252\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2468 - val_loss: 0.6261\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 1.8201 - val_loss: 0.2895\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3098 - val_loss: 0.2348\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2461 - val_loss: 0.2219\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2338 - val_loss: 0.2568\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2368 - val_loss: 0.3503\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.7626 - val_loss: 0.3276\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2574 - val_loss: 0.2317\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2356 - val_loss: 0.2164\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2248 - val_loss: 0.2413\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2272 - val_loss: 0.2952\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2360 - val_loss: 0.2352\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2350 - val_loss: 0.2624\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2335 - val_loss: 0.2246\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2406 - val_loss: 0.2680\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2345 - val_loss: 0.2121\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2274 - val_loss: 0.2292\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2444 - val_loss: 0.2420\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2369 - val_loss: 0.2936\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2360 - val_loss: 0.2229\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.3068 - val_loss: 0.2209\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2415 - val_loss: 0.2167\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2225 - val_loss: 0.2460\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2118 - val_loss: 0.2112\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2066 - val_loss: 0.2112\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2103\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2057 - val_loss: 0.2119\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2052 - val_loss: 0.2094\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2046 - val_loss: 0.2094\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2042 - val_loss: 0.2098\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2059 - val_loss: 0.2098\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2048 - val_loss: 0.2113\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2041 - val_loss: 0.2105\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2057 - val_loss: 0.2100\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2059 - val_loss: 0.2126\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2021 - val_loss: 0.2091\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2019 - val_loss: 0.2093\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2017 - val_loss: 0.2098\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2019 - val_loss: 0.2095\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2017 - val_loss: 0.2093\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2017 - val_loss: 0.2094\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2018 - val_loss: 0.2093\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2014 - val_loss: 0.2098\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2010 - val_loss: 0.2091\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2019 - val_loss: 0.2093\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2093\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2003 - val_loss: 0.2091\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2006 - val_loss: 0.2092\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2011 - val_loss: 0.2095\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2095\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2007 - val_loss: 0.2092\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2008 - val_loss: 0.2092\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2007 - val_loss: 0.2091\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2012 - val_loss: 0.2092\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2009 - val_loss: 0.2091\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.1999 - val_loss: 0.2092\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2007 - val_loss: 0.2091\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2005 - val_loss: 0.2092\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2011 - val_loss: 0.2092\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2092\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2011 - val_loss: 0.2092\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2092\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2093\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2007 - val_loss: 0.2092\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2008 - val_loss: 0.2092\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2005 - val_loss: 0.2092\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.1994 - val_loss: 0.2092\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2006 - val_loss: 0.2092\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2003 - val_loss: 0.2092\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2004 - val_loss: 0.2092\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2001 - val_loss: 0.2092\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.1997 - val_loss: 0.2092\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2006 - val_loss: 0.2092\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2003 - val_loss: 0.2092\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2007 - val_loss: 0.2092\n",
      "Fold 3 NN: 0.20909\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 14ms/step - loss: 21.0162 - val_loss: 1.3988\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.8092 - val_loss: 0.5916\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6937 - val_loss: 0.4825\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.7655 - val_loss: 0.8508\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.8862 - val_loss: 0.5065\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6349 - val_loss: 1.6054\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 1.0557 - val_loss: 0.2387\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2608 - val_loss: 0.2433\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2572 - val_loss: 0.2786\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2551 - val_loss: 0.2766\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2777 - val_loss: 0.2326\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2827 - val_loss: 0.2890\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 1.4429 - val_loss: 0.2607\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2625 - val_loss: 0.2354\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2347 - val_loss: 0.3021\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2314 - val_loss: 0.2304\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2401 - val_loss: 0.2480\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2352 - val_loss: 0.2200\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2412 - val_loss: 0.2533\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2361 - val_loss: 0.3906\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2492 - val_loss: 0.2280\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2424 - val_loss: 0.2520\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2354 - val_loss: 0.2253\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2500 - val_loss: 0.2363\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2405 - val_loss: 0.2754\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2151 - val_loss: 0.2162\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2059 - val_loss: 0.2175\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2060 - val_loss: 0.2172\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2067 - val_loss: 0.2171\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2066 - val_loss: 0.2183\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2068 - val_loss: 0.2150\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2055 - val_loss: 0.2173\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2054 - val_loss: 0.2202\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2066 - val_loss: 0.2210\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2051 - val_loss: 0.2204\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2068 - val_loss: 0.2182\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2102 - val_loss: 0.2169\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2082 - val_loss: 0.2170\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2032 - val_loss: 0.2154\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2034 - val_loss: 0.2153\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2030 - val_loss: 0.2152\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2025 - val_loss: 0.2169\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2028 - val_loss: 0.2158\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2024 - val_loss: 0.2170\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2021 - val_loss: 0.2160\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2023 - val_loss: 0.2158\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2015 - val_loss: 0.2159\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2016 - val_loss: 0.2160\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2016 - val_loss: 0.2156\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2018 - val_loss: 0.2159\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2011 - val_loss: 0.2156\n",
      "Fold 4 NN: 0.21499\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 14ms/step - loss: 32.7954 - val_loss: 0.9868\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 1.0166 - val_loss: 0.6044\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.5945 - val_loss: 0.4791\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5925 - val_loss: 0.6527\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.5912 - val_loss: 0.7418\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.4774 - val_loss: 0.2477\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2717 - val_loss: 0.2551\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2943 - val_loss: 0.2622\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.3032 - val_loss: 0.2543\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.3138 - val_loss: 0.3060\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3080 - val_loss: 1.2261\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 5.7817 - val_loss: 0.3898\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4181 - val_loss: 0.2534\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2281 - val_loss: 0.2269\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2222 - val_loss: 0.2264\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2210 - val_loss: 0.2256\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2203 - val_loss: 0.2251\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2194 - val_loss: 0.2236\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2172 - val_loss: 0.2251\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2175 - val_loss: 0.2251\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2163 - val_loss: 0.2309\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2175 - val_loss: 0.2212\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2160 - val_loss: 0.2225\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 3s 19ms/step - loss: 0.2160 - val_loss: 0.2213\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2175 - val_loss: 0.2268\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2172 - val_loss: 0.2233\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2181 - val_loss: 0.2310\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2173 - val_loss: 0.2201\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2155 - val_loss: 0.2221\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2146 - val_loss: 0.2227\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2127 - val_loss: 0.2233\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2151 - val_loss: 0.2193\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2128 - val_loss: 0.2395\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2213 - val_loss: 0.2242\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2242 - val_loss: 0.2180\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2119 - val_loss: 0.2575\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2216 - val_loss: 0.2241\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2121 - val_loss: 0.2301\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2131 - val_loss: 0.2410\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2158 - val_loss: 0.2295\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2167 - val_loss: 0.2746\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2155 - val_loss: 0.2186\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2051 - val_loss: 0.2162\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2046 - val_loss: 0.2158\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2045 - val_loss: 0.2156\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2047 - val_loss: 0.2168\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2049 - val_loss: 0.2157\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2045 - val_loss: 0.2228\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2052 - val_loss: 0.2200\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2170\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2148\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2041 - val_loss: 0.2161\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2049 - val_loss: 0.2158\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2039 - val_loss: 0.2182\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2061 - val_loss: 0.2169\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2035 - val_loss: 0.2189\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2043 - val_loss: 0.2151\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2048 - val_loss: 0.2160\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2016 - val_loss: 0.2157\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2017 - val_loss: 0.2160\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2018 - val_loss: 0.2154\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2023 - val_loss: 0.2149\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2015 - val_loss: 0.2159\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2015 - val_loss: 0.2149\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2018 - val_loss: 0.2148\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2015 - val_loss: 0.2150\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2016 - val_loss: 0.2148\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2018 - val_loss: 0.2147\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2019 - val_loss: 0.2151\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2012 - val_loss: 0.2147\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2015 - val_loss: 0.2149\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2011 - val_loss: 0.2148\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2026 - val_loss: 0.2149\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2016 - val_loss: 0.2148\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2018 - val_loss: 0.2149\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2016 - val_loss: 0.2149\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2017 - val_loss: 0.2148\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2010 - val_loss: 0.2149\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2010 - val_loss: 0.2149\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2015 - val_loss: 0.2148\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2009 - val_loss: 0.2148\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2013 - val_loss: 0.2148\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2024 - val_loss: 0.2149\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2015 - val_loss: 0.2149\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2014 - val_loss: 0.2149\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2010 - val_loss: 0.2148\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2013 - val_loss: 0.2149\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2009 - val_loss: 0.2148\n",
      "Fold 5 NN: 0.21466\n"
     ]
    }
   ],
   "source": [
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train_nn)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "test_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "\n",
    "train_nn[pred_name] = 0\n",
    "test_nn[target_name] = 0\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "    \n",
    "    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n",
    "    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.006),\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
    "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83ba39de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:45:35.542998Z",
     "iopub.status.busy": "2021-09-19T04:45:35.542127Z",
     "iopub.status.idle": "2021-09-19T04:45:35.545931Z",
     "shell.execute_reply": "2021-09-19T04:45:35.546362Z"
    },
    "papermill": {
     "duration": 4.337048,
     "end_time": "2021-09-19T04:45:35.546573",
     "exception": false,
     "start_time": "2021-09-19T04:45:31.209525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0016537 , 0.00240042, 0.00240042])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c0f048f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:45:44.820906Z",
     "iopub.status.busy": "2021-09-19T04:45:44.548335Z",
     "iopub.status.idle": "2021-09-19T04:59:42.597136Z",
     "shell.execute_reply": "2021-09-19T04:59:42.596499Z"
    },
    "papermill": {
     "duration": 842.766344,
     "end_time": "2021-09-19T04:59:42.597290",
     "exception": false,
     "start_time": "2021-09-19T04:45:39.830946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 14ms/step - loss: 26.3398 - val_loss: 2.0143\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.6007 - val_loss: 0.5580\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6841 - val_loss: 0.6334\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5943 - val_loss: 0.6410\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5986 - val_loss: 0.5516\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5177 - val_loss: 0.6237\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5554 - val_loss: 0.5768\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4849 - val_loss: 0.7349\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.7410 - val_loss: 0.3881\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4388 - val_loss: 0.4234\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4326 - val_loss: 0.3831\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.7784 - val_loss: 0.3333\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.3599 - val_loss: 0.3645\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.4145 - val_loss: 0.3956\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3474 - val_loss: 0.4381\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3388 - val_loss: 0.2442\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2309 - val_loss: 0.2143\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2227 - val_loss: 0.2360\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2343 - val_loss: 0.2205\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2321 - val_loss: 0.2128\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2409 - val_loss: 0.2529\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2576 - val_loss: 0.2118\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2350 - val_loss: 0.3552\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2538 - val_loss: 0.2878\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2671 - val_loss: 0.2927\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2743 - val_loss: 0.3556\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.5041 - val_loss: 0.2240\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2302 - val_loss: 0.2151\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2288 - val_loss: 0.2178\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2158 - val_loss: 0.2126\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2147 - val_loss: 0.2125\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2145 - val_loss: 0.2134\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2148 - val_loss: 0.2117\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2133 - val_loss: 0.2137\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2132 - val_loss: 0.2139\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2136 - val_loss: 0.2133\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2112 - val_loss: 0.2136\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2118 - val_loss: 0.2121\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2117 - val_loss: 0.2101\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2128 - val_loss: 0.2102\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2111 - val_loss: 0.2118\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2129 - val_loss: 0.2168\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2125 - val_loss: 0.2130\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2118 - val_loss: 0.2184\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2117 - val_loss: 0.2098\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2110 - val_loss: 0.2136\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2119 - val_loss: 0.2121\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2108 - val_loss: 0.2121\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2091 - val_loss: 0.2172\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2109 - val_loss: 0.2100\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2111 - val_loss: 0.2113\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2106 - val_loss: 0.2262\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2081\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2055 - val_loss: 0.2081\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2050 - val_loss: 0.2092\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2048 - val_loss: 0.2085\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2052 - val_loss: 0.2088\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2052 - val_loss: 0.2084\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2047 - val_loss: 0.2084\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2049 - val_loss: 0.2086\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2035 - val_loss: 0.2081\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2043 - val_loss: 0.2082\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2037 - val_loss: 0.2082\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2036 - val_loss: 0.2083\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2036 - val_loss: 0.2083\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2028 - val_loss: 0.2085\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2048 - val_loss: 0.2083\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2031 - val_loss: 0.2081\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2035 - val_loss: 0.2080\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2035 - val_loss: 0.2082\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2033 - val_loss: 0.2082\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2035 - val_loss: 0.2082\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2030 - val_loss: 0.2082\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2038 - val_loss: 0.2082\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2034 - val_loss: 0.2081\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2036 - val_loss: 0.2082\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2030 - val_loss: 0.2081\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2035 - val_loss: 0.2081\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2027 - val_loss: 0.2081\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2035 - val_loss: 0.2081\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2027 - val_loss: 0.2081\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2033 - val_loss: 0.2081\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2037 - val_loss: 0.2081\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2034 - val_loss: 0.2082\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2031 - val_loss: 0.2081\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2035 - val_loss: 0.2081\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2045 - val_loss: 0.2081\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2034 - val_loss: 0.2081\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2027 - val_loss: 0.2081\n",
      "Fold 1 NN: 0.20803\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 4s 17ms/step - loss: 24.2446 - val_loss: 5.1467\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 2.2858 - val_loss: 0.5941\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.7485 - val_loss: 1.0105\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.7604 - val_loss: 0.5179\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6665 - val_loss: 0.4879\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6431 - val_loss: 0.6109\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6972 - val_loss: 0.4800\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5876 - val_loss: 0.4551\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5302 - val_loss: 0.5067\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.9356 - val_loss: 0.2535\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2477 - val_loss: 0.2424\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5847 - val_loss: 0.4392\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.4180 - val_loss: 0.4191\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.3800 - val_loss: 0.4238\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3726 - val_loss: 0.4643\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3536 - val_loss: 0.4893\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3651 - val_loss: 0.3126\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3196 - val_loss: 0.3322\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2249 - val_loss: 0.2184\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2094 - val_loss: 0.2173\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2087 - val_loss: 0.2179\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2090 - val_loss: 0.2165\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2079 - val_loss: 0.2174\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2108 - val_loss: 0.2212\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2091 - val_loss: 0.2190\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2079 - val_loss: 0.2165\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2087 - val_loss: 0.2158\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2082 - val_loss: 0.2149\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2094 - val_loss: 0.2187\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2167\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2078 - val_loss: 0.2209\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2084 - val_loss: 0.2174\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2103 - val_loss: 0.2187\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2087 - val_loss: 0.2169\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2071 - val_loss: 0.2150\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2048 - val_loss: 0.2141\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2031 - val_loss: 0.2145\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2040 - val_loss: 0.2145\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2038 - val_loss: 0.2143\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2041 - val_loss: 0.2163\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2038 - val_loss: 0.2141\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2027 - val_loss: 0.2137\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2032 - val_loss: 0.2136\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2037 - val_loss: 0.2142\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2041 - val_loss: 0.2146\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2037 - val_loss: 0.2142\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2037 - val_loss: 0.2155\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2032 - val_loss: 0.2194\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2032 - val_loss: 0.2142\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2022 - val_loss: 0.2153\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2020 - val_loss: 0.2145\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2010 - val_loss: 0.2149\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2013 - val_loss: 0.2136\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2016 - val_loss: 0.2133\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2011 - val_loss: 0.2146\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2018 - val_loss: 0.2145\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2014 - val_loss: 0.2160\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2016 - val_loss: 0.2136\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2012 - val_loss: 0.2138\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2012 - val_loss: 0.2152\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2018 - val_loss: 0.2158\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2139\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2009 - val_loss: 0.2137\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2004 - val_loss: 0.2138\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2007 - val_loss: 0.2142\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2003 - val_loss: 0.2145\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2012 - val_loss: 0.2153\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2004 - val_loss: 0.2145\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2005 - val_loss: 0.2140\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2001 - val_loss: 0.2142\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2004 - val_loss: 0.2142\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2009 - val_loss: 0.2144\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2006 - val_loss: 0.2143\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2004 - val_loss: 0.2143\n",
      "Fold 2 NN: 0.21334\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 26.2787 - val_loss: 0.7386\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6454 - val_loss: 0.6126\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6148 - val_loss: 0.7003\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5914 - val_loss: 0.5410\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6358 - val_loss: 0.6294\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6562 - val_loss: 0.2303\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2825 - val_loss: 0.8459\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5062 - val_loss: 0.2293\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2683 - val_loss: 2.2393\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 1.3307 - val_loss: 0.4059\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.3659 - val_loss: 0.2411\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2381 - val_loss: 0.2423\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2292 - val_loss: 0.2420\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2409 - val_loss: 0.2201\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2592 - val_loss: 2.7352\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.0319 - val_loss: 0.2399\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2253 - val_loss: 0.2190\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2283 - val_loss: 0.2132\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2219 - val_loss: 0.2147\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2267 - val_loss: 0.2131\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2261 - val_loss: 0.2179\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2304 - val_loss: 0.3349\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2400 - val_loss: 0.2650\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2289 - val_loss: 0.2177\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2273 - val_loss: 0.2117\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2333 - val_loss: 0.2308\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2297 - val_loss: 0.2237\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2462 - val_loss: 0.2693\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2334 - val_loss: 0.2524\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2389 - val_loss: 0.2166\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2391 - val_loss: 0.2171\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2514 - val_loss: 0.2631\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2145 - val_loss: 0.2093\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2071 - val_loss: 0.2089\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2067 - val_loss: 0.2093\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2051 - val_loss: 0.2106\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2058 - val_loss: 0.2090\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2096\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2052 - val_loss: 0.2113\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2072 - val_loss: 0.2137\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2058 - val_loss: 0.2190\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2055 - val_loss: 0.2083\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2037 - val_loss: 0.2090\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2035 - val_loss: 0.2090\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2029 - val_loss: 0.2089\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2029 - val_loss: 0.2088\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2036 - val_loss: 0.2086\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2032 - val_loss: 0.2088\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2030 - val_loss: 0.2085\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2027 - val_loss: 0.2083\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2021 - val_loss: 0.2086\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2025 - val_loss: 0.2085\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2026 - val_loss: 0.2084\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2030 - val_loss: 0.2084\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 3s 16ms/step - loss: 0.2014 - val_loss: 0.2084\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2026 - val_loss: 0.2090\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2020 - val_loss: 0.2085\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2024 - val_loss: 0.2084\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2022 - val_loss: 0.2084\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2022 - val_loss: 0.2085\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2020 - val_loss: 0.2084\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2016 - val_loss: 0.2085\n",
      "Fold 3 NN: 0.20828\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 14ms/step - loss: 25.7808 - val_loss: 0.7039\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.4631 - val_loss: 4.8044\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 1.3422 - val_loss: 1.3535\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.8355 - val_loss: 0.4593\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.6996 - val_loss: 0.8069\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6630 - val_loss: 0.2892\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6936 - val_loss: 0.6218\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6377 - val_loss: 0.4625\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5848 - val_loss: 0.6412\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.7436 - val_loss: 0.4146\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.5317 - val_loss: 0.4330\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.6518 - val_loss: 0.4572\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6182 - val_loss: 0.7076\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2820 - val_loss: 0.2330\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2180 - val_loss: 0.2243\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2145 - val_loss: 0.2275\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2142 - val_loss: 0.2219\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2116 - val_loss: 0.2405\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2159 - val_loss: 0.2218\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 3s 17ms/step - loss: 0.2126 - val_loss: 0.2245\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2135 - val_loss: 0.2245\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2159 - val_loss: 0.2215\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2120 - val_loss: 0.2182\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2114 - val_loss: 0.2301\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2118 - val_loss: 0.2198\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2105 - val_loss: 0.2189\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2108 - val_loss: 0.2218\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2149 - val_loss: 0.2214\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2273\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2257 - val_loss: 0.2269\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2066 - val_loss: 0.2162\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2051 - val_loss: 0.2163\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2050 - val_loss: 0.2158\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2046 - val_loss: 0.2157\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2052 - val_loss: 0.2165\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2050 - val_loss: 0.2163\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2052 - val_loss: 0.2157\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2047 - val_loss: 0.2152\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2047 - val_loss: 0.2160\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2048 - val_loss: 0.2215\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2054 - val_loss: 0.2175\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2047 - val_loss: 0.2154\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2040 - val_loss: 0.2166\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2049 - val_loss: 0.2163\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2061 - val_loss: 0.2168\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2021 - val_loss: 0.2146\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2024 - val_loss: 0.2150\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2022 - val_loss: 0.2153\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2020 - val_loss: 0.2154\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2021 - val_loss: 0.2149\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2023 - val_loss: 0.2157\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2019 - val_loss: 0.2146\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2018 - val_loss: 0.2153\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2024 - val_loss: 0.2148\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2016 - val_loss: 0.2147\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2013 - val_loss: 0.2148\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2019 - val_loss: 0.2149\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2013 - val_loss: 0.2148\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2019 - val_loss: 0.2147\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2014 - val_loss: 0.2149\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2009 - val_loss: 0.2147\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2015 - val_loss: 0.2146\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2016 - val_loss: 0.2147\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2016 - val_loss: 0.2147\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2016 - val_loss: 0.2147\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2015 - val_loss: 0.2147\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2018 - val_loss: 0.2148\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2019 - val_loss: 0.2148\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2011 - val_loss: 0.2147\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2014 - val_loss: 0.2147\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2020 - val_loss: 0.2147\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2010 - val_loss: 0.2147\n",
      "Fold 4 NN: 0.21458\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 14ms/step - loss: 29.6790 - val_loss: 2.4554\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 1.6533 - val_loss: 0.3735\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.6099 - val_loss: 0.6528\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5959 - val_loss: 0.7234\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.5686 - val_loss: 0.5676\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.5771 - val_loss: 0.5019\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.5200 - val_loss: 1.1369\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.9446 - val_loss: 0.4941\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.4483 - val_loss: 0.5814\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2768 - val_loss: 0.2231\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2169 - val_loss: 0.2228\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2178 - val_loss: 0.2225\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2165 - val_loss: 0.2216\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2162 - val_loss: 0.2208\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2129 - val_loss: 0.2242\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2143 - val_loss: 0.2217\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2135 - val_loss: 0.2300\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2147 - val_loss: 0.2212\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2139 - val_loss: 0.2226\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2186 - val_loss: 0.2293\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2159 - val_loss: 0.2240\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2104 - val_loss: 0.2178\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2068 - val_loss: 0.2179\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2177\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2081 - val_loss: 0.2198\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2082 - val_loss: 0.2186\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2074 - val_loss: 0.2184\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2072 - val_loss: 0.2171\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2186\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2067 - val_loss: 0.2174\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2064 - val_loss: 0.2178\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2071 - val_loss: 0.2202\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2073 - val_loss: 0.2209\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2082 - val_loss: 0.2185\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2078 - val_loss: 0.2177\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2061 - val_loss: 0.2175\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2048 - val_loss: 0.2167\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2166\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2049 - val_loss: 0.2166\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2048 - val_loss: 0.2164\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2051 - val_loss: 0.2168\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2047 - val_loss: 0.2170\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2051 - val_loss: 0.2169\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2041 - val_loss: 0.2165\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2049 - val_loss: 0.2182\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2047 - val_loss: 0.2171\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 2s 15ms/step - loss: 0.2052 - val_loss: 0.2165\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 3s 15ms/step - loss: 0.2037 - val_loss: 0.2165\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2044 - val_loss: 0.2164\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2037 - val_loss: 0.2165\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2034 - val_loss: 0.2162\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2034 - val_loss: 0.2167\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2038 - val_loss: 0.2164\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2038 - val_loss: 0.2165\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2038 - val_loss: 0.2168\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 2s 12ms/step - loss: 0.2038 - val_loss: 0.2166\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2036 - val_loss: 0.2169\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2044 - val_loss: 0.2171\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2036 - val_loss: 0.2162\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2033 - val_loss: 0.2163\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2037 - val_loss: 0.2164\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2039 - val_loss: 0.2164\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2039 - val_loss: 0.2164\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 2s 14ms/step - loss: 0.2035 - val_loss: 0.2167\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2036 - val_loss: 0.2166\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2032 - val_loss: 0.2164\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2036 - val_loss: 0.2164\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2038 - val_loss: 0.2164\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2032 - val_loss: 0.2164\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2034 - val_loss: 0.2165\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 2s 13ms/step - loss: 0.2034 - val_loss: 0.2163\n",
      "Fold 5 NN: 0.21623\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(41)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(41)\n",
    "from tensorflow import keras\n",
    "\n",
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2021)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train1)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train1[features_to_consider] = train1[features_to_consider].fillna(train1[features_to_consider].mean())\n",
    "test1[features_to_consider] = test1[features_to_consider].fillna(train1[features_to_consider].mean())\n",
    "\n",
    "train1[pred_name] = 0\n",
    "test1[target_name] = 0\n",
    "test_predictions_nn1 = np.zeros(test_nn.shape[0])\n",
    "\n",
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "    \n",
    "    X_train = train1.loc[train1.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train1.loc[train1.time_id.isin(indexes), target_name]\n",
    "    X_test = train1.loc[train1.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train1.loc[train1.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.006),\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
    "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "    test_predictions_nn1 += model.predict([test1['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d515a75a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T04:59:59.532567Z",
     "iopub.status.busy": "2021-09-19T04:59:59.531888Z",
     "iopub.status.idle": "2021-09-19T04:59:59.558490Z",
     "shell.execute_reply": "2021-09-19T04:59:59.558977Z"
    },
    "papermill": {
     "duration": 8.511812,
     "end_time": "2021-09-19T04:59:59.559156",
     "exception": false,
     "start_time": "2021-09-19T04:59:51.047344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wap1_sum</th>\n",
       "      <th>wap1_std</th>\n",
       "      <th>wap2_sum</th>\n",
       "      <th>wap2_std</th>\n",
       "      <th>wap3_sum</th>\n",
       "      <th>wap3_std</th>\n",
       "      <th>wap4_sum</th>\n",
       "      <th>wap4_std</th>\n",
       "      <th>log_return1_realized_volatility</th>\n",
       "      <th>log_return2_realized_volatility</th>\n",
       "      <th>...</th>\n",
       "      <th>bid_ask_spread_sum_1c1</th>\n",
       "      <th>bid_ask_spread_sum_3c1</th>\n",
       "      <th>bid_ask_spread_sum_4c1</th>\n",
       "      <th>bid_ask_spread_sum_6c1</th>\n",
       "      <th>size_tau2_0c1</th>\n",
       "      <th>size_tau2_1c1</th>\n",
       "      <th>size_tau2_3c1</th>\n",
       "      <th>size_tau2_4c1</th>\n",
       "      <th>size_tau2_6c1</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-2.429043</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-2.723074</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-2.722340</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.714581</td>\n",
       "      <td>-3.421779</td>\n",
       "      <td>-3.467215</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.199337</td>\n",
       "      <td>0.035373</td>\n",
       "      <td>0.221821</td>\n",
       "      <td>0.431936</td>\n",
       "      <td>-0.086921</td>\n",
       "      <td>3.161571</td>\n",
       "      <td>0.590543</td>\n",
       "      <td>-0.161021</td>\n",
       "      <td>-0.482473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212994</td>\n",
       "      <td>0.035373</td>\n",
       "      <td>0.221821</td>\n",
       "      <td>0.431936</td>\n",
       "      <td>-0.086921</td>\n",
       "      <td>-0.085898</td>\n",
       "      <td>0.590543</td>\n",
       "      <td>-0.161021</td>\n",
       "      <td>-0.482473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212994</td>\n",
       "      <td>0.035373</td>\n",
       "      <td>0.221821</td>\n",
       "      <td>0.431936</td>\n",
       "      <td>-0.086921</td>\n",
       "      <td>-0.085898</td>\n",
       "      <td>0.590543</td>\n",
       "      <td>-0.161021</td>\n",
       "      <td>-0.482473</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows  247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   wap1_sum  wap1_std  wap2_sum  wap2_std  wap3_sum  wap3_std  wap4_sum  \\\n",
       "0 -5.199338 -2.429043 -5.199338 -2.723074 -5.199338 -2.722340 -5.199338   \n",
       "1  0.000022  0.000281  0.000025  0.000382  0.000019  0.000336  0.000018   \n",
       "2  0.000022  0.000281  0.000025  0.000382  0.000019  0.000336  0.000018   \n",
       "\n",
       "   wap4_std  log_return1_realized_volatility  log_return2_realized_volatility  \\\n",
       "0 -3.714581                        -3.421779                        -3.467215   \n",
       "1  0.000429                         0.001459                         0.001485   \n",
       "2  0.000429                         0.001459                         0.001485   \n",
       "\n",
       "   ...  bid_ask_spread_sum_1c1  bid_ask_spread_sum_3c1  \\\n",
       "0  ...               -5.199337                0.035373   \n",
       "1  ...               -0.212994                0.035373   \n",
       "2  ...               -0.212994                0.035373   \n",
       "\n",
       "   bid_ask_spread_sum_4c1  bid_ask_spread_sum_6c1  size_tau2_0c1  \\\n",
       "0                0.221821                0.431936      -0.086921   \n",
       "1                0.221821                0.431936      -0.086921   \n",
       "2                0.221821                0.431936      -0.086921   \n",
       "\n",
       "   size_tau2_1c1  size_tau2_3c1  size_tau2_4c1  size_tau2_6c1  target  \n",
       "0       3.161571       0.590543      -0.161021      -0.482473       0  \n",
       "1      -0.085898       0.590543      -0.161021      -0.482473       0  \n",
       "2      -0.085898       0.590543      -0.161021      -0.482473       0  \n",
       "\n",
       "[3 rows x 247 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62485bea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-19T05:00:16.489981Z",
     "iopub.status.busy": "2021-09-19T05:00:16.489142Z",
     "iopub.status.idle": "2021-09-19T05:00:16.511922Z",
     "shell.execute_reply": "2021-09-19T05:00:16.512503Z"
    },
    "papermill": {
     "duration": 8.466644,
     "end_time": "2021-09-19T05:00:16.512702",
     "exception": false,
     "start_time": "2021-09-19T05:00:08.046058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.002380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.002183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.002183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.002380\n",
       "1   0-32  0.002183\n",
       "2   0-34  0.002183"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test=pd.read_csv(\"../input/optiver-realized-volatility-prediction/test.csv\")\n",
    "a=test_predictions_nn*0.60+predictions_lgb2*0.40\n",
    "b=test_predictions_nn1*0.55+predictions_lgb1*0.45\n",
    "test[target_name] = (a+b)/2\n",
    "\n",
    "display(test[['row_id', target_name]].head(3))\n",
    "test[['row_id', target_name]].to_csv('submission.csv',index = False)\n",
    "#test[['row_id', target_name]].to_csv('submission.csv',index = False)\n",
    "#kmeans N=5 [0.2101, 0.21399, 0.20923, 0.21398, 0.21175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c115e7",
   "metadata": {
    "papermill": {
     "duration": 8.419246,
     "end_time": "2021-09-19T05:00:33.364642",
     "exception": false,
     "start_time": "2021-09-19T05:00:24.945396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb90ac3",
   "metadata": {
    "papermill": {
     "duration": 8.395786,
     "end_time": "2021-09-19T05:00:50.241376",
     "exception": false,
     "start_time": "2021-09-19T05:00:41.845590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14136b",
   "metadata": {
    "papermill": {
     "duration": 8.524972,
     "end_time": "2021-09-19T05:01:07.159652",
     "exception": false,
     "start_time": "2021-09-19T05:00:58.634680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4dc114",
   "metadata": {
    "papermill": {
     "duration": 8.442618,
     "end_time": "2021-09-19T05:01:24.132057",
     "exception": false,
     "start_time": "2021-09-19T05:01:15.689439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb5275",
   "metadata": {
    "papermill": {
     "duration": 8.585977,
     "end_time": "2021-09-19T05:01:41.134088",
     "exception": false,
     "start_time": "2021-09-19T05:01:32.548111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3403.770759,
   "end_time": "2021-09-19T05:01:51.946814",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-19T04:05:08.176055",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
