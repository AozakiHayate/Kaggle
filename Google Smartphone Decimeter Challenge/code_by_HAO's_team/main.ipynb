{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"v01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_to_areas = {\n",
    "    '2020-05-14-US-MTV-1': 'highway',\n",
    "    '2020-05-14-US-MTV-2': 'highway',\n",
    "    '2020-05-21-US-MTV-1': 'highway',\n",
    "    '2020-05-21-US-MTV-2': 'highway',\n",
    "    '2020-05-29-US-MTV-1': 'highway',\n",
    "    '2020-05-29-US-MTV-2': 'highway',\n",
    "    '2020-06-04-US-MTV-1': 'highway',\n",
    "    '2020-06-05-US-MTV-1': 'highway',\n",
    "    '2020-06-05-US-MTV-2': 'highway',\n",
    "    '2020-06-11-US-MTV-1': 'highway',\n",
    "    '2020-07-08-US-MTV-1': 'highway',\n",
    "    '2020-07-17-US-MTV-1': 'highway',\n",
    "    '2020-07-17-US-MTV-2': 'highway',\n",
    "    '2020-08-03-US-MTV-1': 'highway',\n",
    "    '2020-08-06-US-MTV-2': 'highway',\n",
    "    '2020-09-04-US-SF-1': 'highway',\n",
    "    '2020-09-04-US-SF-2': 'highway',\n",
    "    '2021-01-04-US-RWC-1': 'highway',\n",
    "    '2021-01-04-US-RWC-2': 'highway',\n",
    "    '2021-01-05-US-SVL-1': 'highway',\n",
    "    '2021-01-05-US-SVL-2': 'highway',\n",
    "    '2021-03-10-US-SVL-1': 'tree',\n",
    "    '2021-04-15-US-MTV-1': 'tree',\n",
    "    '2021-04-22-US-SJC-1': 'downtown',\n",
    "    '2021-04-26-US-SVL-1': 'tree',\n",
    "    '2021-04-28-US-MTV-1': 'tree',\n",
    "    '2021-04-28-US-SJC-1': 'downtown',\n",
    "    '2021-04-29-US-MTV-1': 'tree',\n",
    "    '2021-04-29-US-SJC-2': 'downtown',\n",
    "'2020-05-15-US-MTV-1':'highway',\n",
    " '2020-05-28-US-MTV-1':'highway',\n",
    " '2020-05-28-US-MTV-2':'highway',\n",
    " '2020-06-04-US-MTV-2':'highway',\n",
    " '2020-06-10-US-MTV-1':'highway',\n",
    " '2020-06-10-US-MTV-2':'highway',\n",
    " '2020-08-03-US-MTV-2':'highway',\n",
    " '2020-08-13-US-MTV-1':'highway',\n",
    " '2021-03-16-US-MTV-2':'highway',\n",
    " '2021-03-16-US-RWC-2':'tree',\n",
    " '2021-03-25-US-PAO-1':'tree',\n",
    " '2021-04-02-US-SJC-1':'tree',\n",
    " '2021-04-08-US-MTV-1':'tree',\n",
    " '2021-04-21-US-MTV-1':'tree',\n",
    " '2021-04-22-US-SJC-2': 'downtown',\n",
    " '2021-04-26-US-SVL-2':'tree',\n",
    " '2021-04-28-US-MTV-2':'tree',\n",
    " '2021-04-29-US-MTV-2':'tree',\n",
    " '2021-04-29-US-SJC-3': 'downtown',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-12T12:03:35.599766Z",
     "iopub.status.busy": "2021-06-12T12:03:35.599038Z",
     "iopub.status.idle": "2021-06-12T12:03:37.812400Z",
     "shell.execute_reply": "2021-06-12T12:03:37.811454Z",
     "shell.execute_reply.started": "2021-06-12T12:03:35.599611Z"
    }
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2, venn2_circles\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold,GroupKFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T12:03:49.725589Z",
     "iopub.status.busy": "2021-06-12T12:03:49.725204Z",
     "iopub.status.idle": "2021-06-12T12:03:49.732225Z",
     "shell.execute_reply": "2021-06-12T12:03:49.731429Z",
     "shell.execute_reply.started": "2021-06-12T12:03:49.725558Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculates the great circle distance between two points\n",
    "    on the earth. Inputs are array-like and specified in decimal degrees.\n",
    "    \"\"\"\n",
    "    RADIUS = 6_367_000\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    dist = 2 * RADIUS * np.arcsin(a**0.5)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T12:03:50.589856Z",
     "iopub.status.busy": "2021-06-12T12:03:50.589045Z",
     "iopub.status.idle": "2021-06-12T12:03:50.600279Z",
     "shell.execute_reply": "2021-06-12T12:03:50.599180Z",
     "shell.execute_reply.started": "2021-06-12T12:03:50.589807Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_trafic(df, center, zoom=9):\n",
    "    fig = px.scatter_mapbox(df,\n",
    "                            \n",
    "                            # Here, plotly gets, (x,y) coordinates\n",
    "                            lat=\"latDeg\",\n",
    "                            lon=\"lngDeg\",\n",
    "                            \n",
    "                            #Here, plotly detects color of series\n",
    "                            color=\"phoneName\",\n",
    "                            labels=\"phoneName\",\n",
    "                            \n",
    "                            zoom=zoom,\n",
    "                            center=center,\n",
    "                            height=600,\n",
    "                            width=800)\n",
    "    fig.update_layout(mapbox_style='stamen-terrain')\n",
    "    fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "    fig.update_layout(title_text=\"GPS trafic\")\n",
    "    fig.show()\n",
    "    \n",
    "def visualize_collection(df, collection):\n",
    "    target_df = df[df['collectionName']==collection].copy()\n",
    "    lat_center = target_df['latDeg'].mean()\n",
    "    lng_center = target_df['lngDeg'].mean()\n",
    "    center = {\"lat\":lat_center, \"lon\":lng_center}\n",
    "    \n",
    "    visualize_trafic(target_df, center)\n",
    "    \n",
    "def visualize_collection_with_gt(df,ground_truth,collection):\n",
    "    tmp1 = df.copy()\n",
    "    tmp1['phoneName'] = tmp1['phoneName'] + '_BASE'\n",
    "    tmp2 = ground_truth.copy()\n",
    "    tmp2['phoneName'] = tmp2['phoneName'] + '_GT'\n",
    "    tmp = pd.concat([tmp1, tmp2])    \n",
    "    visualize_collection(tmp,collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T12:03:51.612679Z",
     "iopub.status.busy": "2021-06-12T12:03:51.612310Z",
     "iopub.status.idle": "2021-06-12T12:03:51.616868Z",
     "shell.execute_reply": "2021-06-12T12:03:51.615772Z",
     "shell.execute_reply.started": "2021-06-12T12:03:51.612647Z"
    }
   },
   "outputs": [],
   "source": [
    "# directory setting\n",
    "INPUT = '../input/google-smartphone-decimeter-challenge/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T12:03:54.291454Z",
     "iopub.status.busy": "2021-06-12T12:03:54.291105Z",
     "iopub.status.idle": "2021-06-12T12:03:54.913204Z",
     "shell.execute_reply": "2021-06-12T12:03:54.912299Z",
     "shell.execute_reply.started": "2021-06-12T12:03:54.291412Z"
    }
   },
   "outputs": [],
   "source": [
    "base_train = pd.read_csv(INPUT + 'baseline_locations_train.csv')\n",
    "base_test = pd.read_csv(INPUT +  'baseline_locations_test.csv')\n",
    "sample_sub = pd.read_csv(INPUT + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T12:03:57.831198Z",
     "iopub.status.busy": "2021-06-12T12:03:57.830854Z",
     "iopub.status.idle": "2021-06-12T12:03:58.961601Z",
     "shell.execute_reply": "2021-06-12T12:03:58.960770Z",
     "shell.execute_reply.started": "2021-06-12T12:03:57.831167Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 226.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# ground_truth\n",
    "p = pathlib.Path(INPUT)\n",
    "gt_files = list(p.glob('train/*/*/ground_truth.csv'))\n",
    "gts = []\n",
    "for gt_file in tqdm(gt_files):\n",
    "    gts.append(pd.read_csv(gt_file))\n",
    "ground_truth = pd.concat(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth[\"area\"] = ground_truth[\"collectionName\"].map(collections_to_areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reject outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T12:07:53.776549Z",
     "iopub.status.busy": "2021-06-12T12:07:53.776161Z",
     "iopub.status.idle": "2021-06-12T12:07:53.785253Z",
     "shell.execute_reply": "2021-06-12T12:07:53.784497Z",
     "shell.execute_reply.started": "2021-06-12T12:07:53.776515Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_distance_diff(df):\n",
    "    df['latDeg_prev'] = df['latDeg'].shift(1)\n",
    "    df['latDeg_next'] = df['latDeg'].shift(-1)\n",
    "    df['lngDeg_prev'] = df['lngDeg'].shift(1)\n",
    "    df['lngDeg_next'] = df['lngDeg'].shift(-1)\n",
    "    df['phone_prev'] = df['phone'].shift(1)\n",
    "    df['phone_next'] = df['phone'].shift(-1)\n",
    "    \n",
    "    df['dist_prev'] = calc_haversine(df['latDeg'], df['lngDeg'], df['latDeg_prev'], df['lngDeg_prev'])\n",
    "    df['dist_next'] = calc_haversine(df['latDeg'], df['lngDeg'], df['latDeg_next'], df['lngDeg_next'])\n",
    "    \n",
    "    df.loc[df['phone']!=df['phone_prev'], ['latDeg_prev', 'lngDeg_prev', 'dist_prev']] = np.nan\n",
    "    df.loc[df['phone']!=df['phone_next'], ['latDeg_next', 'lngDeg_next', 'dist_next']] = np.nan\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kalman filter\n",
    "https://www.kaggle.com/emaerthin/demonstration-of-the-kalman-filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T12:07:57.192669Z",
     "iopub.status.busy": "2021-06-12T12:07:57.191983Z",
     "iopub.status.idle": "2021-06-12T12:08:05.796559Z",
     "shell.execute_reply": "2021-06-12T12:08:05.795250Z",
     "shell.execute_reply.started": "2021-06-12T12:07:57.192630Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install simdkalman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T12:08:05.799282Z",
     "iopub.status.busy": "2021-06-12T12:08:05.798800Z",
     "iopub.status.idle": "2021-06-12T12:08:05.809860Z",
     "shell.execute_reply": "2021-06-12T12:08:05.808912Z",
     "shell.execute_reply.started": "2021-06-12T12:08:05.799226Z"
    }
   },
   "outputs": [],
   "source": [
    "import simdkalman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T12:08:05.812426Z",
     "iopub.status.busy": "2021-06-12T12:08:05.812097Z",
     "iopub.status.idle": "2021-06-12T12:08:05.826464Z",
     "shell.execute_reply": "2021-06-12T12:08:05.825096Z",
     "shell.execute_reply.started": "2021-06-12T12:08:05.812386Z"
    }
   },
   "outputs": [],
   "source": [
    "T = 1.0\n",
    "state_transition = np.array([[1, 0, T, 0, 0.5 * T ** 2, 0], [0, 1, 0, T, 0, 0.5 * T ** 2], [0, 0, 1, 0, T, 0],\n",
    "                             [0, 0, 0, 1, 0, T], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\n",
    "process_noise = np.diag([1e-5, 1e-5, 5e-6, 5e-6, 1e-6, 1e-6]) + np.ones((6, 6)) * 1e-9\n",
    "observation_model = np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]])\n",
    "observation_noise = np.diag([5e-5, 5e-5]) + np.ones((2, 2)) * 1e-9\n",
    "\n",
    "kf = simdkalman.KalmanFilter(\n",
    "        state_transition = state_transition,\n",
    "        process_noise = process_noise,\n",
    "        observation_model = observation_model,\n",
    "        observation_noise = observation_noise)\n",
    "\n",
    "def apply_kf_smoothing(df, kf_=kf):\n",
    "    unique_paths = df[['collectionName', 'phoneName']].drop_duplicates().to_numpy()\n",
    "    for collection, phone in unique_paths:\n",
    "        cond = np.logical_and(df['collectionName'] == collection, df['phoneName'] == phone)\n",
    "        data = df[cond][['latDeg', 'lngDeg']].to_numpy()\n",
    "        data = data.reshape(1, len(data), 2)\n",
    "        smoothed = kf_.smooth(data)\n",
    "        df.loc[cond, 'latDeg'] = smoothed.states.mean[0, :, 0]\n",
    "        df.loc[cond, 'lngDeg'] = smoothed.states.mean[0, :, 1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate train score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T12:10:16.826631Z",
     "iopub.status.busy": "2021-06-12T12:10:16.825989Z",
     "iopub.status.idle": "2021-06-12T12:10:16.830425Z",
     "shell.execute_reply": "2021-06-12T12:10:16.829595Z",
     "shell.execute_reply.started": "2021-06-12T12:10:16.826597Z"
    }
   },
   "outputs": [],
   "source": [
    "def percentile50(x):\n",
    "    return np.percentile(x, 50)\n",
    "def percentile95(x):\n",
    "    return np.percentile(x, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_score(df, gt,area=False):\n",
    "    cols_for_score = ['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'latDeg','lngDeg']\n",
    "    df = df[cols_for_score]\n",
    "    gt = gt[cols_for_score]\n",
    "    gt = gt.rename(columns={'latDeg':'latDeg_gt', 'lngDeg':'lngDeg_gt'})\n",
    "    df = df.merge(gt, on=['collectionName', 'phoneName', 'millisSinceGpsEpoch'], how='inner')\n",
    "    # calc_distance_error\n",
    "    df['err'] = calc_haversine(df['latDeg_gt'], df['lngDeg_gt'], df['latDeg'], df['lngDeg'])\n",
    "    # calc_evaluate_score\n",
    "    df['phone'] = df['collectionName'] + '_' + df['phoneName']\n",
    "    res = df.groupby([\"collectionName\",'phone'])['err'].agg([percentile50, percentile95]).reset_index()\n",
    "    res[\"area\"] = res[\"collectionName\"].map(collections_to_areas)\n",
    "    res['p50_p90_mean'] = (res['percentile50'] + res['percentile95']) / 2 \n",
    "    if area:\n",
    "        print(res.groupby([\"area\"])['p50_p90_mean'].mean())\n",
    "    score = res['p50_p90_mean'].mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. make KF baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "# reject outlier\n",
    "train_ro = add_distance_diff(base_train)\n",
    "th = 50\n",
    "train_ro.loc[((train_ro['dist_prev'] > th) & (train_ro['dist_next'] > th)), ['latDeg', 'lngDeg']] = np.nan\n",
    "cols = ['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']\n",
    "train_pred = apply_kf_smoothing(train_ro[cols])\n",
    "train_pred.to_pickle(\"train_kf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "test_ro = add_distance_diff(base_test)\n",
    "th = 50\n",
    "test_ro.loc[((test_ro['dist_prev'] > th) & (test_ro['dist_next'] > th)), ['latDeg', 'lngDeg']] = np.nan\n",
    "cols = ['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']\n",
    "test_pred = apply_kf_smoothing(test_ro[cols])\n",
    "test_pred.to_pickle(\"test_kf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. mean pred and denoise stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lerp_data(df):\n",
    "    '''\n",
    "    Generate interpolated lat,lng values for different phone times in the same collection.\n",
    "    '''\n",
    "    org_columns = df.columns\n",
    "\n",
    "    # Generate a combination of time x collection x phone and combine it with the original data (generate records to be interpolated)\n",
    "    time_list = df[['collectionName', 'millisSinceGpsEpoch']].drop_duplicates()\n",
    "    phone_list =df[['collectionName', 'phoneName']].drop_duplicates()\n",
    "    tmp = time_list.merge(phone_list, on='collectionName', how='outer')\n",
    "\n",
    "    lerp_df = tmp.merge(df, on=['collectionName', 'millisSinceGpsEpoch', 'phoneName'], how='left')\n",
    "    lerp_df['phone'] = lerp_df['collectionName'] + '_' + lerp_df['phoneName']\n",
    "    lerp_df = lerp_df.sort_values(['phone', 'millisSinceGpsEpoch'])\n",
    "    \n",
    "\n",
    "    lerp_df = lerp_df.set_index(\"millisSinceGpsEpoch\")\n",
    "    lerp_df[\"latDeg_\"]=lerp_df.groupby('phone')[\"latDeg\"].apply(\n",
    "        lambda group: group.interpolate(method='index',limit_area='inside')).values\n",
    "    lerp_df[\"lngDeg_\"]=lerp_df.groupby('phone')[\"lngDeg\"].apply(\n",
    "        lambda group: group.interpolate(method='index',limit_area='inside')).values\n",
    "    lerp_df = lerp_df.reset_index()\n",
    "    lerp_df = lerp_df[(lerp_df[\"latDeg\"].isnull())&(lerp_df[\"latDeg_\"].notnull())]\n",
    "    lerp_df[\"latDeg\"] = lerp_df[\"latDeg_\"]\n",
    "    lerp_df[\"lngDeg\"] = lerp_df[\"lngDeg_\"]    \n",
    "    return lerp_df[org_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_pred(df, lerp_df,weights_list):\n",
    "    weights_dic ={\n",
    "        'Pixel4':weights_list[0],\n",
    "        'Pixel4XLModded':weights_list[1],\n",
    "        'Pixel4XL':weights_list[2], \n",
    "        'Mi8':weights_list[3], \n",
    "        'Pixel4Modded':weights_list[4],\n",
    "        'Pixel5':weights_list[5], \n",
    "        'SamsungS20Ultra':weights_list[6],\n",
    "    }\n",
    "    '''\n",
    "    Make a prediction based on the average of the predictions of phones in the same collection.\n",
    "    '''\n",
    "    add_lerp = pd.concat([df, lerp_df])\n",
    "    add_lerp[\"weights\"] = add_lerp[\"phoneName\"].map(weights_dic)\n",
    "\n",
    "    add_lerp[\"latDeg_w\"] = add_lerp[\"latDeg\"]*add_lerp[\"weights\"]\n",
    "    add_lerp[\"lngDeg_w\"] = add_lerp[\"lngDeg\"]*add_lerp[\"weights\"]\n",
    "\n",
    "    mean_pred_result = add_lerp.groupby(['collectionName', 'millisSinceGpsEpoch'])[[\"latDeg_w\",\"lngDeg_w\",\"weights\"]].sum().reset_index()\n",
    "\n",
    "    mean_pred_result[\"latDeg\"] = mean_pred_result[\"latDeg_w\"]/mean_pred_result[\"weights\"]\n",
    "    mean_pred_result[\"lngDeg\"] = mean_pred_result[\"lngDeg_w\"]/mean_pred_result[\"weights\"]\n",
    "\n",
    "    mean_pred_df = df[['collectionName', 'phoneName', 'millisSinceGpsEpoch']].copy()\n",
    "    mean_pred_df = mean_pred_df.merge(mean_pred_result[['collectionName', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']], on=['collectionName', 'millisSinceGpsEpoch'], how='left')\n",
    "    return mean_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_stopping(df,thresh=2,window=30):\n",
    "\n",
    "    df[\"phone\"] = df[\"collectionName\"]+\"_\"+df[\"phoneName\"]\n",
    "    df[\"latDeg_shift_1\"] = df.groupby([\"phone\"])[\"latDeg\"].shift()\n",
    "    df[\"lngDeg_shift_1\"] = df.groupby([\"phone\"])[\"lngDeg\"].shift()\n",
    "    df[\"dist_shift1\"] = calc_haversine(\n",
    "        df['latDeg'],\n",
    "        df['lngDeg'],\n",
    "        df[\"latDeg_shift_1\"],\n",
    "        df[\"lngDeg_shift_1\"]\n",
    "    )\n",
    "    idx_stopping = df[\"dist_shift1\"]<thresh\n",
    "    df[\"latDeg_rolling_median\"] = df.groupby('phone')[\n",
    "        'latDeg'].transform(lambda s: s.rolling(window=window, center=True,min_periods = 1).median())\n",
    "    df[\"lngDeg_rolling_median\"] = df.groupby('phone')[\n",
    "        'lngDeg'].transform(lambda s: s.rolling(window=window, center=True,min_periods = 1).median())\n",
    "    df.loc[idx_stopping,\"latDeg\"] = df.loc[idx_stopping,\"latDeg_rolling_median\"]\n",
    "    df.loc[idx_stopping,\"lngDeg\"] = df.loc[idx_stopping,\"lngDeg_rolling_median\"]    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_postprocess(\n",
    "    df_pred,\n",
    "    thresh_stopping,\n",
    "    median_window,\n",
    "    repeats,\n",
    "    phone_weights,\n",
    "    evalate = False,\n",
    "):\n",
    "    for i in range(repeats):\n",
    "        df_lerp = make_lerp_data(df_pred)\n",
    "        df_pred = calc_mean_pred(df_pred, df_lerp,phone_weights)\n",
    "        df_pred = fill_stopping(df_pred,thresh=thresh_stopping,window=median_window)\n",
    "        if evalate:\n",
    "            score = get_train_score(df_pred, ground_truth)\n",
    "            print(\"count : {}, score : {:.4f}\".format(i+1,score))\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pd.read_pickle(\"train_kf.pkl\")\n",
    "test_pred = pd.read_pickle(\"test_kf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred[\"area\"] = train_pred[\"collectionName\"].map(collections_to_areas)\n",
    "test_pred[\"area\"] = test_pred[\"collectionName\"].map(collections_to_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_all = []\n",
    "test_pred_all = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_area = 'highway'\n",
    "thresh_stopping = 2\n",
    "median_window = 30\n",
    "repeats = 2\n",
    "phone_weights = [\n",
    "    0.8185272369412809,\n",
    "    0.34843268384881615,\n",
    "    0.4846644591847249,\n",
    "    0.9452238465987974,\n",
    "    0.9986981935560018,\n",
    "    0.18031216717585496,\n",
    "    0.445328696935905\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_area = train_pred[\n",
    "    train_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "train_pred_area = repeat_postprocess(\n",
    "    train_pred_area,\n",
    "    thresh_stopping,\n",
    "    median_window,\n",
    "    repeats,\n",
    "    phone_weights,\n",
    "    #evalate=True\n",
    ")\n",
    "train_pred_all.append(train_pred_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_area = test_pred[\n",
    "    test_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "test_pred_area = repeat_postprocess(\n",
    "    test_pred_area,\n",
    "    thresh_stopping,\n",
    "    median_window,\n",
    "    repeats,\n",
    "    phone_weights,\n",
    "    #evalate=True\n",
    ")\n",
    "test_pred_all.append(test_pred_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.2tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_area = 'tree'\n",
    "thresh_stopping = 1.5\n",
    "median_window = 30\n",
    "repeats = 7\n",
    "phone_weights = [\n",
    "    0.3115193764226983,\n",
    "    0.196919988137904,\n",
    "    0.1757293314492492,\n",
    "    0.4811756152653266,\n",
    "    0.9662722758231396,\n",
    "    0.48662174873037445,\n",
    "    0.1015768847715516\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_area = train_pred[\n",
    "    train_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "train_pred_area = repeat_postprocess(\n",
    "    train_pred_area,\n",
    "    thresh_stopping,\n",
    "    median_window,\n",
    "    repeats,\n",
    "    phone_weights,\n",
    "    #evalate=True\n",
    ")\n",
    "train_pred_all.append(train_pred_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_area = test_pred[\n",
    "    test_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "test_pred_area = repeat_postprocess(\n",
    "    test_pred_area,\n",
    "    thresh_stopping,\n",
    "    median_window,\n",
    "    repeats,\n",
    "    phone_weights,\n",
    "    #evalate=True\n",
    ")\n",
    "test_pred_all.append(test_pred_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 downtown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_area = 'downtown'\n",
    "thresh_stopping = 2\n",
    "median_window = 17\n",
    "repeats = 90\n",
    "phone_weights = [\n",
    "    0.30701282127989166,\n",
    "    0.8541717731249667,\n",
    "    0.1375108003326193,\n",
    "    0.34089358646969836,\n",
    "    0.3585169573830474,\n",
    "    0.253223739710727,\n",
    "    0.4830857599143877\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_area = train_pred[\n",
    "    train_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "train_pred_area = repeat_postprocess(\n",
    "    train_pred_area,\n",
    "    thresh_stopping,\n",
    "    median_window,\n",
    "    repeats,\n",
    "    phone_weights,\n",
    "    #evalate=True\n",
    ")\n",
    "train_pred_all.append(train_pred_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_area = test_pred[\n",
    "    test_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "test_pred_area = repeat_postprocess(\n",
    "    test_pred_area,\n",
    "    thresh_stopping,\n",
    "    median_window,\n",
    "    repeats,\n",
    "    phone_weights,\n",
    "    #evalate=True\n",
    ")\n",
    "test_pred_all.append(test_pred_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pd.concat(train_pred_all,ignore_index=True)\n",
    "test_pred = pd.concat(test_pred_all,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred.to_pickle(f\"train_pp_{exp_name}.pkl\")\n",
    "test_pred.to_pickle(f\"test_pp_{exp_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area\n",
      "downtown    13.904846\n",
      "highway      2.129207\n",
      "tree         3.361387\n",
      "Name: p50_p90_mean, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.333376347713986"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_train_score(train_pred, ground_truth,area = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Search GroundTruth Neighour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "def interp(observed):\n",
    "    rate = 0.1\n",
    "    x_observed = np.arange(0, len(observed),1)\n",
    "    x_latent = np.arange(0, len(observed)-1,0.1)\n",
    "    interp_func = interpolate.interp1d(x_observed,observed)\n",
    "    latent = interp_func(x_latent)    \n",
    "    return latent\n",
    "\n",
    "def snap_to_grid(df,line_points,max_neighbor = 5):\n",
    "    df = df.copy()\n",
    "    line_points[\"phone\"] = line_points[\"collectionName\"]+\"_\"+line_points[\"phoneName\"]\n",
    "    line_points_interp = pd.DataFrame()\n",
    "    line_points_interp[\"latDeg\"] = np.concatenate(\n",
    "        line_points.groupby([\"phone\"])[\"latDeg\"].apply(\n",
    "            lambda x:interp(x.values)).values\n",
    "    )\n",
    "    line_points_interp[\"lngDeg\"] = np.concatenate(\n",
    "        line_points.groupby([\"phone\"])[\"lngDeg\"].apply(\n",
    "            lambda x:interp(x.values)).values\n",
    "    )\n",
    "    line_points = line_points_interp\n",
    "    line_points = line_points.drop_duplicates([\"latDeg\",\"lngDeg\"]).reset_index()\n",
    "\n",
    "    pred_point = df.apply(lambda row:[row[\"latDeg\"],row[\"lngDeg\"]],axis=1).tolist()\n",
    "    grid_point = line_points.apply(lambda row:[row[\"latDeg\"],row[\"lngDeg\"]],axis=1).tolist()\n",
    "    neigh = NearestNeighbors(n_neighbors=1,n_jobs = -1)\n",
    "\n",
    "    neigh.fit(grid_point)\n",
    "    idx_neighbor_all = neigh.kneighbors(pred_point,max_neighbor , return_distance=False)\n",
    "    for i in range(max_neighbor):\n",
    "        idx_neighbor = idx_neighbor_all[:,i]\n",
    "        df[f\"latDeg_nearest_{i}\"] = line_points.loc[idx_neighbor][\"latDeg\"].values\n",
    "        df[f\"lngDeg_nearest_{i}\"] = line_points.loc[idx_neighbor][\"lngDeg\"].values\n",
    "        df[f\"dist_nearest_{i}\"] = calc_haversine(df[\"latDeg\"], df[\"lngDeg\"],df[f\"latDeg_nearest_{i}\"],df[f\"lngDeg_nearest_{i}\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pd.read_pickle(f\"train_pp_{exp_name}.pkl\")\n",
    "test_pred = pd.read_pickle(f\"test_pp_{exp_name}.pkl\")\n",
    "train_pred[\"area\"] = train_pred[\"collectionName\"].map(collections_to_areas)\n",
    "test_pred[\"area\"] = test_pred[\"collectionName\"].map(collections_to_areas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 get nearest GT feature for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [04:31<00:00, 12.91s/it]\n",
      "100%|██████████| 5/5 [00:12<00:00,  2.48s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "train_snap_all = []\n",
    "for target_area in train_pred[\"area\"].unique():\n",
    "    train_pred_area = train_pred[train_pred[\"area\"] == target_area].reset_index(drop=True)\n",
    "    ground_truth_area = ground_truth[ground_truth[\"area\"] == target_area].reset_index(drop=True)\n",
    "    target_collections = train_pred_area[\"collectionName\"].unique()\n",
    "    for target_collection in tqdm(target_collections):\n",
    "        target_pred_df = train_pred_area[\n",
    "            train_pred_area[\"collectionName\"]==target_collection].reset_index(drop=True)\n",
    "        target_gt_df = ground_truth_area[\n",
    "            (ground_truth_area[\"collectionName\"]!=target_collection)].reset_index(drop=True)\n",
    "        target_pred_df_snapped = snap_to_grid(target_pred_df,target_gt_df,max_neighbor=5)\n",
    "        train_snap_all.append(target_pred_df_snapped) \n",
    "train_pred = pd.concat(train_snap_all,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred.to_pickle(f\"train_snap_{exp_name}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 get nearest GT feature for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:19<00:00,  6.55s/it]\n"
     ]
    }
   ],
   "source": [
    "test_snap_all = []\n",
    "for target_area in tqdm(test_pred[\"area\"].unique()):\n",
    "    target_pred_df = test_pred[\n",
    "        test_pred[\"area\"] == target_area].reset_index(drop=True)\n",
    "    target_gt_df = ground_truth[\n",
    "        ground_truth[\"area\"] == target_area].reset_index(drop=True)\n",
    "    target_pred_df_snapped = snap_to_grid(target_pred_df,target_gt_df,max_neighbor=5)\n",
    "    test_snap_all.append(target_pred_df_snapped)\n",
    "test_pred = pd.concat(test_snap_all,ignore_index=True)\n",
    "test_pred.to_pickle(f\"test_snap_{exp_name}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pd.read_pickle(f\"train_snap_{exp_name}.pkl\")\n",
    "test_pred = pd.read_pickle(f\"test_snap_{exp_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area\n",
      "downtown    13.904846\n",
      "highway      2.129207\n",
      "tree         3.361387\n",
      "Name: p50_p90_mean, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.333376347713986"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_train_score(train_pred, ground_truth,area = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 add other phone feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lerp = make_lerp_data(train_pred)\n",
    "train_lerp = train_lerp.pivot(index=['collectionName',  'millisSinceGpsEpoch'], columns='phoneName', values=['latDeg','lngDeg'])\n",
    "train_lerp.columns = [\"other_\"+col[1]+\"_\"+col[0] for col in train_lerp.columns]\n",
    "train_lerp = train_lerp.reset_index()\n",
    "train_pred = train_pred.merge(train_lerp,on=['collectionName', 'millisSinceGpsEpoch'],how=\"left\")\n",
    "test_lerp = make_lerp_data(test_pred)\n",
    "test_lerp = test_lerp.pivot(index=['collectionName',  'millisSinceGpsEpoch'], columns='phoneName', values=['latDeg','lngDeg'])\n",
    "test_lerp.columns = [\"other_\"+col[1]+\"_\"+col[0] for col in test_lerp.columns]\n",
    "test_lerp = test_lerp.reset_index()\n",
    "test_pred = test_pred.merge(test_lerp,on=['collectionName', 'millisSinceGpsEpoch'],how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = ground_truth.copy()\n",
    "gt = gt.rename(columns={'latDeg':'latDeg_gt', 'lngDeg':'lngDeg_gt'})\n",
    "train_pred = train_pred.merge(gt[['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'latDeg_gt',\n",
    "       'lngDeg_gt']], on=['collectionName', 'phoneName', 'millisSinceGpsEpoch'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_ml(df):\n",
    "    df[\"phone\"] = df[\"collectionName\"]+\"_\"+df[\"phoneName\"]\n",
    "    df[\"latDeg_nearest_mean\"] = df[[\n",
    "        col for col in df.columns if \"latDeg_nearest\" in col]].mean(axis=1)\n",
    "    df[\"lngDeg_nearest_mean\"] = df[[\n",
    "        col for col in df.columns if \"lngDeg_nearest\" in col]].mean(axis=1)\n",
    "    df[\"latDeg_diff_p1\"] = df.groupby(['phone'])[\"latDeg\"].diff()\n",
    "    df[\"lngDeg_diff_p1\"] = df.groupby(['phone'])[\"lngDeg\"].diff()\n",
    "    df[\"latDeg_diff_m1\"] = df.groupby(['phone'])[\"latDeg\"].diff(-1)\n",
    "    df[\"lngDeg_diff_m1\"] = df.groupby(['phone'])[\"lngDeg\"].diff(-1)\n",
    "    latdeg_cols = [col for col in df.columns if \"latDeg\"  in col]\n",
    "    lngdeg_cols = [col for col in df.columns if \"lngDeg\"  in col]\n",
    "    relative_cols = latdeg_cols + lngdeg_cols\n",
    "    for col in relative_cols:\n",
    "        if \"latDeg\" in col:\n",
    "            df[col+\"_rel\"] = df[col]-df[\"latDeg\"]\n",
    "        elif \"lngDeg\" in col:\n",
    "            df[col+\"_rel\"] = df[col]-df[\"lngDeg\"]\n",
    "    phone_dic = {\n",
    "        'Pixel4':0,\n",
    "        'Pixel4XLModded':1,\n",
    "        'Pixel4XL':2,\n",
    "        'Mi8':3,\n",
    "        'Pixel4Modded':4,\n",
    "        'Pixel5':5,\n",
    "        'SamsungS20Ultra':6\n",
    "    }\n",
    "    df[\"phoneName_num\"] = df[\"phoneName\"].map(phone_dic)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = preprocess_for_ml(train_pred)\n",
    "test_pred = preprocess_for_ml(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = []\n",
    "test_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_use_cols = ['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'phone', 'latDeg_gt',\n",
    " 'lngDeg_gt','latDeg_gt_rel','lngDeg_gt_rel',\"area\"]\n",
    "use_cols = [col for col in train_pred.columns if col not in not_use_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'boosting': 'gbdt',\n",
    "          'metric': 'rmse', \n",
    "          'objective': 'regression',\n",
    "          \"learning_rate\":0.01\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 run lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb(df_train,df_test,target_col):\n",
    "\n",
    "    n_fold=3\n",
    "    folds=GroupKFold(n_splits=n_fold)\n",
    "    y_train = df_train[target_col]\n",
    "    oof = np.zeros(len(df_train))\n",
    "    predictions = np.zeros(len(df_test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train[use_cols],y_train,df_train[\"collectionName\"].values)):\n",
    "        trn_data = lgb.Dataset(df_train.iloc[trn_idx][use_cols], label=y_train[trn_idx])\n",
    "        val_data = lgb.Dataset(df_train.iloc[val_idx][use_cols], label=y_train[val_idx])\n",
    "        num_round = 1500\n",
    "        model= lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=False,\n",
    "                        early_stopping_rounds = 200)\n",
    "        oof[val_idx] = model.predict(df_train.iloc[val_idx][use_cols], num_iteration=model.best_iteration)\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"Feature\"] = use_cols\n",
    "        fold_importance_df[\"importance\"] = model.feature_importance(\"gain\")\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        predictions += model.predict(df_test[use_cols], num_iteration=model.best_iteration) / folds.n_splits\n",
    "    return oof,predictions,feature_importance_df\n",
    "\n",
    "def show_feature_importance(feature_importance_df):\n",
    "    cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "            .groupby(\"Feature\")\n",
    "            .mean()\n",
    "            .sort_values(by=\"importance\", ascending=False)[:80].index)\n",
    "\n",
    "    best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=(14,15))\n",
    "    sns.barplot(x=\"importance\",\n",
    "                y=\"Feature\",\n",
    "                data=best_features.sort_values(by=\"importance\",\n",
    "                                               ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.3.1 highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_area = \"highway\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18366\n",
      "[LightGBM] [Info] Number of data points in the train set: 63081, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score -0.000003\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18366\n",
      "[LightGBM] [Info] Number of data points in the train set: 62560, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score -0.000003\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18366\n",
      "[LightGBM] [Info] Number of data points in the train set: 62331, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score -0.000003\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18366\n",
      "[LightGBM] [Info] Number of data points in the train set: 63081, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score -0.000005\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008601 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18366\n",
      "[LightGBM] [Info] Number of data points in the train set: 62560, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18366\n",
      "[LightGBM] [Info] Number of data points in the train set: 62331, number of used features: 73\n",
      "[LightGBM] [Info] Start training from score -0.000005\n"
     ]
    }
   ],
   "source": [
    "df_train_area = train_pred[train_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "df_test_area = test_pred[test_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "oof,predictions,feature_importance_df = run_lgb(\n",
    "    df_train_area,df_test_area,'latDeg_gt_rel')\n",
    "df_train_area[\"latDeg\"] = df_train_area[\"latDeg\"] + oof\n",
    "df_test_area[\"latDeg\"] = df_test_area[\"latDeg\"] +predictions\n",
    "oof,predictions,feature_importance_df = run_lgb(\n",
    "    df_train_area,df_test_area,'lngDeg_gt_rel')\n",
    "df_train_area[\"lngDeg\"] = df_train_area[\"lngDeg\"] +oof\n",
    "df_test_area[\"lngDeg\"] = df_test_area[\"lngDeg\"] +predictions\n",
    "train_all.append(df_train_area)\n",
    "test_all.append(df_test_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_feature_importance(feature_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_area = \"tree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003520 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17342\n",
      "[LightGBM] [Info] Number of data points in the train set: 15938, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score -0.000007\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002703 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17324\n",
      "[LightGBM] [Info] Number of data points in the train set: 14808, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score -0.000006\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17328\n",
      "[LightGBM] [Info] Number of data points in the train set: 14940, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score -0.000005\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17342\n",
      "[LightGBM] [Info] Number of data points in the train set: 15938, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 0.000005\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17324\n",
      "[LightGBM] [Info] Number of data points in the train set: 14808, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 0.000012\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17328\n",
      "[LightGBM] [Info] Number of data points in the train set: 14940, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 0.000007\n"
     ]
    }
   ],
   "source": [
    "df_train_area = train_pred[train_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "df_test_area = test_pred[test_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "oof,predictions,feature_importance_df = run_lgb(\n",
    "    df_train_area,df_test_area,'latDeg_gt_rel')\n",
    "df_train_area[\"latDeg\"] = df_train_area[\"latDeg\"] + oof\n",
    "df_test_area[\"latDeg\"] = df_test_area[\"latDeg\"] +predictions\n",
    "oof,predictions,feature_importance_df = run_lgb(\n",
    "    df_train_area,df_test_area,'lngDeg_gt_rel')\n",
    "df_train_area[\"lngDeg\"] = df_train_area[\"lngDeg\"] +oof\n",
    "df_test_area[\"lngDeg\"] = df_test_area[\"lngDeg\"] +predictions\n",
    "train_all.append(df_train_area)\n",
    "test_all.append(df_test_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 downtown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_area = train_pred[train_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "# df_test_area = test_pred[test_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "# oof,predictions,feature_importance_df = run_lgb(\n",
    "#     df_train_area,df_test_area,'latDeg_gt_rel')\n",
    "# df_train_area[\"latDeg\"] = df_train_area[\"latDeg\"] + oof\n",
    "# df_test_area[\"latDeg\"] = df_test_area[\"latDeg\"] +predictions\n",
    "# oof,predictions,feature_importance_df = run_lgb(\n",
    "#     df_train_area,df_test_area,'lngDeg_gt_rel')\n",
    "# df_train_area[\"lngDeg\"] = df_train_area[\"lngDeg\"] +oof\n",
    "# df_test_area[\"lngDeg\"] = df_test_area[\"lngDeg\"] +predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_angle(lat1, lon1, lat2, lon2, lat3, lon3):\n",
    "    \"\"\"Calculates angle\n",
    "    \"\"\"\n",
    "    vec12 = np.array([lat1, lon1]) - np.array([lat2, lon2]) \n",
    "    vec32 = np.array([lat3, lon3]) - np.array([lat2, lon2])\n",
    "    norm12 = np.linalg.norm(vec12)\n",
    "    norm32 = np.linalg.norm(vec32)\n",
    "    cos = np.inner(vec12, vec32) / (norm12 * norm32)\n",
    "    rad = np.arccos(cos)\n",
    "    degree = np.rad2deg(rad)\n",
    "    \n",
    "    return degree\n",
    "def speed_predict(train_df, gt_df, test_df):\n",
    "\n",
    "    use_cols = [\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\",\n",
    "                \"latDeg\", \"lngDeg\", \"phone\"]\n",
    "    if \"phone\" not in train_df.columns:\n",
    "        train_df['phone'] = train_df['collectionName'] + '_' + train_df['phoneName']\n",
    "        test_df['phone'] = test_df['collectionName'] + '_' + test_df['phoneName']\n",
    "    train_indices = train_df[\"collectionName\"].str.contains(\"SJC\")\n",
    "    test_indices = test_df[\"collectionName\"].str.contains(\"SJC\")\n",
    "    trn_df = pd.merge(\n",
    "        train_df.loc[train_indices, use_cols].reset_index(drop=True),\n",
    "        gt_df.rename(columns={\n",
    "            \"latDeg\": \"gt_lat\",\n",
    "            \"lngDeg\": \"gt_lng\",\n",
    "            \"heightAboveWgs84EllipsoidM\": \"gt_heightAboveWgs84EllipsoidM\"\n",
    "        }),\n",
    "        on=[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    tst_df = test_df[test_indices].reset_index(drop=True)\n",
    "\n",
    "    def time_feature(df):\n",
    "        df[\"min_time\"] = df.groupby([\"collectionName\", \"phoneName\"])[\"millisSinceGpsEpoch\"].transform(\"min\")\n",
    "        df[\"max_time\"] = df.groupby([\"collectionName\", \"phoneName\"])[\"millisSinceGpsEpoch\"].transform(\"max\")\n",
    "        df[\"norm_time\"] = (df[\"millisSinceGpsEpoch\"] - df[\"min_time\"]) / (df[\"max_time\"] - df[\"min_time\"])\n",
    "        df.drop([\"min_time\", \"max_time\"], axis=1, inplace=True)\n",
    "        return df\n",
    "        \n",
    "    def add_prev_next(df):\n",
    "        # linear interpolation\n",
    "        df['latDeg_prev'] = df['latDeg'].shift(1)\n",
    "        df['latDeg_next'] = df['latDeg'].shift(-1)\n",
    "        df['lngDeg_prev'] = df['lngDeg'].shift(1)\n",
    "        df['lngDeg_next'] = df['lngDeg'].shift(-1)\n",
    "        df['phone_prev'] = df['phone'].shift(1)\n",
    "        df['phone_next'] = df['phone'].shift(-1)\n",
    "        df['time_prev'] = df['millisSinceGpsEpoch'].shift(1)\n",
    "        df['time_next'] = df['millisSinceGpsEpoch'].shift(-1)\n",
    "        \n",
    "        df['dist_prev'] = calc_haversine(df['latDeg'], df['lngDeg'], df['latDeg_prev'], df['lngDeg_prev'])\n",
    "        df['dist_next'] = calc_haversine(df['latDeg'], df['lngDeg'], df['latDeg_next'], df['lngDeg_next'])\n",
    "        \n",
    "        df.loc[df['phone']!=df['phone_prev'], ['latDeg_prev', 'lngDeg_prev', 'dist_prev']] = np.nan\n",
    "        df.loc[df['phone']!=df['phone_next'], ['latDeg_next', 'lngDeg_next', 'dist_next']] = np.nan\n",
    "        \n",
    "        df[\"angle\"] = df.apply(lambda x: calc_angle(\n",
    "            x[\"latDeg_prev\"], x[\"lngDeg_prev\"],\n",
    "            x[\"latDeg\"], x[\"lngDeg\"],\n",
    "            x[\"latDeg_next\"], x[\"lngDeg_next\"],\n",
    "        ), axis=1)\n",
    "        \n",
    "        df[\"speed_prev\"] = df[\"dist_prev\"] / (df[\"millisSinceGpsEpoch\"] - df[\"time_prev\"])\n",
    "        df[\"speed_next\"] = df[\"dist_next\"] / (df[\"time_next\"] - df[\"millisSinceGpsEpoch\"])\n",
    "        df[\"speed_change\"] = df[\"speed_next\"] - df[\"speed_prev\"]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def rolling_feature(df):\n",
    "        # angle\n",
    "        calc_cols = [\"angle\", \"dist_next\", \"speed_next\"]\n",
    "        window_list = [5, 11, 21]\n",
    "        for col in calc_cols:\n",
    "            for w in window_list:\n",
    "                df[f\"{col}_{w}_mean\"] = df.groupby(\"phone\")[col].rolling(\n",
    "                    window=w, center=True).mean().reset_index(level=0, drop=True)\n",
    "                df[f\"{col}_{w}_std\"] = df.groupby(\"phone\")[col].rolling(\n",
    "                    window=w, center=True).std().reset_index(level=0, drop=True)\n",
    "                df[f\"{col}_{w}_min\"] = df.groupby(\"phone\")[col].rolling(\n",
    "                    window=w, center=True).min().reset_index(level=0, drop=True)\n",
    "                df[f\"{col}_{w}_max\"] = df.groupby(\"phone\")[col].rolling(\n",
    "                    window=w, center=True).max().reset_index(level=0, drop=True)\n",
    "        return df\n",
    "    \n",
    "    def device_feature(df):\n",
    "        df[\"device_same_point_lat_std\"] = df.groupby(\n",
    "            [\"collectionName\", \"millisSinceGpsEpoch\"])[\"latDeg\"].transform(\"std\")\n",
    "        df[\"device_same_point_lng_std\"] = df.groupby(\n",
    "            [\"collectionName\", \"millisSinceGpsEpoch\"])[\"lngDeg\"].transform(\"std\")\n",
    "        return df\n",
    "\n",
    "    # train \n",
    "    trn_df = time_feature(trn_df)\n",
    "    trn_df = add_prev_next(trn_df)\n",
    "    trn_df = rolling_feature(trn_df)\n",
    "    # test\n",
    "    tst_df = time_feature(tst_df)\n",
    "    tst_df = add_prev_next(tst_df)\n",
    "    tst_df = rolling_feature(tst_df)\n",
    "\n",
    "    # lightgbm training\n",
    "    n_folds = 3\n",
    "    folds = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "    except_cols = [\n",
    "        \"collectionName\",\n",
    "        \"phoneName\",\n",
    "        \"millisSinceGpsEpoch\",\n",
    "        \"latDeg\", \"lngDeg\",\n",
    "        \"gt_heightAboveWgs84EllipsoidM\",\n",
    "        \"phone\",\n",
    "        \"gt_lat\", \"gt_lng\",\n",
    "        \"timeSinceFirstFixSeconds\",\n",
    "        \"hDop\",\n",
    "        \"vDop\",\n",
    "        \"speedMps\",\n",
    "        \"courseDegree\",\n",
    "        \"phone_prev\",\n",
    "        \"phone_next\",\n",
    "        \"error\",\n",
    "        \"area\"\n",
    "    ]\n",
    "\n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        \"metric\": \"rmse\",\n",
    "        'num_leaves': 128,\n",
    "        'min_data_in_leaf': 30,\n",
    "        'max_depth': 4,\n",
    "        'learning_rate': 0.05,\n",
    "        \"boosting\": \"gbdt\",\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"lambda_l1\": 0.6,\n",
    "        \"lambda_l2\": 0.3,\n",
    "        \"seed\": 2021,\n",
    "        \"verbosity\": -1,\n",
    "        \"num_threads\": -1,\n",
    "    }\n",
    "    oof_preds = np.zeros(len(trn_df))\n",
    "    test_preds = np.zeros(len(tst_df))\n",
    "\n",
    "    fe_df_list = []\n",
    "\n",
    "    for fold_ix, (trn_idx, val_idx) in enumerate(folds.split(trn_df, groups=trn_df[\"collectionName\"])):\n",
    "        print(f\"Fold {fold_ix}\")\n",
    "        use_feats = [c for c in trn_df.columns if c not in except_cols]\n",
    "        _trn_df = trn_df.iloc[trn_idx].reset_index(drop=True)\n",
    "        _val_df = trn_df.iloc[val_idx].reset_index(drop=True)\n",
    "        \n",
    "        # make lgb dataset\n",
    "        trn_data = lgb.Dataset(\n",
    "            _trn_df[use_feats],\n",
    "            label=_trn_df[\"speedMps\"]\n",
    "        )\n",
    "        val_data = lgb.Dataset(\n",
    "            _val_df[use_feats],\n",
    "            label=_val_df[\"speedMps\"]\n",
    "        )\n",
    "        model = lgb.train(\n",
    "            lgb_params,\n",
    "            trn_data,\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[trn_data, val_data],\n",
    "            verbose_eval=500,\n",
    "            early_stopping_rounds=100)\n",
    "        \n",
    "        oof_preds[val_idx] = model.predict(_val_df[use_feats])\n",
    "        test_preds += model.predict(tst_df[use_feats]) / n_folds\n",
    "        \n",
    "        fe_df = pd.DataFrame({\n",
    "            \"feature\": use_feats,\n",
    "            \"importance\": model.feature_importance(\"gain\")\n",
    "        })\n",
    "        fe_df_list.append(fe_df)\n",
    "        \n",
    "    importance_df = pd.concat(fe_df_list).groupby(\n",
    "        \"feature\")[\"importance\"].mean().sort_values(ascending=False).reset_index()\n",
    "    \n",
    "    res_df = pd.DataFrame({\n",
    "        \"preds\": oof_preds,\n",
    "        \"phone\": trn_df[\"phone\"],\n",
    "        \"latDeg\": trn_df[\"latDeg\"],\n",
    "        \"lngDeg\": trn_df[\"lngDeg\"],\n",
    "        \"gt_lat\": trn_df[\"gt_lat\"],\n",
    "        \"gt_lng\": trn_df[\"gt_lng\"],\n",
    "        \"speed\": trn_df[\"speedMps\"],\n",
    "    })\n",
    "    rmse = np.sqrt(mean_squared_error(res_df[\"speed\"], res_df[\"preds\"]))\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "\n",
    "    train_df.loc[train_indices, \"speed_pred\"] = oof_preds\n",
    "    test_df.loc[test_indices, \"speed_pred\"] = test_preds\n",
    "    \n",
    "    if 0:\n",
    "        # if SVC and stop, convert to NaN\n",
    "        speed_thresh = 2\n",
    "        time_from = 0.05\n",
    "        time_to = 0.95\n",
    "\n",
    "        def time_feature(df):\n",
    "            df[\"min_time\"] = df.groupby([\"collectionName\", \"phoneName\"])[\"millisSinceGpsEpoch\"].transform(\"min\")\n",
    "            df[\"max_time\"] = df.groupby([\"collectionName\", \"phoneName\"])[\"millisSinceGpsEpoch\"].transform(\"max\")\n",
    "            df[\"norm_time\"] = (df[\"millisSinceGpsEpoch\"] - df[\"min_time\"]) / (df[\"max_time\"] - df[\"min_time\"])\n",
    "            df.drop([\"min_time\", \"max_time\"], axis=1, inplace=True)\n",
    "            return df\n",
    "\n",
    "        train_df = time_feature(train_df)\n",
    "        test_df = time_feature(test_df)\n",
    "\n",
    "        train_df.loc[\n",
    "            (train_df[\"collectionName\"].str.contains(\"SJC\"))&\n",
    "            (train_df[\"speed_pred\"] < speed_thresh)&(\n",
    "            (train_df[\"norm_time\"] > time_from) & (train_df[\"norm_time\"] < time_to)), \"latDeg\"] = np.nan\n",
    "        train_df.loc[\n",
    "            (train_df[\"collectionName\"].str.contains(\"SJC\"))&\n",
    "            (train_df[\"speed_pred\"] < speed_thresh)&(\n",
    "            (train_df[\"norm_time\"] > time_from) & (train_df[\"norm_time\"] < time_to)), \"lngDeg\"] = np.nan\n",
    "\n",
    "        test_df.loc[\n",
    "            (test_df[\"collectionName\"].str.contains(\"SJC\"))&\n",
    "            (test_df[\"speed_pred\"] < speed_thresh)&(\n",
    "            (test_df[\"norm_time\"] > time_from) & (test_df[\"norm_time\"] < time_to)), \"latDeg\"] = np.nan\n",
    "        test_df.loc[\n",
    "            (test_df[\"collectionName\"].str.contains(\"SJC\"))&\n",
    "            (test_df[\"speed_pred\"] < speed_thresh)&(\n",
    "            (test_df[\"norm_time\"] > time_from) & (test_df[\"norm_time\"] < time_to)), \"lngDeg\"] = np.nan\n",
    "\n",
    "        train_df[\"latDeg\"] = train_df.groupby([\"collectionName\", \"phoneName\"])[\"latDeg\"].apply(\n",
    "            lambda group: group.interpolate(method=\"linear\")).fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "        train_df[\"lngDeg\"] = train_df.groupby([\"collectionName\", \"phoneName\"])[\"lngDeg\"].apply(\n",
    "            lambda group: group.interpolate(method=\"linear\")).fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "\n",
    "        test_df[\"latDeg\"] = test_df.groupby([\"collectionName\", \"phoneName\"])[\"latDeg\"].apply(\n",
    "            lambda group: group.interpolate(method=\"linear\")).fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "        test_df[\"lngDeg\"] = test_df.groupby([\"collectionName\", \"phoneName\"])[\"lngDeg\"].apply(\n",
    "            lambda group: group.interpolate(method=\"linear\")).fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "\n",
    "        del train_df[\"norm_time\"], test_df[\"norm_time\"]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def minimalize_search(query, ref):\n",
    "\n",
    "    n_query = len(query)\n",
    "\n",
    "    fit_lat_list = []\n",
    "    fit_lng_list = []\n",
    "    fit_cname_list = []\n",
    "    fit_timediff_list = []\n",
    "    fit_angle_list = []\n",
    "    fit_dist_list = []\n",
    "\n",
    "    # 最初はqueryのまま\n",
    "    fit_lat_list.append(query[\"latDeg\"].iloc[0])\n",
    "    fit_lng_list.append(query[\"lngDeg\"].iloc[0])\n",
    "    fit_cname_list.append(None)\n",
    "    fit_timediff_list.append(None)\n",
    "    fit_angle_list.append(None)\n",
    "    fit_dist_list.append(0)\n",
    "\n",
    "    # その次以降\n",
    "    for ix in tqdm(range(1, len(query))):\n",
    "\n",
    "        query_lat = query[\"latDeg\"].iloc[ix]\n",
    "        query_lng = query[\"lngDeg\"].iloc[ix]\n",
    "        query_speed = query[\"speed_pred\"].iloc[ix]\n",
    "\n",
    "        prev_lat = fit_lat_list[-1]\n",
    "        prev_lng = fit_lng_list[-1]\n",
    "        prev_dist = fit_dist_list[-1]\n",
    "        prev_cname = fit_cname_list[-1]\n",
    "\n",
    "        # 前の点と同じだった場合\n",
    "        if query_lat == prev_lat and query_lng == prev_lng:\n",
    "            fit_lat_list.append(query_lat)\n",
    "            fit_lng_list.append(query_lng)\n",
    "            fit_cname_list.append(fit_cname_list[-1])\n",
    "            fit_timediff_list.append(fit_timediff_list[-1])\n",
    "            fit_angle_list.append(None)\n",
    "            fit_dist_list.append(0)\n",
    "\n",
    "\n",
    "        else:\n",
    "            dists = calc_haversine(query_lat, query_lng, ref[\"latDeg\"].values, ref[\"lngDeg\"].values)\n",
    "\n",
    "            if dists.min() > 100:\n",
    "                fit_lat_list.append(query_lat)\n",
    "                fit_lng_list.append(query_lng)\n",
    "                fit_cname_list.append(fit_cname_list[-1])\n",
    "                fit_timediff_list.append(fit_timediff_list[-1])\n",
    "                fit_angle_list.append(None)\n",
    "                continue\n",
    "\n",
    "\n",
    "            candidate0_lat = query_lat\n",
    "            candidate0_lng = query_lng\n",
    "\n",
    "            argsort_indices = np.argsort(dists)\n",
    "\n",
    "            candidates = ref.iloc[argsort_indices[:30]].reset_index(drop=True)\n",
    "            candidates_lat = candidates[\"latDeg\"]\n",
    "            candidates_lng = candidates[\"lngDeg\"]\n",
    "            candidates_cname = candidates[\"collectionName\"]\n",
    "            candidates_time = candidates[\"millisSinceGpsEpoch\"]\n",
    "\n",
    "            # predとの距離\n",
    "            pred_dists = calc_haversine(\n",
    "                query_lat, query_lng,\n",
    "                candidates_lat,\n",
    "                candidates_lng\n",
    "            )\n",
    "            # 前のfitポイントとの距離\n",
    "            prev_fit_dists = calc_haversine(\n",
    "                prev_lat, prev_lng,\n",
    "                candidates_lat,\n",
    "                candidates_lng\n",
    "            )\n",
    "            \n",
    "            cost_pred_dist = pred_dists.values\n",
    "            cost_prev_dist = np.abs(prev_fit_dists - query_speed).values\n",
    "            cost_pred_fit_dist = np.abs(pred_dists - prev_dist).values\n",
    "            cost_cname = (prev_cname ==candidates_cname).values.astype(float)\n",
    "            \n",
    "            cost_total = cost_pred_dist * 0.5 + cost_prev_dist * 0.5+cost_pred_fit_dist*0.3+0.5*(cost_cname)\n",
    "            cost_min_idx = cost_total.argmin()\n",
    "\n",
    "            fit_lat_list.append(candidates_lat[cost_min_idx])\n",
    "            fit_lng_list.append(candidates_lng[cost_min_idx])\n",
    "            fit_cname_list.append(candidates_cname[cost_min_idx])\n",
    "            fit_timediff_list.append(candidates_time[cost_min_idx])\n",
    "            fit_angle_list.append(None)\n",
    "            fit_dist_list.append(cost_pred_dist[cost_min_idx])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"latDeg\": fit_lat_list,\n",
    "        \"lngDeg\": fit_lng_list,\n",
    "        \"millisSinceGpsEpoch\": query[\"millisSinceGpsEpoch\"],\n",
    "        \"collectionName\": query[\"collectionName\"],\n",
    "        \"phoneName\": query[\"phoneName\"]\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cost_minimalize(train_df, test_df, gt_df):\n",
    "\n",
    "    sjc_names = [c for c in collection_names if \"SJC\" in c]\n",
    "\n",
    "    sjc_dfs = {\n",
    "        0: train_df[train_df[\"collectionName\"] == sjc_names[0]].reset_index(drop=True),\n",
    "        1: train_df[train_df[\"collectionName\"] == sjc_names[1]].reset_index(drop=True),\n",
    "        2: train_df[train_df[\"collectionName\"] == sjc_names[2]].reset_index(drop=True)\n",
    "    }\n",
    "    sjc_gts = {\n",
    "        0: gt_df[gt_df[\"collectionName\"] == sjc_names[0]].reset_index(drop=True),\n",
    "        1: gt_df[gt_df[\"collectionName\"] == sjc_names[1]].reset_index(drop=True),\n",
    "        2: gt_df[gt_df[\"collectionName\"] == sjc_names[2]].reset_index(drop=True)\n",
    "    }\n",
    "    sjc_phones = {\n",
    "        0: sjc_dfs[0][\"phoneName\"].unique(),\n",
    "        1: sjc_dfs[1][\"phoneName\"].unique(),\n",
    "        2: sjc_dfs[2][\"phoneName\"].unique()\n",
    "    }\n",
    "    train_patterns = [\n",
    "        [0, [1, 2]],\n",
    "        [1, [0, 2]],\n",
    "        [2, [0, 1]],\n",
    "    ]\n",
    "    train_fit_df_list = []\n",
    "    query_df_list = []\n",
    "\n",
    "    # for train data\n",
    "    for query_ix, ref_indices in train_patterns:\n",
    "        print(f\"Query {query_ix}\")\n",
    "        for phone_ix in range(len(sjc_phones[query_ix])):\n",
    "\n",
    "            # queryの指定\n",
    "            query_all = sjc_dfs[query_ix]\n",
    "\n",
    "            query = query_all[\n",
    "                query_all[\"phoneName\"] == sjc_phones[query_ix][phone_ix]\n",
    "            ].reset_index(drop=True)\n",
    "            query[\"speed_pred\"] = query[\"speed_pred\"].clip(0, None)\n",
    "\n",
    "            # referenceの指定\n",
    "            ref_list = [sjc_gts[ix] for ix in ref_indices]\n",
    "            ref = pd.concat(ref_list, ignore_index=True)\n",
    "            \n",
    "            ret_df = minimalize_search(query, ref)\n",
    "\n",
    "            train_fit_df_list.append(ret_df)\n",
    "            query_df_list.append(query)\n",
    "\n",
    "    train_ret_df = pd.concat(train_fit_df_list, ignore_index=True)\n",
    "    \n",
    "    # for test data\n",
    "    test_fit_df_list = []\n",
    "    test_ref_indices = [0, 1, 2]\n",
    "    test_collection_names = test_df[\"collectionName\"].unique()\n",
    "    \n",
    "    for c_name in test_collection_names:\n",
    "        if \"SJC\" not in c_name:\n",
    "            continue\n",
    "        print(f\"Test: {c_name}\")\n",
    "        c_test_df = test_df[test_df[\"collectionName\"] == c_name]\n",
    "        for phone_name in c_test_df[\"phoneName\"].unique():\n",
    "            \n",
    "            query = c_test_df[\n",
    "                c_test_df[\"phoneName\"] == phone_name].reset_index(drop=True)\n",
    "            query[\"speed_pred\"] = query[\"speed_pred\"].clip(0, None)            \n",
    "            ref_list = [sjc_gts[ix] for ix in ref_indices]\n",
    "            ref = pd.concat(ref_list, ignore_index=True)\n",
    "            \n",
    "            ret_df = minimalize_search(query, ref)\n",
    "            test_fit_df_list.append(ret_df)\n",
    "            \n",
    "    test_ret_df = pd.concat(test_fit_df_list, ignore_index=True)\n",
    "    \n",
    "    return train_ret_df, test_ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_area = \"downtown\"\n",
    "snap_thresh = 100\n",
    "snap_weight = 1\n",
    "df_train_area = train_pred[train_pred[\"area\"]==target_area].copy().reset_index(drop=True)\n",
    "df_test_area = test_pred[test_pred[\"area\"]==target_area].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-59-c106aa9b72ab>:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cos = np.inner(vec12, vec32) / (norm12 * norm32)\n",
      "<ipython-input-59-c106aa9b72ab>:9: RuntimeWarning: invalid value encountered in arccos\n",
      "  rad = np.arccos(cos)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[209]\ttraining's rmse: 0.575718\tvalid_1's rmse: 1.06142\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[88]\ttraining's rmse: 0.805434\tvalid_1's rmse: 0.811816\n",
      "Fold 2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[397]\ttraining's rmse: 0.505702\tvalid_1's rmse: 0.998693\n",
      "RMSE: 0.9688705240859429\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = speed_predict(df_train_area, ground_truth, df_test_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_names = train_df[\"collectionName\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2889 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2889/2889 [00:14<00:00, 204.26it/s]\n",
      "100%|██████████| 2825/2825 [00:13<00:00, 204.57it/s]\n",
      "  1%|          | 20/2013 [00:00<00:09, 199.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2013/2013 [00:10<00:00, 198.73it/s]\n",
      "100%|██████████| 2082/2082 [00:10<00:00, 199.60it/s]\n",
      "  1%|          | 21/2329 [00:00<00:11, 203.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2329/2329 [00:11<00:00, 202.18it/s]\n",
      "100%|██████████| 2369/2369 [00:11<00:00, 201.33it/s]\n",
      "  1%|          | 21/2323 [00:00<00:11, 202.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 2021-04-22-US-SJC-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2323/2323 [00:11<00:00, 202.07it/s]\n",
      "  1%|          | 21/1978 [00:00<00:09, 203.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 2021-04-29-US-SJC-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1978/1978 [00:09<00:00, 201.93it/s]\n",
      "100%|██████████| 2060/2060 [00:10<00:00, 202.37it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ret_df, test_ret_df = cost_minimalize(train_df, test_df, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.62700921517106"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_train_score(train_ret_df,ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all.append(train_ret_df)\n",
    "test_all.append(test_ret_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rule_for_stg(df,snap_thresh,snap_weight):\n",
    "#     idx = df[\"dist_nearest_0\"]<snap_thresh\n",
    "#     df.loc[idx,\"latDeg\"] = (1-snap_weight)*df.loc[\n",
    "#         idx,\"latDeg\"]+snap_weight*df.loc[idx,\"latDeg_nearest_0\"]\n",
    "#     df.loc[idx,\"lngDeg\"] = (1-snap_weight)*df.loc[\n",
    "#         idx,\"lngDeg\"]+snap_weight*df.loc[idx,\"lngDeg_nearest_0\"]\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_area = rule_for_stg(df_train_area,snap_thresh,snap_weight)\n",
    "# df_test_area = rule_for_stg(df_test_area,snap_thresh,snap_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_all.append(df_train_area)\n",
    "# test_all.append(df_test_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.4 concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pd.concat(train_all,ignore_index=True)\n",
    "test_pred = pd.concat(test_all,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area\n",
      "downtown    9.627009\n",
      "highway     2.025704\n",
      "tree        3.117775\n",
      "Name: p50_p90_mean, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.859907389338233"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_train_score(train_pred, ground_truth,area = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred.to_pickle(f\"train_ml_{exp_name}.pkl\")\n",
    "test_pred.to_pickle(f\"test_ml_{exp_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred =pd.read_pickle(f\"train_ml_{exp_name}.pkl\")\n",
    "test_pred =pd.read_pickle(f\"test_ml_{exp_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred[\"phone\"] = train_pred[\"collectionName\"]+\"_\"+train_pred[\"phoneName\"]\n",
    "test_pred[\"phone\"] = test_pred[\"collectionName\"]+\"_\"+test_pred[\"phoneName\"]\n",
    "train_pred[\"area\"] = train_pred[\"collectionName\"].map(collections_to_areas)\n",
    "test_pred[\"area\"] = test_pred[\"collectionName\"].map(collections_to_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area\n",
      "downtown    9.627009\n",
      "highway     2.025704\n",
      "tree        3.117775\n",
      "Name: p50_p90_mean, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.859907389338233"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_train_score(train_pred, ground_truth,area = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count : 1, score : 2.0147\n"
     ]
    }
   ],
   "source": [
    "train_pred_all = []\n",
    "test_pred_all = []\n",
    "#### 2.1 highway\n",
    "target_area = 'highway'\n",
    "thresh_stopping = 2\n",
    "median_window = 30\n",
    "repeats = 1\n",
    "phone_weights = [\n",
    "    0.8185272369412809,\n",
    "    0.34843268384881615,\n",
    "    0.4846644591847249,\n",
    "    0.9452238465987974,\n",
    "    0.9986981935560018,\n",
    "    0.18031216717585496,\n",
    "    0.445328696935905\n",
    "    ]\n",
    "\n",
    "train_pred_area = train_pred[\n",
    "    train_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "train_pred_area = repeat_postprocess(\n",
    "    train_pred_area,\n",
    "    thresh_stopping,\n",
    "    median_window,\n",
    "    repeats,\n",
    "    phone_weights,\n",
    "    evalate=True\n",
    ")\n",
    "train_pred_all.append(train_pred_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_pred_area = test_pred[\n",
    "    test_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "test_pred_area = repeat_postprocess(\n",
    "    test_pred_area,\n",
    "    thresh_stopping,\n",
    "    median_window,\n",
    "    repeats,\n",
    "    phone_weights,\n",
    "    #evalate=True\n",
    ")\n",
    "test_pred_all.append(test_pred_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count : 1, score : 3.1163\n",
      "count : 2, score : 3.1165\n",
      "count : 3, score : 3.1123\n",
      "count : 4, score : 3.1117\n",
      "count : 5, score : 3.1120\n",
      "count : 6, score : 3.1083\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "####  2.2tree\n",
    "\n",
    "target_area = 'tree'\n",
    "thresh_stopping = 1.5\n",
    "median_window = 30\n",
    "repeats = 6\n",
    "phone_weights = [\n",
    "    0.3115193764226983,\n",
    "    0.196919988137904,\n",
    "    0.1757293314492492,\n",
    "    0.4811756152653266,\n",
    "    0.9662722758231396,\n",
    "    0.48662174873037445,\n",
    "    0.1015768847715516\n",
    "]\n",
    "\n",
    "train_pred_area = train_pred[\n",
    "    train_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "train_pred_area = repeat_postprocess(\n",
    "    train_pred_area,\n",
    "    thresh_stopping,\n",
    "    median_window,\n",
    "    repeats,\n",
    "    phone_weights,\n",
    "    evalate=True\n",
    ")\n",
    "train_pred_all.append(train_pred_area)\n",
    "\n",
    "test_pred_area = test_pred[\n",
    "    test_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "test_pred_area = repeat_postprocess(\n",
    "    test_pred_area,\n",
    "    thresh_stopping,\n",
    "    median_window,\n",
    "    repeats,\n",
    "    phone_weights,\n",
    "    #evalate=True\n",
    ")\n",
    "test_pred_all.append(test_pred_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count : 1, score : 9.4646\n"
     ]
    }
   ],
   "source": [
    "#### 2.3 downtown\n",
    "\n",
    "target_area = 'downtown'\n",
    "thresh_stopping = 2\n",
    "median_window = 17\n",
    "repeats = 1\n",
    "phone_weights = [\n",
    "    0.30701282127989166,\n",
    "    0.8541717731249667,\n",
    "    0.1375108003326193,\n",
    "    0.34089358646969836,\n",
    "    0.3585169573830474,\n",
    "    0.253223739710727,\n",
    "    0.4830857599143877\n",
    "]\n",
    "\n",
    "train_pred_area = train_pred[\n",
    "    train_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "train_pred_area = repeat_postprocess(\n",
    "    train_pred_area,\n",
    "    thresh_stopping,\n",
    "    median_window,\n",
    "    repeats,\n",
    "    phone_weights,\n",
    "    evalate=True\n",
    ")\n",
    "train_pred_all.append(train_pred_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_pred_area = test_pred[\n",
    "    test_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "test_pred_area = repeat_postprocess(\n",
    "    test_pred_area,\n",
    "    thresh_stopping,\n",
    "    median_window,\n",
    "    repeats,\n",
    "    phone_weights,\n",
    "    #evalate=True\n",
    ")\n",
    "test_pred_all.append(test_pred_area)\n",
    "\n",
    "train_pred = pd.concat(train_pred_all,ignore_index=True)\n",
    "test_pred = pd.concat(test_pred_all,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area\n",
      "downtown    9.464606\n",
      "highway     2.014750\n",
      "tree        3.108288\n",
      "Name: p50_p90_mean, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.836786462394463"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_train_score(train_pred,ground_truth,area=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred[\"phone\"] = train_pred[\"collectionName\"]+\"_\"+train_pred[\"phoneName\"]\n",
    "test_pred[\"phone\"] = test_pred[\"collectionName\"]+\"_\"+test_pred[\"phoneName\"]\n",
    "train_pred[\"area\"] = train_pred[\"collectionName\"].map(collections_to_areas)\n",
    "test_pred[\"area\"] = test_pred[\"collectionName\"].map(collections_to_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "from pyproj import Proj, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WGS84_to_ECEF(lat, lon, alt):\n",
    "    # convert to radians\n",
    "    rad_lat = lat * (np.pi / 180.0)\n",
    "    rad_lon = lon * (np.pi / 180.0)\n",
    "    a    = 6378137.0\n",
    "    # f is the flattening factor\n",
    "    finv = 298.257223563\n",
    "    f = 1 / finv   \n",
    "    # e is the eccentricity\n",
    "    e2 = 1 - (1 - f) * (1 - f)    \n",
    "    # N is the radius of curvature in the prime vertical\n",
    "    N = a / np.sqrt(1 - e2 * np.sin(rad_lat) * np.sin(rad_lat))\n",
    "    x = (N + alt) * np.cos(rad_lat) * np.cos(rad_lon)\n",
    "    y = (N + alt) * np.cos(rad_lat) * np.sin(rad_lon)\n",
    "    z = (N * (1 - e2) + alt)        * np.sin(rad_lat)\n",
    "    return x, y, z\n",
    "\n",
    "transformer = pyproj.Transformer.from_crs(\n",
    "    {\"proj\":'geocent', \"ellps\":'WGS84', \"datum\":'WGS84'},\n",
    "    {\"proj\":'latlong', \"ellps\":'WGS84', \"datum\":'WGS84'},)\n",
    "def ECEF_to_WGS84(x,y,z):\n",
    "    lon, lat, alt = transformer.transform(x,y,z,radians=False)\n",
    "    return lon, lat, alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_shift(d,a):\n",
    "    cols = d.columns\n",
    "    d['heightAboveWgs84EllipsoidM'] = 63.5\n",
    "    d['x'], d['y'], d['z'] = zip(*d.apply(lambda x: WGS84_to_ECEF(x.latDeg, x.lngDeg, x.heightAboveWgs84EllipsoidM), axis=1))\n",
    "\n",
    "    #a = -0.2\n",
    "    d.sort_values(['phone','millisSinceGpsEpoch'], inplace=True)\n",
    "    for fi in ['x','y','z']:\n",
    "        d[[fi+'p']] = d[fi].shift().where(d['phone'].eq(d['phone'].shift()))\n",
    "        d[[fi+'diff']] = d[fi]-d[fi+'p']\n",
    "    #d[['yp']] = d['y'].shift().where(d['phone'].eq(d['phone'].shift()))\n",
    "    d[['dist']] = np.sqrt(d['xdiff']**2 + d['ydiff']**2+ d['zdiff']**2)\n",
    "    for fi in ['x','y','z']:\n",
    "        d[[fi+'new']] = d[fi+'p'] + d[fi+'diff']*(1-a/d['dist'])\n",
    "    lng, lat, alt = ECEF_to_WGS84(d['xnew'].values,d['ynew'].values,d['znew'].values)\n",
    "    lng[np.isnan(lng)] = d.loc[np.isnan(lng),'lngDeg']\n",
    "    lat[np.isnan(lat)] = d.loc[np.isnan(lat),'latDeg']\n",
    "    d['latDeg'] = lat\n",
    "    d['lngDeg'] = lng\n",
    "\n",
    "    d.sort_values(['phone','millisSinceGpsEpoch'],inplace = True)    \n",
    "    return d[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = []\n",
    "test_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.15\n",
    "target_area = \"highway\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_area = train_pred[\n",
    "    train_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "train_pred_area = position_shift(train_pred_area,a)\n",
    "train_all.append(train_pred_area)\n",
    "test_pred_area = test_pred[\n",
    "    test_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "test_pred_area = position_shift(test_pred_area,a)\n",
    "test_all.append(test_pred_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.4\n",
    "target_area = \"tree\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_area = train_pred[\n",
    "    train_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "train_pred_area = position_shift(train_pred_area,a)\n",
    "train_all.append(train_pred_area)\n",
    "test_pred_area = test_pred[\n",
    "    test_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "test_pred_area = position_shift(test_pred_area,a)\n",
    "test_all.append(test_pred_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1.35\n",
    "target_area = \"downtown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_area = train_pred[\n",
    "    train_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "train_pred_area = position_shift(train_pred_area,a)\n",
    "train_all.append(train_pred_area)\n",
    "test_pred_area = test_pred[\n",
    "    test_pred[\"area\"]==target_area].reset_index(drop=True)\n",
    "test_pred_area = position_shift(test_pred_area,a)\n",
    "test_all.append(test_pred_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pd.concat(train_all,ignore_index=True)\n",
    "test_pred = pd.concat(test_all,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area\n",
      "downtown    9.238825\n",
      "highway     1.996353\n",
      "tree        3.024461\n",
      "Name: p50_p90_mean, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.788796419204225"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_train_score(train_pred,ground_truth,area = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_collection_with_gt(train_pred, ground_truth,'2020-05-21-US-MTV-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_collection_with_gt(train_pred, ground_truth,'2020-09-04-US-SF-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred[\"phone\"] = test_pred[\"collectionName\"]+ '_' + test_pred['phoneName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sample_sub[[\"phone\",\"millisSinceGpsEpoch\"]].copy()\n",
    "sub = sub.merge(test_pred[[\n",
    "    \"phone\",\"millisSinceGpsEpoch\",\"latDeg\",\"lngDeg\"]],\n",
    "                on=[\"phone\",\"millisSinceGpsEpoch\"],how=\"left\")\n",
    "\n",
    "sub.to_csv(f'sub_{exp_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
